{"0": {
    "doc": "Ansible",
    "title": "Ansible",
    "content": " ",
    "url": "/my-tech/docs/Ansible",
    
    "relUrl": "/docs/Ansible"
  },"1": {
    "doc": "Ansible",
    "title": "",
    "content": "Date Posted: 2023 01 13 . ",
    "url": "/my-tech/docs/Ansible",
    
    "relUrl": "/docs/Ansible"
  },"2": {
    "doc": "Ansible",
    "title": "Ansible cheat sheet",
    "content": "| Category | Command/Option | Explanation | . | General | ansible --version | Display the version of Ansible installed. | . | General | ansible -m ping all | Ping all hosts to check if they are reachable. | . | General | ansible -m shell -a 'free -m' all | Run the ‘free -m’ shell command on all hosts. | . | Playbooks | ansible-playbook playbook.yml | Run the playbook named playbook.yml. | . | Playbooks | ansible-playbook -i inventory.ini playbook.yml | Run a playbook with a specified inventory file. | . | Playbooks | ansible-playbook playbook.yml --syntax-check | Perform a syntax check on the playbook. | . | Playbooks | ansible-playbook playbook.yml --start-at-task='taskname' | Start the playbook at the specified task. | . | Playbooks | ansible-playbook playbook.yml --list-hosts | List all hosts the playbook will run against. | . | Playbooks | ansible-playbook playbook.yml --list-tasks | List all tasks the playbook will execute. | . | Playbooks | ansible-playbook playbook.yml --step | Execute the playbook interactively, asking for confirmation at each step. | . | Playbooks | ansible-playbook playbook.yml --check | Do a dry run of the playbook without making actual changes. | . | Playbooks | ansible-playbook playbook.yml --diff | Show differences in files when running the playbook. | . | Playbooks | ansible-playbook playbook.yml --tags \"tag1,tag2\" | Run only the tasks tagged with tag1 and tag# | . | Playbooks | ansible-playbook playbook.yml --skip-tags \"tag3\" | Skip tasks tagged with tag# | . | Playbooks | ansible-playbook playbook.yml --limit servers | Limit the playbook execution to the group ‘servers’. | . | Playbooks | ansible-playbook playbook.yml --extra-vars \"version=1.2.3\" | Run the playbook with extra variables. | . | Playbooks | ansible-playbook playbook.yml --forks=5 | Set the number of parallel processes to # | . | Playbooks | ansible-playbook playbook.yml -v | Run the playbook in verbose mode. -vvv and -vvvv can be used for more verbosity. | . | Playbooks | ansible-playbook playbook.yml --check --diff | Dry-run the playbook and show file differences. | . | Roles | ansible-galaxy init role_name | Initialize a new role structure with the specified name. | . | Roles | ansible-galaxy install role_name | Install an Ansible role. | . | Vault | ansible-vault create vault.yml | Create a new encrypted file. | . | Vault | ansible-vault view vault.yml | View an encrypted file. | . | Vault | ansible-vault edit vault.yml | Edit an encrypted file. | . | Vault | ansible-vault decrypt vault.yml | Decrypt an encrypted file. | . | Vault | ansible-vault encrypt vault.yml | Encrypt a file. | . | Vault | ansible-vault rekey vault.yml | Change the password on a vault file. | . | Vault | ansible-vault encrypt_string --name 'var_name' 'string' | Encrypt a string and output it in a format ready for use with Ansible. | . | Vault | ansible-playbook playbook.yml --ask-vault-pass | Run a playbook and prompt for the vault password. | . | Vault | ansible-playbook playbook.yml --vault-password-file vault.pass | Run a playbook using a file containing the vault password. | . | Vault | ansible-vault encrypt_string --stdin-name 'var_name' | Read the string from stdin and encrypt it. | . | Vault | ansible-vault encrypt --vault-id dev@prompt vars.yml | Encrypt a file using a prompt for the vault password. | . | Vault | ansible-playbook playbook.yml --vault-id dev@prompt | Run a playbook using a prompt for the vault password. | . | Configuration | ansible-config list | List all configuration options. | . | Configuration | ansible-config dump | Show the current configuration. | . | Configuration | ansible-config view | View the current Ansible configuration. | . | Configuration | ansible-config dump --only-changed | Dump configuration items that have changed from the default. | . | Configuration | ansible-config set ANSIBLE_HOST_KEY_CHECKING=False | Set a specific configuration item. | . | Configuration | ansible-config unset ANSIBLE_HOST_KEY_CHECKING | Unset a specific configuration item. | . | Console | ansible-console | Start an interactive console for executing Ansible tasks. | . | Console | ansible-console -i inventory.ini | Start an interactive console using a specific inventory. | . | Modules | ansible-doc -l | List available modules. | . | Modules | ansible-doc module_name | Get documentation for a specific module. | . | Inventory | ansible-inventory --list -y | List the inventory in YAML format. | . | Inventory | ansible-inventory --graph | Show a graph of the inventory. | . | Inventory | ansible -i inventory.ini all -m ping | Use a specific inventory file and ping all hosts. | . | Inventory | ansible-inventory --host hostname | Display all variables for a specific host. | . | Inventory | ansible-inventory --playbook-dir . --graph | Display a graph of the inventory from the current directory. | . | Inventory | ansible -i 'localhost,' -c local -m ping | Ping localhost using local connection and ad-hoc inventory. | . | Inventory | ansible-inventory --list --yaml | List inventory in YAML format. | . | Pull | ansible-pull -U git_url | Pull a Git repository of Ans | . | Pull | ansible-pull -U git_url | Pull a Git repository of Ansible configurations on the target host. | . | Galaxy | ansible-galaxy collection init my_namespace.my_collection | Initialize a new collection with the given namespace and name. | . | Galaxy | ansible-galaxy collection build | Build an Ansible collection package ready for distribution. | . | Galaxy | ansible-galaxy collection install my_namespace.my_collection | Install an Ansible collection from Galaxy. | . | Galaxy | ansible-galaxy role init my_role | Initialize a new role with the given name. | . | Galaxy | ansible-galaxy role install my_role | Install an Ansible role from Galaxy. | . | Galaxy | ansible-galaxy list | List installed roles and collections. | . | Galaxy | ansible-galaxy collection install -r requirements.yml | Install collections from a requirements file. | . | Galaxy | ansible-galaxy role install -r requirements.yml | Install roles from a requirements file. | . | Galaxy | ansible-galaxy collection publish ./namespace-collection-1.0.0.tar.gz --api-key=your_token | Publish a collection to Galaxy using an API token. | . | Galaxy | ansible-galaxy role init --role-skeleton skeleton my_role | Initialize a new role with a specified role skeleton. | . | Modules | ansible localhost -m file -a \"path=/tmp/test state=touch\" | Create a new file on the local host using the file module. | . | Modules | ansible localhost -m package -a \"name=vim state=present\" | Install a package on the local host using the package module. | . | Modules | ansible localhost -m command -a \"uptime\" | Run a command on the local host using the command module. | . | Modules | ansible localhost -m user -a \"name=testuser state=absent\" | Remove user ‘testuser’ from the local host using the user module. | . | Modules | ansible localhost -m service -a \"name=httpd state=restarted\" | Restart the ‘httpd’ service on the local host using the service module. | . | Modules | ansible localhost -m copy -a \"src=/etc/hosts dest=/tmp/hosts\" | Copy ‘/etc/hosts’ to ‘/tmp/hosts’ on the local host using the copy module. | . | Modules | ansible localhost -m file -a \"path=/tmp/test state=absent\" | Remove file ‘/tmp/test’ on the local host using the file module. | . | Modules | ansible localhost -m apt -a \"name=nginx state=latest\" | Install the latest version of ‘nginx’ on the local host using the apt module (Debian-based systems). | . ",
    "url": "/my-tech/docs/Ansible#ansible-cheat-sheet",
    
    "relUrl": "/docs/Ansible#ansible-cheat-sheet"
  },"3": {
    "doc": "Docker",
    "title": "Docker",
    "content": " ",
    "url": "/my-tech/docs/Docker",
    
    "relUrl": "/docs/Docker"
  },"4": {
    "doc": "Docker",
    "title": "",
    "content": "Date Posted: 2023 01 31 . ",
    "url": "/my-tech/docs/Docker",
    
    "relUrl": "/docs/Docker"
  },"5": {
    "doc": "Docker",
    "title": "Docker Cheat Sheet",
    "content": ". ",
    "url": "/my-tech/docs/Docker#docker-cheat-sheet",
    
    "relUrl": "/docs/Docker#docker-cheat-sheet"
  },"6": {
    "doc": "Docker",
    "title": "Table of Contents",
    "content": ". | Categories . | Basic Docker CLIs | Container Management CLIs | Inspecting the Container | Interacting with Container | Image Management Commands | Image Transfer Commands | Builder Main Commands | The Docker CLI | Docker Security | . | . ",
    "url": "/my-tech/docs/Docker#table-of-contents",
    
    "relUrl": "/docs/Docker#table-of-contents"
  },"7": {
    "doc": "Docker",
    "title": "Basic Docker CLIs in a table",
    "content": ". ",
    "url": "/my-tech/docs/Docker#basic-docker-clis-in-a-table",
    
    "relUrl": "/docs/Docker#basic-docker-clis-in-a-table"
  },"8": {
    "doc": "Docker",
    "title": "Container Management CLIs",
    "content": "Here’s the list of the Docker commands that manages Docker images and containers flawlessly: . ",
    "url": "/my-tech/docs/Docker#container-management-clis",
    
    "relUrl": "/docs/Docker#container-management-clis"
  },"9": {
    "doc": "Docker",
    "title": "Inspecting The Container",
    "content": ". ",
    "url": "/my-tech/docs/Docker#inspecting-the-container",
    
    "relUrl": "/docs/Docker#inspecting-the-container"
  },"10": {
    "doc": "Docker",
    "title": "Interacting with Container",
    "content": ". ",
    "url": "/my-tech/docs/Docker#interacting-with-container",
    
    "relUrl": "/docs/Docker#interacting-with-container"
  },"11": {
    "doc": "Docker",
    "title": "Image Management Commands",
    "content": ". ",
    "url": "/my-tech/docs/Docker#image-management-commands",
    
    "relUrl": "/docs/Docker#image-management-commands"
  },"12": {
    "doc": "Docker",
    "title": "Image Transfer Commands",
    "content": ". ",
    "url": "/my-tech/docs/Docker#image-transfer-commands",
    
    "relUrl": "/docs/Docker#image-transfer-commands"
  },"13": {
    "doc": "Docker",
    "title": "Builder Main Commands",
    "content": ". ",
    "url": "/my-tech/docs/Docker#builder-main-commands",
    
    "relUrl": "/docs/Docker#builder-main-commands"
  },"14": {
    "doc": "Docker",
    "title": "The Docker CLI",
    "content": " ",
    "url": "/my-tech/docs/Docker#the-docker-cli",
    
    "relUrl": "/docs/Docker#the-docker-cli"
  },"15": {
    "doc": "Docker",
    "title": "Manage images",
    "content": ". docker build . docker build [options] . -t \"app/container_name\" # name . Create an image from a Dockerfile. docker run . docker run [options] IMAGE # see `docker create` for options . Run a command in an image. ",
    "url": "/my-tech/docs/Docker#manage-images",
    
    "relUrl": "/docs/Docker#manage-images"
  },"16": {
    "doc": "Docker",
    "title": "Manage containers",
    "content": ". docker create . docker create [options] IMAGE -a, --attach # attach stdout/err -i, --interactive # attach stdin (interactive) -t, --tty # pseudo-tty --name NAME # name your image -p, --publish 5000:5000 # port map --expose 5432 # expose a port to linked containers -P, --publish-all # publish all ports --link container:alias # linking -v, --volume `pwd`:/app # mount (absolute paths needed) -e, --env NAME=hello # env vars . Example . $ docker create --name app_redis_1 \\ --expose 6379 \\ redis:3.0.2 . Create a container from an image. docker exec . docker exec [options] CONTAINER COMMAND -d, --detach # run in background -i, --interactive # stdin -t, --tty # interactive . Example . $ docker exec app_web_1 tail logs/development.log $ docker exec -t -i app_web_1 rails c . Run commands in a container. docker start . docker start [options] CONTAINER -a, --attach # attach stdout/err -i, --interactive # attach stdin docker stop [options] CONTAINER . Start/stop a container. docker ps . $ docker ps $ docker ps -a $ docker kill $ID . Manage containers using ps/kill. ",
    "url": "/my-tech/docs/Docker#manage-containers",
    
    "relUrl": "/docs/Docker#manage-containers"
  },"17": {
    "doc": "Docker",
    "title": "Images",
    "content": "docker images . $ docker images REPOSITORY TAG ID ubuntu 12.10 abcd1234abcd me/myapp latest 1234abcd1234 . $ docker images -a # also show intermediate . Manages images. docker rmi . docker rmi b750fe78269d . Deletes images. ",
    "url": "/my-tech/docs/Docker#images",
    
    "relUrl": "/docs/Docker#images"
  },"18": {
    "doc": "Docker",
    "title": "Also see",
    "content": ". | Getting Started (docker.io) | . ",
    "url": "/my-tech/docs/Docker#also-see",
    
    "relUrl": "/docs/Docker#also-see"
  },"19": {
    "doc": "Docker",
    "title": "Dockerfile",
    "content": "Inheritance . FROM ruby:2.2.2 . Variables . ENV APP_HOME /myapp RUN mkdir $APP_HOME . Initialization . RUN bundle install . WORKDIR /myapp . VOLUME [\"/data\"] # Specification for mount point . ADD file.xyz /file.xyz COPY --chown=user:group host_file.xyz /path/container_file.xyz . Onbuild . ONBUILD RUN bundle install # when used with another file . Commands . EXPOSE 5900 CMD [\"bundle\", \"exec\", \"rails\", \"server\"] . Entrypoint . ENTRYPOINT [\"executable\", \"param1\", \"param2\"] ENTRYPOINT command param1 param2 . Configures a container that will run as an executable. ENTRYPOINT exec top -b . This will use shell processing to substitute shell variables, and will ignore any CMD or docker run command line arguments. Metadata . LABEL version=\"1.0\" . LABEL \"com.example.vendor\"=\"ACME Incorporated\" LABEL com.example.label-with-value=\"foo\" . LABEL description=\"This text illustrates \\ that label-values can span multiple lines.\" . ",
    "url": "/my-tech/docs/Docker#dockerfile",
    
    "relUrl": "/docs/Docker#dockerfile"
  },"20": {
    "doc": "Docker",
    "title": "See also",
    "content": ". | https://docs.docker.com/engine/reference/builder/ | . ",
    "url": "/my-tech/docs/Docker#see-also",
    
    "relUrl": "/docs/Docker#see-also"
  },"21": {
    "doc": "Docker",
    "title": "docker-compose",
    "content": "Basic example . # docker-compose.yml version: '2' services: web: build: . # build from Dockerfile context: ./Path dockerfile: Dockerfile ports: - \"5000:5000\" volumes: - .:/code redis: image: redis . Commands . docker-compose start docker-compose stop . docker-compose pause docker-compose unpause . docker-compose ps docker-compose up docker-compose down . ",
    "url": "/my-tech/docs/Docker#docker-compose",
    
    "relUrl": "/docs/Docker#docker-compose"
  },"22": {
    "doc": "Docker",
    "title": "Reference",
    "content": "Building . web: # build from Dockerfile build: . # build from custom Dockerfile build: context: ./dir dockerfile: Dockerfile.dev . # build from image image: ubuntu image: ubuntu:14.04 image: tutum/influxdb image: example-registry:4000/postgresql image: a4bc65fd . Ports . ports: - \"3000\" - \"8000:80\" # guest:host . # expose ports to linked services (not to host) expose: [\"3000\"] . Commands . # command to execute command: bundle exec thin -p 3000 command: [bundle, exec, thin, -p, 3000] . # override the entrypoint entrypoint: /app/start.sh entrypoint: [php, -d, vendor/bin/phpunit] . Environment variables . # environment vars environment: RACK_ENV: development environment: - RACK_ENV=development . # environment vars from file env_file: .env env_file: [.env, .development.env] . Dependencies . # makes the `db` service available as the hostname `database` # (implies depends_on) links: - db:database - redis . # make sure `db` is alive before starting depends_on: - db . Other options . # make this service extend another extends: file: common.yml # optional service: webapp . volumes: - /var/lib/mysql - ./_data:/var/lib/mysql . ",
    "url": "/my-tech/docs/Docker#reference",
    
    "relUrl": "/docs/Docker#reference"
  },"23": {
    "doc": "Docker",
    "title": "Advanced features",
    "content": "Labels . services: web: labels: com.example.description: \"Accounting web app\" . DNS servers . services: web: dns: 8.8.8.8 dns: - 8.8.8.8 - 8.8.4.4 . Devices . services: web: devices: - \"/dev/ttyUSB0:/dev/ttyUSB0\" . External links . services: web: external_links: - redis_1 - project_db_1:mysql . Hosts . services: web: extra_hosts: - \"somehost:192.168.1.100\" . sevices . To view list of all the services runnning in swarm . docker service ls . To see all running services . docker stack services stack_name . to see all services logs . docker service logs stack_name service_name . To scale services quickly across qualified node . docker service scale stack_name_service_name=replicas . clean up . To clean or prune unused (dangling) images . docker image prune . To remove all images which are not in use containers , add - a . docker image prune -a . To prune your entire system . docker system prune . To leave swarm . docker swarm leave . To remove swarm ( deletes all volume data and database info) . docker stack rm stack_name . To kill all running containers . docker kill $(docekr ps -q ) . ",
    "url": "/my-tech/docs/Docker#advanced-features",
    
    "relUrl": "/docs/Docker#advanced-features"
  },"24": {
    "doc": "Docker",
    "title": "Docker Security",
    "content": "Docker Scout . Command line tool for Docker Scout: . docker scout . Analyzes a software artifact for vulnerabilities . docker scout cves [OPTIONS] IMAGE|DIRECTORY|ARCHIVE . Display vulnerabilities from a docker save tarball . docker save redis &gt; redis.tar . Display vulnerabilities from an OCI directory . skopeo copy --override-os linux docker://alpine oci:redis . Export vulnerabilities to a SARIF JSON file . docker scout cves --format sarif --output redis.sarif.json redis . Comparing two images . docker scout compare --to redis:6.0 redis:6-bullseye . Displaying the Quick Overview of an Image . docker scout quickview redis:6.0 . ",
    "url": "/my-tech/docs/Docker#docker-security",
    
    "relUrl": "/docs/Docker#docker-security"
  },"25": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/"
  },"26": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "this page describes about a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "content": "Date Posted: 2023 04 16 . ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/"
  },"27": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Tutorial: Creating a Basic Infrastructure on AWS",
    "content": "In this tutorial, we will walk you through the process of setting up a basic infrastructure on AWS. This infrastructure will consist of a Virtual Private Cloud (VPC) with public and private subnets, an Internet Gateway (IGW), Network Address Translation (NAT) gateway, security groups, and instances of Amazon Elastic Compute Cloud (EC2) and Amazon RDS running PostgreSQL. By the end of this tutorial, you will have a better understanding of how to configure these essential components on AWS. Let’s get started! . ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#tutorial-creating-a-basic-infrastructure-on-aws",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#tutorial-creating-a-basic-infrastructure-on-aws"
  },"28": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Table of Contents",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#table-of-contents",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#table-of-contents"
  },"29": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Prerequisites",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#prerequisites",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#prerequisites"
  },"30": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Set Up a VPC",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#set-up-a-vpc",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#set-up-a-vpc"
  },"31": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Create Subnets",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#create-subnets",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#create-subnets"
  },"32": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Configure Routing and Gateways",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#configure-routing-and-gateways",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#configure-routing-and-gateways"
  },"33": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Set Up Security Groups",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#set-up-security-groups",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#set-up-security-groups"
  },"34": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Launch EC2 Instances",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#launch-ec2-instances",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#launch-ec2-instances"
  },"35": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Provision RDS Instance with PostgreSQL",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#provision-rds-instance-with-postgresql",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#provision-rds-instance-with-postgresql"
  },"36": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Summary",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#summary",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#summary"
  },"37": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "# Prerequisites",
    "content": ". | An AWS account with appropriate permissions | Basic knowledge of AWS services and concepts | . ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-prerequisites",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-prerequisites"
  },"38": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "# Set Up a VPC",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-set-up-a-vpc",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-set-up-a-vpc"
  },"39": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Log in to your AWS Management Console.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#log-in-to-your-aws-management-console",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#log-in-to-your-aws-management-console"
  },"40": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Go to the AWS VPC service.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#go-to-the-aws-vpc-service",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#go-to-the-aws-vpc-service"
  },"41": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Click on “Create VPC.”",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-create-vpc",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-create-vpc"
  },"42": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Enter a name for your VPC and specify an IPv4 CIDR block.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#enter-a-name-for-your-vpc-and-specify-an-ipv4-cidr-block",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#enter-a-name-for-your-vpc-and-specify-an-ipv4-cidr-block"
  },"43": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Leave the rest of the settings as default and click on “Create.”",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#leave-the-rest-of-the-settings-as-default-and-click-on-create",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#leave-the-rest-of-the-settings-as-default-and-click-on-create"
  },"44": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "# Create Subnets",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-create-subnets",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-create-subnets"
  },"45": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Within your new VPC, go to the “Subnets” section.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#within-your-new-vpc-go-to-the-subnets-section",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#within-your-new-vpc-go-to-the-subnets-section"
  },"46": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Click on “Create subnet.”",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-create-subnet",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-create-subnet"
  },"47": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Provide a name for your subnet, select your VPC, and specify an IPv4 CIDR block.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#provide-a-name-for-your-subnet-select-your-vpc-and-specify-an-ipv4-cidr-block",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#provide-a-name-for-your-subnet-select-your-vpc-and-specify-an-ipv4-cidr-block"
  },"48": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Repeat these steps to create both a public and private subnet.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#repeat-these-steps-to-create-both-a-public-and-private-subnet",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#repeat-these-steps-to-create-both-a-public-and-private-subnet"
  },"49": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "# Configure Routing and Gateways",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-configure-routing-and-gateways",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-configure-routing-and-gateways"
  },"50": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Go to the “Internet Gateways” section within VPC.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#go-to-the-internet-gateways-section-within-vpc",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#go-to-the-internet-gateways-section-within-vpc"
  },"51": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Click on “Create internet gateway.”",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-create-internet-gateway",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-create-internet-gateway"
  },"52": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Attach the internet gateway to your VPC.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#attach-the-internet-gateway-to-your-vpc",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#attach-the-internet-gateway-to-your-vpc"
  },"53": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Navigate to the “Route Tables” section and select your VPC’s main route table.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#navigate-to-the-route-tables-section-and-select-your-vpcs-main-route-table",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#navigate-to-the-route-tables-section-and-select-your-vpcs-main-route-table"
  },"54": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Edit the route table by adding a route 0.0.0.0/0 to your internet gateway.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#edit-the-route-table-by-adding-a-route-00000-to-your-internet-gateway",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#edit-the-route-table-by-adding-a-route-00000-to-your-internet-gateway"
  },"55": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "# Set Up Security Groups",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-set-up-security-groups",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-set-up-security-groups"
  },"56": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Move to the EC2 service within your chosen region.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#move-to-the-ec2-service-within-your-chosen-region",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#move-to-the-ec2-service-within-your-chosen-region"
  },"57": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Go to the “Security Groups” section in the left-hand menu.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#go-to-the-security-groups-section-in-the-left-hand-menu",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#go-to-the-security-groups-section-in-the-left-hand-menu"
  },"58": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Create a new security group for your EC2 instances, allowing inbound SSH (port 22) access from your IP.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#create-a-new-security-group-for-your-ec2-instances-allowing-inbound-ssh-port-22-access-from-your-ip",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#create-a-new-security-group-for-your-ec2-instances-allowing-inbound-ssh-port-22-access-from-your-ip"
  },"59": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Create another security group for your RDS instance, allowing inbound PostgreSQL (port 5432) access from your EC2 security group.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#create-another-security-group-for-your-rds-instance-allowing-inbound-postgresql-port-5432-access-from-your-ec2-security-group",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#create-another-security-group-for-your-rds-instance-allowing-inbound-postgresql-port-5432-access-from-your-ec2-security-group"
  },"60": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "# Launch EC2 Instances",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-launch-ec2-instances",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-launch-ec2-instances"
  },"61": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Within the EC2 service, navigate to “Instances” in the left-hand menu.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#within-the-ec2-service-navigate-to-instances-in-the-left-hand-menu",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#within-the-ec2-service-navigate-to-instances-in-the-left-hand-menu"
  },"62": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Click on “Launch Instances” and choose an Amazon Machine Image (AMI) of your preference.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-launch-instances-and-choose-an-amazon-machine-image-ami-of-your-preference",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-launch-instances-and-choose-an-amazon-machine-image-ami-of-your-preference"
  },"63": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Select your VPC and public subnet.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#select-your-vpc-and-public-subnet",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#select-your-vpc-and-public-subnet"
  },"64": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Configure the EC2 instance as per your needs (instance type, storage, security group, etc.).",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#configure-the-ec2-instance-as-per-your-needs-instance-type-storage-security-group-etc",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#configure-the-ec2-instance-as-per-your-needs-instance-type-storage-security-group-etc"
  },"65": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Launch the instance and repeat these steps to launch a second instance in your private subnet.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#launch-the-instance-and-repeat-these-steps-to-launch-a-second-instance-in-your-private-subnet",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#launch-the-instance-and-repeat-these-steps-to-launch-a-second-instance-in-your-private-subnet"
  },"66": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "# Provision RDS Instance with PostgreSQL",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-provision-rds-instance-with-postgresql",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-provision-rds-instance-with-postgresql"
  },"67": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Go to the Amazon RDS service.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#go-to-the-amazon-rds-service",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#go-to-the-amazon-rds-service"
  },"68": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Click on “Create database” and select PostgreSQL as the engine.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-create-database-and-select-postgresql-as-the-engine",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#click-on-create-database-and-select-postgresql-as-the-engine"
  },"69": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Specify the details for your RDS instance (instance size, storage, username, etc.).",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#specify-the-details-for-your-rds-instance-instance-size-storage-username-etc",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#specify-the-details-for-your-rds-instance-instance-size-storage-username-etc"
  },"70": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Choose your VPC, private subnet, and security group created for RDS.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#choose-your-vpc-private-subnet-and-security-group-created-for-rds",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#choose-your-vpc-private-subnet-and-security-group-created-for-rds"
  },"71": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "Complete the remaining steps to provision your RDS instance.",
    "content": " ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#complete-the-remaining-steps-to-provision-your-rds-instance",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#complete-the-remaining-steps-to-provision-your-rds-instance"
  },"72": {
    "doc": "a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql",
    "title": "# Summary",
    "content": "Congratulations! You have successfully created a basic infrastructure on AWS. You have set up a VPC with public and private subnets, configured routing and gateways, established security groups, and launched EC2 instances and an RDS instance running PostgreSQL. ",
    "url": "/my-tech/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-summary",
    
    "relUrl": "/doc/Misc/a-tutorial-about-creating-an-AWS-basic-infrastructure,-with-VPC,-publicc,-and-private-subnets,-IGW,-NAT,-security-groups,-EC2-and-RDS-that-runs-postgresql/#-summary"
  },"73": {
    "doc": "Create a host, ssh into the public EC2 instnace",
    "title": "Create a host, ssh into the public EC2 instnace",
    "content": " ",
    "url": "/my-tech/doc/Ansible/ansible-1/",
    
    "relUrl": "/doc/Ansible/ansible-1/"
  },"74": {
    "doc": "Create a host, ssh into the public EC2 instnace",
    "title": "",
    "content": "Date Posted: 2023 01 12 . Playbook to ssh into an EC2 instance - name: SSH into EC2 instances and echo \"hello world\" hosts: public remote_user: ec2-user become: true vars: ansible_ssh_private_key_file: \"Key file\" tasks: - name: Run \"echo hello world\" command command: echo \"hello world\" register: result - name: Display the output debug: var: result.stdout . ",
    "url": "/my-tech/doc/Ansible/ansible-1/",
    
    "relUrl": "/doc/Ansible/ansible-1/"
  },"75": {
    "doc": "Create a host, ssh into the public EC2 instnace",
    "title": "Test",
    "content": "# Store hosts file cat &lt;&lt; EOF &gt; /etc/ansible/hosts [public] 123.123.123.123 EOF # Run a playbook ansible-playbook playbook.yaml . ",
    "url": "/my-tech/doc/Ansible/ansible-1/#test",
    
    "relUrl": "/doc/Ansible/ansible-1/#test"
  },"76": {
    "doc": "CICD",
    "title": "CICD",
    "content": " ",
    "url": "/my-tech/docs/CICD",
    
    "relUrl": "/docs/CICD"
  },"77": {
    "doc": "CICD",
    "title": "",
    "content": "Date Posted: 2023 03 01 . ",
    "url": "/my-tech/docs/CICD",
    
    "relUrl": "/docs/CICD"
  },"78": {
    "doc": "CICD",
    "title": "Tutorial: Deploy self-hosted CI/CD runners and agents with Azure Container Apps jobs",
    "content": "GitHub Actions and Azure Pipelines allow you to run CI/CD workflows with self-hosted runners and agents. You can run self-hosted runners and agents using event-driven Azure Container Apps jobs. Self-hosted runners are useful when you need to run workflows that require access to local resources or tools that aren’t available to a cloud-hosted runner. For example, a self-hosted runner in a Container Apps job allows your workflow to access resources inside the job’s virtual network that isn’t accessible to a cloud-hosted runner. Running self-hosted runners as event-driven jobs allows you to take advantage of the serverless nature of Azure Container Apps. Jobs execute automatically when a workflow is triggered and exit when the job completes. You only pay for the time that the job is running. ::: zone pivot=”container-apps-jobs-self-hosted-ci-cd-github-actions” . In this tutorial, you learn how to run GitHub Actions runners as an event-driven Container Apps job. [!div class=”checklist”] . | Create a Container Apps environment to deploy your self-hosted runner | Create a GitHub repository for running a workflow that uses a self-hosted runner | Build a container image that runs a GitHub Actions runner | Deploy the runner as a job to the Container Apps environment | Create a workflow that uses the self-hosted runner and verify that it runs | . [!IMPORTANT] Self-hosted runners are only recommended for private repositories. Using them with public repositories can allow dangerous code to execute on your self-hosted runner. For more information, see Self-hosted runner security. ::: zone-end . ::: zone pivot=”container-apps-jobs-self-hosted-ci-cd-azure-pipelines” . In this tutorial, you learn how to run Azure Pipelines agents as an event-driven Container Apps job. [!div class=”checklist”] . | Create a Container Apps environment to deploy your self-hosted agent | Create an Azure DevOps organization and project | Build a container image that runs an Azure Pipelines agent | Use a manual job to create a placeholder agent in the Container Apps environment | Deploy the agent as a job to the Container Apps environment | Create a pipeline that uses the self-hosted agent and verify that it runs | . [!IMPORTANT] Self-hosted agents are only recommended for private projects. Using them with public projects can allow dangerous code to execute on your self-hosted agent. For more information, see Self-hosted agent security. ::: zone-end . [!NOTE] Container apps and jobs don’t support running Docker in containers. Any steps in your workflows that use Docker commands will fail when run on a self-hosted runner or agent in a Container Apps job. ",
    "url": "/my-tech/docs/CICD#tutorial-deploy-self-hosted-cicd-runners-and-agents-with-azure-container-apps-jobs",
    
    "relUrl": "/docs/CICD#tutorial-deploy-self-hosted-cicd-runners-and-agents-with-azure-container-apps-jobs"
  },"79": {
    "doc": "CICD",
    "title": "Prerequisites",
    "content": ". | Azure account: If you don’t have one, you can create one for free. | Azure CLI: Install the Azure CLI. ::: zone pivot=”container-apps-jobs-self-hosted-ci-cd-azure-pipelines” | Azure DevOps organization: If you don’t have a DevOps organization with an active subscription, you can create one for free. ::: zone-end | . Refer to jobs preview limitations for a list of limitations. ",
    "url": "/my-tech/docs/CICD#prerequisites",
    
    "relUrl": "/docs/CICD#prerequisites"
  },"80": {
    "doc": "CICD",
    "title": "Setup",
    "content": " ",
    "url": "/my-tech/docs/CICD#setup",
    
    "relUrl": "/docs/CICD#setup"
  },"81": {
    "doc": "CICD",
    "title": "To sign in to Azure from the CLI, run the following command and follow the prompts to complete the authentication process.",
    "content": "- Bash ```bash az login ``` - PowerShell ```powershell az login ``` --- . ",
    "url": "/my-tech/docs/CICD#to-sign-in-to-azure-from-the-cli-run-the-following-command-and-follow-the-prompts-to-complete-the-authentication-process",
    
    "relUrl": "/docs/CICD#to-sign-in-to-azure-from-the-cli-run-the-following-command-and-follow-the-prompts-to-complete-the-authentication-process"
  },"82": {
    "doc": "CICD",
    "title": "Ensure you’re running the latest version of the CLI via the upgrade command.",
    "content": "- Bash ```bash az upgrade ``` - PowerShell ```powershell az upgrade ``` --- . ",
    "url": "/my-tech/docs/CICD#ensure-youre-running-the-latest-version-of-the-cli-via-the-upgrade-command",
    
    "relUrl": "/docs/CICD#ensure-youre-running-the-latest-version-of-the-cli-via-the-upgrade-command"
  },"83": {
    "doc": "CICD",
    "title": "Install the latest version of the Azure Container Apps CLI extension.",
    "content": "- Bash ```bash az extension add --name containerapp --upgrade ``` - PowerShell ```powershell az extension add --name containerapp --upgrade ``` --- . ",
    "url": "/my-tech/docs/CICD#install-the-latest-version-of-the-azure-container-apps-cli-extension",
    
    "relUrl": "/docs/CICD#install-the-latest-version-of-the-azure-container-apps-cli-extension"
  },"84": {
    "doc": "CICD",
    "title": "Register the Microsoft.App and Microsoft.OperationalInsights namespaces if you haven’t already registered them in your Azure subscription.",
    "content": "- Bash ```bash az provider register --namespace Microsoft.App az provider register --namespace Microsoft.OperationalInsights ``` - PowerShell ```powershell az provider register --namespace Microsoft.App az provider register --namespace Microsoft.OperationalInsights ``` --- . ",
    "url": "/my-tech/docs/CICD#register-the-microsoftapp-and-microsoftoperationalinsights-namespaces-if-you-havent-already-registered-them-in-your-azure-subscription",
    
    "relUrl": "/docs/CICD#register-the-microsoftapp-and-microsoftoperationalinsights-namespaces-if-you-havent-already-registered-them-in-your-azure-subscription"
  },"85": {
    "doc": "CICD",
    "title": "Define the environment variables that are used throughout this article.",
    "content": "::: zone pivot=\"container-apps-jobs-self-hosted-ci-cd-github-actions\" - Bash ```bash RESOURCE_GROUP=\"jobs-sample\" LOCATION=\"northcentralus\" ENVIRONMENT=\"env-jobs-sample\" JOB_NAME=\"github-actions-runner-job\" ``` - PowerShell ```powershell $RESOURCE_GROUP=\"jobs-sample\" $LOCATION=\"northcentralus\" $ENVIRONMENT=\"env-jobs-sample\" $JOB_NAME=\"github-actions-runner-job\" ``` --- ::: zone-end ::: zone pivot=\"container-apps-jobs-self-hosted-ci-cd-azure-pipelines\" - Bash ```bash RESOURCE_GROUP=\"jobs-sample\" LOCATION=\"northcentralus\" ENVIRONMENT=\"env-jobs-sample\" JOB_NAME=\"azure-pipelines-agent-job\" PLACEHOLDER_JOB_NAME=\"placeholder-agent-job\" ``` - PowerShell ```powershell $RESOURCE_GROUP=\"jobs-sample\" $LOCATION=\"northcentralus\" $ENVIRONMENT=\"env-jobs-sample\" $JOB_NAME=\"azure-pipelines-agent-job\" $PLACEHOLDER_JOB_NAME=\"placeholder-agent-job\" ``` --- ::: zone-end . ",
    "url": "/my-tech/docs/CICD#define-the-environment-variables-that-are-used-throughout-this-article",
    
    "relUrl": "/docs/CICD#define-the-environment-variables-that-are-used-throughout-this-article"
  },"86": {
    "doc": "CICD",
    "title": "Create a Container Apps environment",
    "content": "The Azure Container Apps environment acts as a secure boundary around container apps and jobs so they can share the same network and communicate with each other. [!NOTE] To create a Container Apps environment that’s integrated with an existing virtual network, see Provide a virtual network to an internal Azure Container Apps environment. ",
    "url": "/my-tech/docs/CICD#create-a-container-apps-environment",
    
    "relUrl": "/docs/CICD#create-a-container-apps-environment"
  },"87": {
    "doc": "CICD",
    "title": "Create a resource group using the following command.",
    "content": "- Bash ```bash az group create \\ --name \"$RESOURCE_GROUP\" \\ --location \"$LOCATION\" ``` - PowerShell ```powershell az group create ` --name \"$RESOURCE_GROUP\" ` --location \"$LOCATION\" ``` --- . ",
    "url": "/my-tech/docs/CICD#create-a-resource-group-using-the-following-command",
    
    "relUrl": "/docs/CICD#create-a-resource-group-using-the-following-command"
  },"88": {
    "doc": "CICD",
    "title": "Create the Container Apps environment using the following command.",
    "content": "- Bash ```bash az containerapp env create \\ --name \"$ENVIRONMENT\" \\ --resource-group \"$RESOURCE_GROUP\" \\ --location \"$LOCATION\" ``` - PowerShell ```powershell az containerapp env create ` --name \"$ENVIRONMENT\" ` --resource-group \"$RESOURCE_GROUP\" ` --location \"$LOCATION\" ``` --- . ::: zone pivot=”container-apps-jobs-self-hosted-ci-cd-github-actions” . ",
    "url": "/my-tech/docs/CICD#create-the-container-apps-environment-using-the-following-command",
    
    "relUrl": "/docs/CICD#create-the-container-apps-environment-using-the-following-command"
  },"89": {
    "doc": "CICD",
    "title": "Create a GitHub repository for running a workflow",
    "content": "To execute a workflow, you need to create a GitHub repository that contains the workflow definition. ",
    "url": "/my-tech/docs/CICD#create-a-github-repository-for-running-a-workflow",
    
    "relUrl": "/docs/CICD#create-a-github-repository-for-running-a-workflow"
  },"90": {
    "doc": "CICD",
    "title": "Navigate to GitHub and sign in.",
    "content": " ",
    "url": "/my-tech/docs/CICD#navigate-to-github-and-sign-in",
    
    "relUrl": "/docs/CICD#navigate-to-github-and-sign-in"
  },"91": {
    "doc": "CICD",
    "title": "Create a new repository by entering the following values.",
    "content": "| Setting | Value |---|---| Owner | Select your GitHub username. | Repository name | Enter a name for your repository. | Visibility | Select **Private**. | Initialize this repository with | Select **Add a README file**. | Leave the rest of the values as their default selection. ",
    "url": "/my-tech/docs/CICD#create-a-new-repository-by-entering-the-following-values",
    
    "relUrl": "/docs/CICD#create-a-new-repository-by-entering-the-following-values"
  },"92": {
    "doc": "CICD",
    "title": "Select Create repository.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-create-repository",
    
    "relUrl": "/docs/CICD#select-create-repository"
  },"93": {
    "doc": "CICD",
    "title": "In your new repository, select Actions.",
    "content": " ",
    "url": "/my-tech/docs/CICD#in-your-new-repository-select-actions",
    
    "relUrl": "/docs/CICD#in-your-new-repository-select-actions"
  },"94": {
    "doc": "CICD",
    "title": "Search for the Simple workflow template and select Configure.",
    "content": " ",
    "url": "/my-tech/docs/CICD#search-for-the-simple-workflow-template-and-select-configure",
    
    "relUrl": "/docs/CICD#search-for-the-simple-workflow-template-and-select-configure"
  },"95": {
    "doc": "CICD",
    "title": "Select Commit changes to add the workflow to your repository.",
    "content": "The workflow runs on the ubuntu-latest GitHub-hosted runner and prints a message to the console. Later, you replace the GitHub-hosted runner with a self-hosted runner. ",
    "url": "/my-tech/docs/CICD#select-commit-changes-to-add-the-workflow-to-your-repository",
    
    "relUrl": "/docs/CICD#select-commit-changes-to-add-the-workflow-to-your-repository"
  },"96": {
    "doc": "CICD",
    "title": "Get a GitHub personal access token",
    "content": "To run a self-hosted runner, you need to create a personal access token (PAT) in GitHub. Each time a runner starts, the PAT is used to generate a token to register the runner with GitHub. The PAT is also used by the GitHub Actions runner scale rule to monitor the repository’s workflow queue and start runners as needed. ",
    "url": "/my-tech/docs/CICD#get-a-github-personal-access-token",
    
    "relUrl": "/docs/CICD#get-a-github-personal-access-token"
  },"97": {
    "doc": "CICD",
    "title": "In GitHub, select your profile picture in the upper-right corner and select Settings.",
    "content": " ",
    "url": "/my-tech/docs/CICD#in-github-select-your-profile-picture-in-the-upper-right-corner-and-select-settings",
    
    "relUrl": "/docs/CICD#in-github-select-your-profile-picture-in-the-upper-right-corner-and-select-settings"
  },"98": {
    "doc": "CICD",
    "title": "Select Developer settings.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-developer-settings",
    
    "relUrl": "/docs/CICD#select-developer-settings"
  },"99": {
    "doc": "CICD",
    "title": "Under Personal access tokens, select Fine-grained tokens.",
    "content": " ",
    "url": "/my-tech/docs/CICD#under-personal-access-tokens-select-fine-grained-tokens",
    
    "relUrl": "/docs/CICD#under-personal-access-tokens-select-fine-grained-tokens"
  },"100": {
    "doc": "CICD",
    "title": "Select Generate new token.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-generate-new-token",
    
    "relUrl": "/docs/CICD#select-generate-new-token"
  },"101": {
    "doc": "CICD",
    "title": "In the New fine-grained personal access token screen, enter the following values.",
    "content": "| Setting | Value |---|---| Token name | Enter a name for your token. | Expiration | Select **30 days**. | Repository access | Select **Only select repositories** and select the repository you created. | Enter the following values for *Repository permissions*. | Setting | Value |---|---| Actions | Select **Read-only**. | Administration | Select **Read and write**. | Metadata | Select **Read-only**. | . ",
    "url": "/my-tech/docs/CICD#in-the-new-fine-grained-personal-access-token-screen-enter-the-following-values",
    
    "relUrl": "/docs/CICD#in-the-new-fine-grained-personal-access-token-screen-enter-the-following-values"
  },"102": {
    "doc": "CICD",
    "title": "Select Generate token.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-generate-token",
    
    "relUrl": "/docs/CICD#select-generate-token"
  },"103": {
    "doc": "CICD",
    "title": "Copy the token value.",
    "content": " ",
    "url": "/my-tech/docs/CICD#copy-the-token-value",
    
    "relUrl": "/docs/CICD#copy-the-token-value"
  },"104": {
    "doc": "CICD",
    "title": "Define variables that are used to configure the runner and scale rule later.",
    "content": "- Bash ```bash GITHUB_PAT=\"&lt;GITHUB_PAT&gt;\" REPO_OWNER=\"&lt;REPO_OWNER&gt;\" REPO_NAME=\"&lt;REPO_NAME&gt;\" ``` - PowerShell ```powershell $GITHUB_PAT=\"&lt;GITHUB_PAT&gt;\" $REPO_OWNER=\"&lt;REPO_OWNER&gt;\" $REPO_NAME=\"&lt;REPO_NAME&gt;\" ``` --- Replace the placeholders with the following values: | Placeholder | Value |---|---| `&lt;GITHUB_PAT&gt;` | The GitHub PAT you generated. | `&lt;REPO_OWNER&gt;` | The owner of the repository you created earlier. This value is usually your GitHub username. | `&lt;REPO_NAME&gt;` | The name of the repository you created earlier. This value is the same name you entered in the *Repository name* field. | . ",
    "url": "/my-tech/docs/CICD#define-variables-that-are-used-to-configure-the-runner-and-scale-rule-later",
    
    "relUrl": "/docs/CICD#define-variables-that-are-used-to-configure-the-runner-and-scale-rule-later"
  },"105": {
    "doc": "CICD",
    "title": "Build the GitHub Actions runner container image",
    "content": "To create a self-hosted runner, you need to build a container image that executes the runner. In this section, you build the container image and push it to a container registry. [!NOTE] The image you build in this tutorial contains a basic self-hosted runner that’s suitable for running as a Container Apps job. You can customize it to include additional tools or dependencies that your workflows require. ",
    "url": "/my-tech/docs/CICD#build-the-github-actions-runner-container-image",
    
    "relUrl": "/docs/CICD#build-the-github-actions-runner-container-image"
  },"106": {
    "doc": "CICD",
    "title": "Define a name for your container image and registry.",
    "content": "- Bash ```bash CONTAINER_IMAGE_NAME=\"github-actions-runner:1.0\" CONTAINER_REGISTRY_NAME=\"&lt;CONTAINER_REGISTRY_NAME&gt;\" ``` - PowerShell ```powershell $CONTAINER_IMAGE_NAME=\"github-actions-runner:1.0\" $CONTAINER_REGISTRY_NAME=\"&lt;CONTAINER_REGISTRY_NAME&gt;\" ``` --- Replace `&lt;CONTAINER_REGISTRY_NAME&gt;` with a unique name for creating a container registry. Container registry names must be *unique within Azure* and be from 5 to 50 characters in length containing numbers and lowercase letters only. ",
    "url": "/my-tech/docs/CICD#define-a-name-for-your-container-image-and-registry",
    
    "relUrl": "/docs/CICD#define-a-name-for-your-container-image-and-registry"
  },"107": {
    "doc": "CICD",
    "title": "Create a container registry.",
    "content": "- Bash ```bash az acr create \\ --name \"$CONTAINER_REGISTRY_NAME\" \\ --resource-group \"$RESOURCE_GROUP\" \\ --location \"$LOCATION\" \\ --sku Basic \\ --admin-enabled true ``` - PowerShell ```powershell az acr create ` --name \"$CONTAINER_REGISTRY_NAME\" ` --resource-group \"$RESOURCE_GROUP\" ` --location \"$LOCATION\" ` --sku Basic ` --admin-enabled true ``` --- . ",
    "url": "/my-tech/docs/CICD#create-a-container-registry",
    
    "relUrl": "/docs/CICD#create-a-container-registry"
  },"108": {
    "doc": "CICD",
    "title": "The Dockerfile for creating the runner image is available on GitHub. Run the following command to clone the repository and build the container image in the cloud using the az acr build command.",
    "content": "- Bash ```bash az acr build \\ --registry \"$CONTAINER_REGISTRY_NAME\" \\ --image \"$CONTAINER_IMAGE_NAME\" \\ --file \"Dockerfile.github\" \\ \"https://github.com/Azure-Samples/container-apps-ci-cd-runner-tutorial.git\" ``` - PowerShell ```powershell az acr build ` --registry \"$CONTAINER_REGISTRY_NAME\" ` --image \"$CONTAINER_IMAGE_NAME\" ` --file \"Dockerfile.github\" ` \"https://github.com/Azure-Samples/container-apps-ci-cd-runner-tutorial.git\" ``` --- The image is now available in the container registry. ",
    "url": "/my-tech/docs/CICD#the-dockerfile-for-creating-the-runner-image-is-available-on-github-run-the-following-command-to-clone-the-repository-and-build-the-container-image-in-the-cloud-using-the-az-acr-build-command",
    
    "relUrl": "/docs/CICD#the-dockerfile-for-creating-the-runner-image-is-available-on-github-run-the-following-command-to-clone-the-repository-and-build-the-container-image-in-the-cloud-using-the-az-acr-build-command"
  },"109": {
    "doc": "CICD",
    "title": "Deploy a self-hosted runner as a job",
    "content": "You can now create a job that uses to use the container image. In this section, you create a job that executes the self-hosted runner and authenticates with GitHub using the PAT you generated earlier. The job uses the github-runner scale rule to create job executions based on the number of pending workflow runs. ",
    "url": "/my-tech/docs/CICD#deploy-a-self-hosted-runner-as-a-job",
    
    "relUrl": "/docs/CICD#deploy-a-self-hosted-runner-as-a-job"
  },"110": {
    "doc": "CICD",
    "title": "Create a job in the Container Apps environment.",
    "content": "- Bash ```bash az containerapp job create -n \"$JOB_NAME\" -g \"$RESOURCE_GROUP\" --environment \"$ENVIRONMENT\" \\ --trigger-type Event \\ --replica-timeout 1800 \\ --replica-retry-limit 1 \\ --replica-completion-count 1 \\ --parallelism 1 \\ --image \"$CONTAINER_REGISTRY_NAME.azurecr.io/$CONTAINER_IMAGE_NAME\" \\ --min-executions 0 \\ --max-executions 10 \\ --polling-interval 30 \\ --scale-rule-name \"github-runner\" \\ --scale-rule-type \"github-runner\" \\ --scale-rule-metadata \"github-runner=https://api.github.com\" \"owner=$REPO_OWNER\" \"runnerScope=repo\" \"repos=$REPO_NAME\" \"targetWorkflowQueueLength=1\" \\ --scale-rule-auth \"personalAccessToken=personal-access-token\" \\ --cpu \"2.0\" \\ --memory \"4Gi\" \\ --secrets \"personal-access-token=$GITHUB_PAT\" \\ --env-vars \"GITHUB_PAT=secretref:personal-access-token\" \"REPO_URL=https://github.com/$REPO_OWNER/$REPO_NAME\" \"REGISTRATION_TOKEN_API_URL=https://api.github.com/repos/$REPO_OWNER/$REPO_NAME/actions/runners/registration-token\" \\ --registry-server \"$CONTAINER_REGISTRY_NAME.azurecr.io\" ``` - PowerShell ```powershell az containerapp job create -n \"$JOB_NAME\" -g \"$RESOURCE_GROUP\" --environment \"$ENVIRONMENT\" ` --trigger-type Event ` --replica-timeout 1800 ` --replica-retry-limit 1 ` --replica-completion-count 1 ` --parallelism 1 ` --image \"$CONTAINER_REGISTRY_NAME.azurecr.io/$CONTAINER_IMAGE_NAME\" ` --min-executions 0 ` --max-executions 10 ` --polling-interval 30 ` --scale-rule-name \"github-runner\" ` --scale-rule-type \"github-runner\" ` --scale-rule-metadata \"github-runner=https://api.github.com\" \"owner=$REPO_OWNER\" \"runnerScope=repo\" \"repos=$REPO_NAME\" \"targetWorkflowQueueLength=1\" ` --scale-rule-auth \"personalAccessToken=personal-access-token\" ` --cpu \"2.0\" ` --memory \"4Gi\" ` --secrets \"personal-access-token=$GITHUB_PAT\" ` --env-vars \"GITHUB_PAT=secretref:personal-access-token\" \"REPO_URL=https://github.com/$REPO_OWNER/$REPO_NAME\" \"REGISTRATION_TOKEN_API_URL=https://api.github.com/repos/$REPO_OWNER/$REPO_NAME/actions/runners/registration-token\" ` --registry-server \"$CONTAINER_REGISTRY_NAME.azurecr.io\" ``` --- The following table describes the key parameters used in the command. | Parameter | Description | --- | --- | `--replica-timeout` | The maximum duration a replica can execute. | `--replica-retry-limit` | The number of times to retry a failed replica. | `--replica-completion-count` | The number of replicas to complete successfully before a job execution is considered successful. | `--parallelism` | The number of replicas to start per job execution. | `--min-executions` | The minimum number of job executions to run per polling interval. | `--max-executions` | The maximum number of job executions to run per polling interval. | `--polling-interval` | The polling interval at which to evaluate the scale rule. | `--scale-rule-name` | The name of the scale rule. | `--scale-rule-type` | The type of scale rule to use. To learn more about the GitHub runner scaler, see the KEDA [documentation](https://keda.sh/docs/latest/scalers/github-runner/). | `--scale-rule-metadata` | The metadata for the scale rule. | `--scale-rule-auth` | The authentication for the scale rule. | `--secrets` | The secrets to use for the job. | `--env-vars` | The environment variables to use for the job. | `--registry-server` | The container registry server to use for the job. For an Azure Container Registry, the command automatically configures authentication. | The scale rule configuration defines the event source to monitor. It's evaluated on each polling interval and determines how many job executions to trigger. To learn more, see [Set scaling rules](scale-app.md). The event-driven job is now created in the Container Apps environment. ",
    "url": "/my-tech/docs/CICD#create-a-job-in-the-container-apps-environment",
    
    "relUrl": "/docs/CICD#create-a-job-in-the-container-apps-environment"
  },"111": {
    "doc": "CICD",
    "title": "Run a workflow and verify the job",
    "content": "The job is configured to evaluate the scale rule every 30 seconds. During each evaluation, it checks the number of pending workflow runs that require a self-hosted runner and starts a new job execution for pending workflow, up to a configured maximum of 10 executions. To verify the job was configured correctly, you modify the workflow to use a self-hosted runner and trigger a workflow run. You can then view the job execution logs to see the workflow run. ",
    "url": "/my-tech/docs/CICD#run-a-workflow-and-verify-the-job",
    
    "relUrl": "/docs/CICD#run-a-workflow-and-verify-the-job"
  },"112": {
    "doc": "CICD",
    "title": "In the GitHub repository, navigate to the workflow you generated earlier. It’s a YAML file in the .github/workflows directory.",
    "content": " ",
    "url": "/my-tech/docs/CICD#in-the-github-repository-navigate-to-the-workflow-you-generated-earlier-its-a-yaml-file-in-the-githubworkflows-directory",
    
    "relUrl": "/docs/CICD#in-the-github-repository-navigate-to-the-workflow-you-generated-earlier-its-a-yaml-file-in-the-githubworkflows-directory"
  },"113": {
    "doc": "CICD",
    "title": "Select Edit in place.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-edit-in-place",
    
    "relUrl": "/docs/CICD#select-edit-in-place"
  },"114": {
    "doc": "CICD",
    "title": "Update the runs-on property to self-hosted:",
    "content": "```yaml runs-on: self-hosted ``` . ",
    "url": "/my-tech/docs/CICD#update-the-runs-on-property-to-self-hosted",
    
    "relUrl": "/docs/CICD#update-the-runs-on-property-to-self-hosted"
  },"115": {
    "doc": "CICD",
    "title": "Select Commit changes….",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-commit-changes",
    
    "relUrl": "/docs/CICD#select-commit-changes"
  },"116": {
    "doc": "CICD",
    "title": "Select Commit changes.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-commit-changes-1",
    
    "relUrl": "/docs/CICD#select-commit-changes-1"
  },"117": {
    "doc": "CICD",
    "title": "Navigate to the Actions tab.",
    "content": "A new workflow is now queued. Within 30 seconds, the job execution will start and the workflow will complete soon after. Wait for the action to complete before going on the next step. ",
    "url": "/my-tech/docs/CICD#navigate-to-the-actions-tab",
    
    "relUrl": "/docs/CICD#navigate-to-the-actions-tab"
  },"118": {
    "doc": "CICD",
    "title": "List the executions of the job to confirm a job execution was created and completed successfully.",
    "content": "- Bash ```bash az containerapp job execution list \\ --name \"$JOB_NAME\" \\ --resource-group \"$RESOURCE_GROUP\" \\ --output table \\ --query '[].{Status: properties.status, Name: name, StartTime: properties.startTime}' ``` - PowerShell ```powershell az containerapp job execution list ` --name \"$JOB_NAME\" ` --resource-group \"$RESOURCE_GROUP\" ` --output table ` --query '[].{Status: properties.status, Name: name, StartTime: properties.startTime}' ``` --- . ::: zone-end . ::: zone pivot=”container-apps-jobs-self-hosted-ci-cd-azure-pipelines” . ",
    "url": "/my-tech/docs/CICD#list-the-executions-of-the-job-to-confirm-a-job-execution-was-created-and-completed-successfully",
    
    "relUrl": "/docs/CICD#list-the-executions-of-the-job-to-confirm-a-job-execution-was-created-and-completed-successfully"
  },"119": {
    "doc": "CICD",
    "title": "Create an Azure DevOps project and repository",
    "content": "To execute a pipeline, you need an Azure DevOps project and repository. ",
    "url": "/my-tech/docs/CICD#create-an-azure-devops-project-and-repository",
    
    "relUrl": "/docs/CICD#create-an-azure-devops-project-and-repository"
  },"120": {
    "doc": "CICD",
    "title": "Navigate to Azure DevOps and sign in to your account.",
    "content": " ",
    "url": "/my-tech/docs/CICD#navigate-to-azure-devops-and-sign-in-to-your-account",
    
    "relUrl": "/docs/CICD#navigate-to-azure-devops-and-sign-in-to-your-account"
  },"121": {
    "doc": "CICD",
    "title": "Select an existing organization or create a new one.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-an-existing-organization-or-create-a-new-one",
    
    "relUrl": "/docs/CICD#select-an-existing-organization-or-create-a-new-one"
  },"122": {
    "doc": "CICD",
    "title": "In the organization overview page, select New project and enter the following values.",
    "content": "| Setting | Value |---|---| *Project name* | Enter a name for your project. | *Visibility* | Select **Private**. | . ",
    "url": "/my-tech/docs/CICD#in-the-organization-overview-page-select-new-project-and-enter-the-following-values",
    
    "relUrl": "/docs/CICD#in-the-organization-overview-page-select-new-project-and-enter-the-following-values"
  },"123": {
    "doc": "CICD",
    "title": "Select Create.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-create",
    
    "relUrl": "/docs/CICD#select-create"
  },"124": {
    "doc": "CICD",
    "title": "From the side navigation, select Repos.",
    "content": " ",
    "url": "/my-tech/docs/CICD#from-the-side-navigation-select-repos",
    
    "relUrl": "/docs/CICD#from-the-side-navigation-select-repos"
  },"125": {
    "doc": "CICD",
    "title": "Under Initialize main branch with a README or .gitignore, select Add a README.",
    "content": " ",
    "url": "/my-tech/docs/CICD#under-initialize-main-branch-with-a-readme-or-gitignore-select-add-a-readme",
    
    "relUrl": "/docs/CICD#under-initialize-main-branch-with-a-readme-or-gitignore-select-add-a-readme"
  },"126": {
    "doc": "CICD",
    "title": "Leave the rest of the values as defaults and select Initialize.",
    "content": " ",
    "url": "/my-tech/docs/CICD#leave-the-rest-of-the-values-as-defaults-and-select-initialize",
    
    "relUrl": "/docs/CICD#leave-the-rest-of-the-values-as-defaults-and-select-initialize"
  },"127": {
    "doc": "CICD",
    "title": "Create a new agent pool",
    "content": "Create a new agent pool to run the self-hosted runner. ",
    "url": "/my-tech/docs/CICD#create-a-new-agent-pool",
    
    "relUrl": "/docs/CICD#create-a-new-agent-pool"
  },"128": {
    "doc": "CICD",
    "title": "In your Azure DevOps project, expand the left navigation bar and select Project settings.",
    "content": ":::image type=\"content\" source=\"media/runners/azure-devops-project-settings.png\" alt-text=\"Screenshot of the Azure DevOps project settings button.\"::: . ",
    "url": "/my-tech/docs/CICD#in-your-azure-devops-project-expand-the-left-navigation-bar-and-select-project-settings",
    
    "relUrl": "/docs/CICD#in-your-azure-devops-project-expand-the-left-navigation-bar-and-select-project-settings"
  },"129": {
    "doc": "CICD",
    "title": "Under the Pipelines section in the Project settings navigation menu, select Agent pools.",
    "content": ":::image type=\"content\" source=\"media/runners/azure-devops-agent-pools.png\" alt-text=\"Screenshot of Azure DevOps agent pools button.\"::: . ",
    "url": "/my-tech/docs/CICD#under-the-pipelines-section-in-the-project-settings-navigation-menu-select-agent-pools",
    
    "relUrl": "/docs/CICD#under-the-pipelines-section-in-the-project-settings-navigation-menu-select-agent-pools"
  },"130": {
    "doc": "CICD",
    "title": "Select Add pool and enter the following values.",
    "content": "| Setting | Value |---|---| *Pool to link* | Select **New**. | *Pool type* | Select **Self-hosted**. | *Name* | Enter **container-apps**. | *Grant access permission to all pipelines* | Select this checkbox. | . ",
    "url": "/my-tech/docs/CICD#select-add-pool-and-enter-the-following-values",
    
    "relUrl": "/docs/CICD#select-add-pool-and-enter-the-following-values"
  },"131": {
    "doc": "CICD",
    "title": "Select Create.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-create-1",
    
    "relUrl": "/docs/CICD#select-create-1"
  },"132": {
    "doc": "CICD",
    "title": "Get an Azure DevOps personal access token",
    "content": "To run a self-hosted runner, you need to create a personal access token (PAT) in Azure DevOps. The PAT is used to authenticate the runner with Azure DevOps. It’s also used by the scale rule to determine the number of pending pipeline runs and trigger new job executions. ",
    "url": "/my-tech/docs/CICD#get-an-azure-devops-personal-access-token",
    
    "relUrl": "/docs/CICD#get-an-azure-devops-personal-access-token"
  },"133": {
    "doc": "CICD",
    "title": "In Azure DevOps, select User settings next to your profile picture in the upper-right corner.",
    "content": " ",
    "url": "/my-tech/docs/CICD#in-azure-devops-select-user-settings-next-to-your-profile-picture-in-the-upper-right-corner",
    
    "relUrl": "/docs/CICD#in-azure-devops-select-user-settings-next-to-your-profile-picture-in-the-upper-right-corner"
  },"134": {
    "doc": "CICD",
    "title": "Select Personal access tokens.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-personal-access-tokens",
    
    "relUrl": "/docs/CICD#select-personal-access-tokens"
  },"135": {
    "doc": "CICD",
    "title": "In the Personal access tokens page, select New Token and enter the following values.",
    "content": "| Setting | Value |---|---| *Name* | Enter a name for your token. | *Organization* | Select the organization you chose or created earlier. | *Scopes* | Select **Custom defined**. | *Show all scopes* | Select **Show all scopes**. | *Agent Pools (Read &amp; manage)* | Select **Agent Pools (Read &amp; manage)**. | Leave all other scopes unselected. ",
    "url": "/my-tech/docs/CICD#in-the-personal-access-tokens-page-select-new-token-and-enter-the-following-values",
    
    "relUrl": "/docs/CICD#in-the-personal-access-tokens-page-select-new-token-and-enter-the-following-values"
  },"136": {
    "doc": "CICD",
    "title": "Select Create.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-create-2",
    
    "relUrl": "/docs/CICD#select-create-2"
  },"137": {
    "doc": "CICD",
    "title": "Copy the token value to a secure location.",
    "content": "You can't retrieve the token after you leave the page. ",
    "url": "/my-tech/docs/CICD#copy-the-token-value-to-a-secure-location",
    
    "relUrl": "/docs/CICD#copy-the-token-value-to-a-secure-location"
  },"138": {
    "doc": "CICD",
    "title": "Define variables that are used to configure the Container Apps jobs later.",
    "content": "- Bash ```bash AZP_TOKEN=\"&lt;AZP_TOKEN&gt;\" ORGANIZATION_URL=\"&lt;ORGANIZATION_URL&gt;\" AZP_POOL=\"container-apps\" ``` - PowerShell ```powershell $AZP_TOKEN=\"&lt;AZP_TOKEN&gt;\" $ORGANIZATION_URL=\"&lt;ORGANIZATION_URL&gt;\" $AZP_POOL=\"container-apps\" ``` --- Replace the placeholders with the following values: | Placeholder | Value | Comments |---|---|---| `&lt;AZP_TOKEN&gt;` | The Azure DevOps PAT you generated. | | `&lt;ORGANIZATION_URL&gt;` | The URL of your Azure DevOps organization. | For example, `https://dev.azure.com/myorg` or `https://myorg.visualstudio.com`. | . ",
    "url": "/my-tech/docs/CICD#define-variables-that-are-used-to-configure-the-container-apps-jobs-later",
    
    "relUrl": "/docs/CICD#define-variables-that-are-used-to-configure-the-container-apps-jobs-later"
  },"139": {
    "doc": "CICD",
    "title": "Build the Azure Pipelines agent container image",
    "content": "To create a self-hosted agent, you need to build a container image that runs the agent. In this section, you build the container image and push it to a container registry. [!NOTE] The image you build in this tutorial contains a basic self-hosted agent that’s suitable for running as a Container Apps job. You can customize it to include additional tools or dependencies that your pipelines require. ",
    "url": "/my-tech/docs/CICD#build-the-azure-pipelines-agent-container-image",
    
    "relUrl": "/docs/CICD#build-the-azure-pipelines-agent-container-image"
  },"140": {
    "doc": "CICD",
    "title": "Back in your terminal, define a name for your container image and registry.",
    "content": "- Bash ```bash CONTAINER_IMAGE_NAME=\"azure-pipelines-agent:1.0\" CONTAINER_REGISTRY_NAME=\"&lt;CONTAINER_REGISTRY_NAME&gt;\" ``` - PowerShell ```powershell $CONTAINER_IMAGE_NAME=\"azure-pipelines-agent:1.0\" $CONTAINER_REGISTRY_NAME=\"&lt;CONTAINER_REGISTRY_NAME&gt;\" ``` --- Replace `&lt;CONTAINER_REGISTRY_NAME&gt;` with a unique name for creating a container registry. Container registry names must be *unique within Azure* and be from 5 to 50 characters in length containing numbers and lowercase letters only. ",
    "url": "/my-tech/docs/CICD#back-in-your-terminal-define-a-name-for-your-container-image-and-registry",
    
    "relUrl": "/docs/CICD#back-in-your-terminal-define-a-name-for-your-container-image-and-registry"
  },"141": {
    "doc": "CICD",
    "title": "Create a container registry.",
    "content": "- Bash ```bash az acr create \\ --name \"$CONTAINER_REGISTRY_NAME\" \\ --resource-group \"$RESOURCE_GROUP\" \\ --location \"$LOCATION\" \\ --sku Basic \\ --admin-enabled true ``` - PowerShell ```powershell az acr create ` --name \"$CONTAINER_REGISTRY_NAME\" ` --resource-group \"$RESOURCE_GROUP\" ` --location \"$LOCATION\" ` --sku Basic ` --admin-enabled true ``` --- . ",
    "url": "/my-tech/docs/CICD#create-a-container-registry-1",
    
    "relUrl": "/docs/CICD#create-a-container-registry-1"
  },"142": {
    "doc": "CICD",
    "title": "The Dockerfile for creating the runner image is available on GitHub. Run the following command to clone the repository and build the container image in the cloud using the az acr build command.",
    "content": "- Bash ```bash az acr build \\ --registry \"$CONTAINER_REGISTRY_NAME\" \\ --image \"$CONTAINER_IMAGE_NAME\" \\ --file \"Dockerfile.azure-pipelines\" \\ \"https://github.com/Azure-Samples/container-apps-ci-cd-runner-tutorial.git\" ``` - PowerShell ```powershell az acr build ` --registry \"$CONTAINER_REGISTRY_NAME\" ` --image \"$CONTAINER_IMAGE_NAME\" ` --file \"Dockerfile.azure-pipelines\" ` \"https://github.com/Azure-Samples/container-apps-ci-cd-runner-tutorial.git\" ``` --- The image is now available in the container registry. ",
    "url": "/my-tech/docs/CICD#the-dockerfile-for-creating-the-runner-image-is-available-on-github-run-the-following-command-to-clone-the-repository-and-build-the-container-image-in-the-cloud-using-the-az-acr-build-command-1",
    
    "relUrl": "/docs/CICD#the-dockerfile-for-creating-the-runner-image-is-available-on-github-run-the-following-command-to-clone-the-repository-and-build-the-container-image-in-the-cloud-using-the-az-acr-build-command-1"
  },"143": {
    "doc": "CICD",
    "title": "Create a placeholder self-hosted agent",
    "content": "Before you can run a self-hosted agent in your new agent pool, you need to create a placeholder agent. The placeholder agent ensures the agent pool is available. Pipelines that use the agent pool fail when there’s no placeholder agent. You can run a manual job to register an offline placeholder agent. The job runs once and can be deleted. The placeholder agent doesn’t consume any resources in Azure Container Apps or Azure DevOps. ",
    "url": "/my-tech/docs/CICD#create-a-placeholder-self-hosted-agent",
    
    "relUrl": "/docs/CICD#create-a-placeholder-self-hosted-agent"
  },"144": {
    "doc": "CICD",
    "title": "Create a manual job in the Container Apps environment that creates the placeholder agent.",
    "content": "- Bash ```bash az containerapp job create -n \"$PLACEHOLDER_JOB_NAME\" -g \"$RESOURCE_GROUP\" --environment \"$ENVIRONMENT\" \\ --trigger-type Manual \\ --replica-timeout 300 \\ --replica-retry-limit 1 \\ --replica-completion-count 1 \\ --parallelism 1 \\ --image \"$CONTAINER_REGISTRY_NAME.azurecr.io/$CONTAINER_IMAGE_NAME\" \\ --cpu \"2.0\" \\ --memory \"4Gi\" \\ --secrets \"personal-access-token=$AZP_TOKEN\" \"organization-url=$ORGANIZATION_URL\" \\ --env-vars \"AZP_TOKEN=secretref:personal-access-token\" \"AZP_URL=secretref:organization-url\" \"AZP_POOL=$AZP_POOL\" \"AZP_PLACEHOLDER=1\" \"AZP_AGENT_NAME=placeholder-agent\" \\ --registry-server \"$CONTAINER_REGISTRY_NAME.azurecr.io\" ``` - PowerShell ```powershell az containerapp job create -n \"$PLACEHOLDER_JOB_NAME\" -g \"$RESOURCE_GROUP\" --environment \"$ENVIRONMENT\" ` --trigger-type Manual ` --replica-timeout 300 ` --replica-retry-limit 1 ` --replica-completion-count 1 ` --parallelism 1 ` --image \"$CONTAINER_REGISTRY_NAME.azurecr.io/$CONTAINER_IMAGE_NAME\" ` --cpu \"2.0\" ` --memory \"4Gi\" ` --secrets \"personal-access-token=$AZP_TOKEN\" \"organization-url=$ORGANIZATION_URL\" ` --env-vars \"AZP_TOKEN=secretref:personal-access-token\" \"AZP_URL=secretref:organization-url\" \"AZP_POOL=$AZP_POOL\" \"AZP_PLACEHOLDER=1\" \"AZP_AGENT_NAME=placeholder-agent\" ` --registry-server \"$CONTAINER_REGISTRY_NAME.azurecr.io\" ``` --- The following table describes the key parameters used in the command. | Parameter | Description | --- | --- | `--replica-timeout` | The maximum duration a replica can execute. | `--replica-retry-limit` | The number of times to retry a failed replica. | `--replica-completion-count` | The number of replicas to complete successfully before a job execution is considered successful. | `--parallelism` | The number of replicas to start per job execution. | `--secrets` | The secrets to use for the job. | `--env-vars` | The environment variables to use for the job. | `--registry-server` | The container registry server to use for the job. For an Azure Container Registry, the command automatically configures authentication. | Setting the `AZP_PLACEHOLDER` environment variable configures the agent container to register as an offline placeholder agent without running a job. ",
    "url": "/my-tech/docs/CICD#create-a-manual-job-in-the-container-apps-environment-that-creates-the-placeholder-agent",
    
    "relUrl": "/docs/CICD#create-a-manual-job-in-the-container-apps-environment-that-creates-the-placeholder-agent"
  },"145": {
    "doc": "CICD",
    "title": "Execute the manual job to create the placeholder agent.",
    "content": "- Bash ```bash az containerapp job start -n \"$PLACEHOLDER_JOB_NAME\" -g \"$RESOURCE_GROUP\" ``` - PowerShell ```powershell az containerapp job start -n \"$PLACEHOLDER_JOB_NAME\" -g \"$RESOURCE_GROUP\" ``` --- . ",
    "url": "/my-tech/docs/CICD#execute-the-manual-job-to-create-the-placeholder-agent",
    
    "relUrl": "/docs/CICD#execute-the-manual-job-to-create-the-placeholder-agent"
  },"146": {
    "doc": "CICD",
    "title": "List the executions of the job to confirm a job execution was created and completed successfully.",
    "content": "- Bash ```bash az containerapp job execution list \\ --name \"$PLACEHOLDER_JOB_NAME\" \\ --resource-group \"$RESOURCE_GROUP\" \\ --output table \\ --query '[].{Status: properties.status, Name: name, StartTime: properties.startTime}' ``` - PowerShell ```powershell az containerapp job execution list ` --name \"$PLACEHOLDER_JOB_NAME\" ` --resource-group \"$RESOURCE_GROUP\" ` --output table ` --query '[].{Status: properties.status, Name: name, StartTime: properties.startTime}' ``` --- . ",
    "url": "/my-tech/docs/CICD#list-the-executions-of-the-job-to-confirm-a-job-execution-was-created-and-completed-successfully-1",
    
    "relUrl": "/docs/CICD#list-the-executions-of-the-job-to-confirm-a-job-execution-was-created-and-completed-successfully-1"
  },"147": {
    "doc": "CICD",
    "title": "Verify the placeholder agent was created in Azure DevOps.",
    "content": "# In Azure DevOps, navigate to your project. # Select **Project settings** &gt; **Agent pools** &gt; **container-apps** &gt; **Agents**. # Confirm that a placeholder agent named `placeholder-agent` is listed and its status is offline. ",
    "url": "/my-tech/docs/CICD#verify-the-placeholder-agent-was-created-in-azure-devops",
    
    "relUrl": "/docs/CICD#verify-the-placeholder-agent-was-created-in-azure-devops"
  },"148": {
    "doc": "CICD",
    "title": "The job isn’t needed again. You can delete it.",
    "content": "- Bash ```bash az containerapp job delete -n \"$PLACEHOLDER_JOB_NAME\" -g \"$RESOURCE_GROUP\" ``` - PowerShell ```powershell az containerapp job delete -n \"$PLACEHOLDER_JOB_NAME\" -g \"$RESOURCE_GROUP\" ``` --- . ",
    "url": "/my-tech/docs/CICD#the-job-isnt-needed-again-you-can-delete-it",
    
    "relUrl": "/docs/CICD#the-job-isnt-needed-again-you-can-delete-it"
  },"149": {
    "doc": "CICD",
    "title": "Create a self-hosted agent as an event-driven job",
    "content": "Now that you have a placeholder agent, you can create a self-hosted agent. In this section, you create an event-driven job that runs a self-hosted agent when a pipeline is triggered. | Bash az containerapp job create -n \"$JOB_NAME\" -g \"$RESOURCE_GROUP\" --environment \"$ENVIRONMENT\" \\ --trigger-type Event \\ --replica-timeout 1800 \\ --replica-retry-limit 1 \\ --replica-completion-count 1 \\ --parallelism 1 \\ --image \"$CONTAINER_REGISTRY_NAME.azurecr.io/$CONTAINER_IMAGE_NAME\" \\ --min-executions 0 \\ --max-executions 10 \\ --polling-interval 30 \\ --scale-rule-name \"azure-pipelines\" \\ --scale-rule-type \"azure-pipelines\" \\ --scale-rule-metadata \"poolName=container-apps\" \"targetPipelinesQueueLength=1\" \\ --scale-rule-auth \"personalAccessToken=personal-access-token\" \"organizationURL=organization-url\" \\ --cpu \"2.0\" \\ --memory \"4Gi\" \\ --secrets \"personal-access-token=$AZP_TOKEN\" \"organization-url=$ORGANIZATION_URL\" \\ --env-vars \"AZP_TOKEN=secretref:personal-access-token\" \"AZP_URL=secretref:organization-url\" \"AZP_POOL=$AZP_POOL\" \\ --registry-server \"$CONTAINER_REGISTRY_NAME.azurecr.io\" . | PowerShell az containerapp job create -n \"$JOB_NAME\" -g \"$RESOURCE_GROUP\" --environment \"$ENVIRONMENT\" \\ --trigger-type Event \\ --replica-timeout 1800 \\ --replica-retry-limit 1 \\ --replica-completion-count 1 \\ --parallelism 1 \\ --image \"$CONTAINER_REGISTRY_NAME.azurecr.io/$CONTAINER_IMAGE_NAME\" \\ --min-executions 0 \\ --max-executions 10 \\ --polling-interval 30 \\ --scale-rule-name \"azure-pipelines\" \\ --scale-rule-type \"azure-pipelines\" \\ --scale-rule-metadata \"poolName=container-apps\" \"targetPipelinesQueueLength=1\" \\ --scale-rule-auth \"personalAccessToken=personal-access-token\" \"organizationURL=organization-url\" \\ --cpu \"2.0\" \\ --memory \"4Gi\" \\ --secrets \"personal-access-token=$AZP_TOKEN\" \"organization-url=$ORGANIZATION_URL\" \\ --env-vars \"AZP_TOKEN=secretref:personal-access-token\" \"AZP_URL=secretref:organization-url\" \"AZP_POOL=$AZP_POOL\" \\ --registry-server \"$CONTAINER_REGISTRY_NAME.azurecr.io\" . | . The following table describes the scale rule parameters used in the command. | Parameter | Description | . | --min-executions | The minimum number of job executions to run per polling interval. | . | --max-executions | The maximum number of job executions to run per polling interval. | . | --polling-interval | The polling interval at which to evaluate the scale rule. | . | --scale-rule-name | The name of the scale rule. | . | --scale-rule-type | The type of scale rule to use. To learn more about the Azure Pipelines scaler, see the KEDA documentation. | . | --scale-rule-metadata | The metadata for the scale rule. | . | --scale-rule-auth | The authentication for the scale rule. | . The scale rule configuration defines the event source to monitor. It’s evaluated on each polling interval and determines how many job executions to trigger. To learn more, see Set scaling rules. The event-driven job is now created in the Container Apps environment. ",
    "url": "/my-tech/docs/CICD#create-a-self-hosted-agent-as-an-event-driven-job",
    
    "relUrl": "/docs/CICD#create-a-self-hosted-agent-as-an-event-driven-job"
  },"150": {
    "doc": "CICD",
    "title": "Run a pipeline and verify the job",
    "content": "Now that you’ve configured a self-hosted agent job, you can run a pipeline and verify it’s working correctly. ",
    "url": "/my-tech/docs/CICD#run-a-pipeline-and-verify-the-job",
    
    "relUrl": "/docs/CICD#run-a-pipeline-and-verify-the-job"
  },"151": {
    "doc": "CICD",
    "title": "In the left-hand navigation of your Azure DevOps project, navigate to Pipelines.",
    "content": " ",
    "url": "/my-tech/docs/CICD#in-the-left-hand-navigation-of-your-azure-devops-project-navigate-to-pipelines",
    
    "relUrl": "/docs/CICD#in-the-left-hand-navigation-of-your-azure-devops-project-navigate-to-pipelines"
  },"152": {
    "doc": "CICD",
    "title": "Select Create pipeline.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-create-pipeline",
    
    "relUrl": "/docs/CICD#select-create-pipeline"
  },"153": {
    "doc": "CICD",
    "title": "Select Azure Repos Git as the location of your code.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-azure-repos-git-as-the-location-of-your-code",
    
    "relUrl": "/docs/CICD#select-azure-repos-git-as-the-location-of-your-code"
  },"154": {
    "doc": "CICD",
    "title": "Select the repository you created earlier.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-the-repository-you-created-earlier",
    
    "relUrl": "/docs/CICD#select-the-repository-you-created-earlier"
  },"155": {
    "doc": "CICD",
    "title": "Select Starter pipeline.",
    "content": " ",
    "url": "/my-tech/docs/CICD#select-starter-pipeline",
    
    "relUrl": "/docs/CICD#select-starter-pipeline"
  },"156": {
    "doc": "CICD",
    "title": "In the pipeline YAML, change the pool from vmImage: ubuntu-latest to name: container-apps.",
    "content": "```yaml pool: name: container-apps ``` . ",
    "url": "/my-tech/docs/CICD#in-the-pipeline-yaml-change-the-pool-from-vmimage-ubuntu-latest-to-name-container-apps",
    
    "relUrl": "/docs/CICD#in-the-pipeline-yaml-change-the-pool-from-vmimage-ubuntu-latest-to-name-container-apps"
  },"157": {
    "doc": "CICD",
    "title": "Select Save and run.",
    "content": "The pipeline runs and uses the self-hosted agent job you created in the Container Apps environment. ",
    "url": "/my-tech/docs/CICD#select-save-and-run",
    
    "relUrl": "/docs/CICD#select-save-and-run"
  },"158": {
    "doc": "CICD",
    "title": "List the executions of the job to confirm a job execution was created and completed successfully.",
    "content": "- Bash ```bash az containerapp job execution list \\ --name \"$JOB_NAME\" \\ --resource-group \"$RESOURCE_GROUP\" \\ --output table \\ --query '[].{Status: properties.status, Name: name, StartTime: properties.startTime}' ``` - PowerShell ```powershell az containerapp job execution list ` --name \"$JOB_NAME\" ` --resource-group \"$RESOURCE_GROUP\" ` --output table ` --query '[].{Status: properties.status, Name: name, StartTime: properties.startTime}' ``` --- . ::: zone-end . [!TIP] Having issues? Let us know on GitHub by opening an issue in the Azure Container Apps repo. ",
    "url": "/my-tech/docs/CICD#list-the-executions-of-the-job-to-confirm-a-job-execution-was-created-and-completed-successfully-2",
    
    "relUrl": "/docs/CICD#list-the-executions-of-the-job-to-confirm-a-job-execution-was-created-and-completed-successfully-2"
  },"159": {
    "doc": "CICD",
    "title": "Clean up resources",
    "content": "Once you’re done, run the following command to delete the resource group that contains your Container Apps resources. [!CAUTION] The following command deletes the specified resource group and all resources contained within it. If resources outside the scope of this tutorial exist in the specified resource group, they will also be deleted. | Bash az group delete \\ --resource-group $RESOURCE_GROUP . | PowerShell az group delete ` --resource-group $RESOURCE_GROUP . | . To delete your GitHub repository, see Deleting a repository. ",
    "url": "/my-tech/docs/CICD#clean-up-resources",
    
    "relUrl": "/docs/CICD#clean-up-resources"
  },"160": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "CloudFormation Stack Deployment Examples",
    "content": " ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/"
  },"161": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "List of CloudFormation Stack Deployments for AWS resources",
    "content": "Date Posted: 2023 05 03 . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/"
  },"162": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Table of Contents",
    "content": ". | Basic Infrastructure CloudFormation Template with EC2 Instance | . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#table-of-contents",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#table-of-contents"
  },"163": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Basic Infrastructure CloudFormation Template with EC2 Instance",
    "content": "Provisioing the Basic Infrastructure by Creating a CloudFormation Stack AWSTemplateFormatVersion: 2010-09-09 Description: Basic Infrastructure CloudFormation Template with EC2 Instance Parameters: KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access to the instance Type: AWS::EC2::KeyPair::KeyName Default: Dev Resources: MyVPC: Type: AWS::EC2::VPC Properties: CidrBlock: \"10.0.0.0/16\" EnableDnsSupport: true EnableDnsHostnames: true MySubnet: Type: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC CidrBlock: \"10.0.0.0/24\" AvailabilityZone: \"us-east-1a\" MyInternetGateway: Type: AWS::EC2::InternetGateway MyVPCGatewayAttachment: Type: AWS::EC2::VPCGatewayAttachment Properties: VpcId: !Ref MyVPC InternetGatewayId: !Ref MyInternetGateway MyRouteTable: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref MyVPC MyRoute: Type: AWS::EC2::Route DependsOn: MyVPCGatewayAttachment Properties: RouteTableId: !Ref MyRouteTable DestinationCidrBlock: \"0.0.0.0/0\" GatewayId: !Ref MyInternetGateway MySubnetRouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref MySubnet RouteTableId: !Ref MyRouteTable MySecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: \"My Security Group\" VpcId: !Ref MyVPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: \"0.0.0.0/0\" MyInstance: Type: AWS::EC2::Instance Properties: InstanceType: t2.micro NetworkInterfaces: - GroupSet: - !Ref MySecurityGroup AssociatePublicIpAddress: true DeviceIndex: 0 DeleteOnTermination: true SubnetId: !Ref MySubnet KeyName: !Ref KeyName ImageId: \"ami-97785bed\" Tags: - Key: Name Value: \"instance-1\" MyEIP: Type: AWS::EC2::EIP MyInstanceEIPAssociation: Type: AWS::EC2::EIPAssociation Properties: InstanceId: !Ref MyInstance EIP: !Ref MyEIP Outputs: Instance1PublicIP: Description: Public IP Address of instance-1 Value: !GetAtt MyInstance.PublicIp . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#basic-infrastructure-cloudformation-template-with-ec2-instance",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#basic-infrastructure-cloudformation-template-with-ec2-instance"
  },"164": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Parameters",
    "content": ". | KeyName: This parameter allows you to specify the name of an existing EC2 KeyPair to enable SSH access to the instance. The default value is set to Dev. | . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#parameters",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#parameters"
  },"165": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Resources",
    "content": "VPC (MyVPC) . | Creates a Virtual Private Cloud (VPC) with the CIDR block 10.0.0.0/16. | . Subnet (MySubnet) . | Creates a subnet within the VPC with the CIDR block 10.0.0.0/24. The subnet is associated with the availability zone us-east-1a. | . Internet Gateway (MyInternetGateway) . | Creates an Internet Gateway that enables internet connectivity for instances within the VPC. | . VPC Gateway Attachment (MyVPCGatewayAttachment) . | Attaches the Internet Gateway created above to the VPC. | . Route Table (MyRouteTable) . | Creates a route table associated with the VPC. | . Route (MyRoute) . | Adds a default route to the route table, directing all traffic (0.0.0.0/0) to the Internet Gateway. | . Subnet Route Table Association (MySubnetRouteTableAssociation) . | Associates the subnet with the route table, enabling internet access for instances in the subnet. | . Security Group (MySecurityGroup) . | Creates a security group allowing SSH (TCP port 22) traffic from any IP (0.0.0.0/0). It is associated with the VPC. | . EC2 Instance (MyInstance) . | Creates an EC2 instance of type t2.micro, associated with the specified KeyPair and placed in the subnet created earlier. The instance uses the specified Amazon Machine Image (AMI) ID. It is also tagged with the name “instance-1”. | . Elastic IP (MyEIP) . | Allocates an Elastic IP address to associate with the EC2 instance. | . EIP Association (MyInstanceEIPAssociation) . | Associates the Elastic IP with the EC2 instance, making it accessible with a static public IP address. | . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#resources",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#resources"
  },"166": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Outputs",
    "content": ". | Instance1PublicIP: This output displays the public IP address of the “instance-1” EC2 instance, allowing you to easily identify and connect to the instance via SSH. | . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#outputs",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#outputs"
  },"167": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Test",
    "content": "To test it out, ensure to ssh into the public IP address . ssh -i \"Dev.pem\" ec2-user@public-IP-Address . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#test",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#test"
  },"168": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Basic Infrastructure CloudFormation Template with Public and Private Subnet with Public Bastion and Private EC2 Instance",
    "content": "This CloudFormation Stack provisions basic infrastructure, with public and private subnets, along with public and private EC2 instances AWSTemplateFormatVersion: 2010-09-09 Description: Infrastructure CloudFormation Template with Bastion (Public), Private Subnets, NAT Gateway, and EC2 Instance Parameters: KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access to the instance Type: AWS::EC2::KeyPair::KeyName Default: Dev Resources: MyVPC: Type: AWS::EC2::VPC Properties: CidrBlock: \"10.0.0.0/16\" EnableDnsSupport: true EnableDnsHostnames: true MyPublicSubnet: Type: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC CidrBlock: \"10.0.0.0/24\" AvailabilityZone: \"us-east-1a\" MyPrivateSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC CidrBlock: \"10.0.1.0/24\" AvailabilityZone: \"us-east-1b\" MyPrivateSubnet2: Type: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC CidrBlock: \"10.0.2.0/24\" AvailabilityZone: \"us-east-1c\" MyInternetGateway: Type: AWS::EC2::InternetGateway MyVPCGatewayAttachment: Type: AWS::EC2::VPCGatewayAttachment Properties: VpcId: !Ref MyVPC InternetGatewayId: !Ref MyInternetGateway MyPublicRouteTable: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref MyVPC MyPublicRoute: Type: AWS::EC2::Route DependsOn: MyVPCGatewayAttachment Properties: RouteTableId: !Ref MyPublicRouteTable DestinationCidrBlock: \"0.0.0.0/0\" GatewayId: !Ref MyInternetGateway MyPublicSubnetRouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref MyPublicSubnet RouteTableId: !Ref MyPublicRouteTable MySecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: \"My Security Group\" VpcId: !Ref MyVPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: \"0.0.0.0/0\" MyBastionInstance: Type: AWS::EC2::Instance Properties: InstanceType: t2.micro KeyName: !Ref KeyName NetworkInterfaces: - GroupSet: - !Ref MySecurityGroup AssociatePublicIpAddress: true DeviceIndex: 0 DeleteOnTermination: true SubnetId: !Ref MyPublicSubnet ImageId: \"ami-97785bed\" Tags: - Key: Name Value: \"bastion-instance\" MyPrivateRouteTable1: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref MyVPC MyPrivateRoute1: Type: AWS::EC2::Route Properties: RouteTableId: !Ref MyPrivateRouteTable1 DestinationCidrBlock: \"0.0.0.0/0\" NatGatewayId: !Ref MyNatGateway1 MyPrivateSubnetRouteTableAssociation1: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref MyPrivateSubnet1 RouteTableId: !Ref MyPrivateRouteTable1 MyPrivateRouteTable2: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref MyVPC MyPrivateRoute2: Type: AWS::EC2::Route Properties: RouteTableId: !Ref MyPrivateRouteTable2 DestinationCidrBlock: \"0.0.0.0/0\" NatGatewayId: !Ref MyNatGateway2 MyPrivateSubnetRouteTableAssociation2: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref MyPrivateSubnet2 RouteTableId: !Ref MyPrivateRouteTable2 MyNatGateway1: Type: AWS::EC2::NatGateway Properties: AllocationId: !GetAtt MyEIP1.AllocationId SubnetId: !Ref MyPublicSubnet MyNatGateway2: Type: AWS::EC2::NatGateway Properties: AllocationId: !GetAtt MyEIP2.AllocationId SubnetId: !Ref MyPublicSubnet MyEIP1: Type: AWS::EC2::EIP MyEIP2: Type: AWS::EC2::EIP MyPrivateSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: \"Private Security Group\" VpcId: !Ref MyVPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: \"0.0.0.0/0\" MyPrivateEC2Instance: Type: AWS::EC2::Instance Properties: InstanceType: t2.micro KeyName: !Ref KeyName NetworkInterfaces: - GroupSet: - !Ref MyPrivateSecurityGroup DeviceIndex: 0 DeleteOnTermination: true SubnetId: !Ref MyPrivateSubnet1 ImageId: \"ami-97785bed\" Tags: - Key: Name Value: \"private-instance\" Outputs: Instance1PublicIP: Description: Public IP Address of bastion-instance (instance-1) Value: !GetAtt MyBastionInstance.PublicIp PrivateInstance1PrivateIP: Description: Private IP Address of private-instance-1 Value: !GetAtt MyPrivateEC2Instance.PrivateIp . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#basic-infrastructure-cloudformation-template-with-public-and-private-subnet-with-public-bastion-and-private-ec2-instance",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#basic-infrastructure-cloudformation-template-with-public-and-private-subnet-with-public-bastion-and-private-ec2-instance"
  },"169": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Parameters",
    "content": ". | KeyName: This parameter allows you to specify the name of an existing EC2 KeyPair to enable SSH access to the instances. The default value is set to Dev. | . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#parameters-1",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#parameters-1"
  },"170": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Resources",
    "content": "VPC (MyVPC) . | Creates a Virtual Private Cloud (VPC) with the CIDR block 10.0.0.0/16. | . Public Subnet (MyPublicSubnet) . | Creates a public subnet within the VPC with the CIDR block 10.0.0.0/24. This subnet is associated with the availability zone us-east-1a. | . Private Subnet 1 (MyPrivateSubnet1) . | Creates the first private subnet within the VPC with the CIDR block 10.0.1.0/24. This subnet is associated with the availability zone us-east-1b. | . Private Subnet 2 (MyPrivateSubnet2) . | Creates the second private subnet within the VPC with the CIDR block 10.0.2.0/24. This subnet is associated with the availability zone us-east-1c. | . Internet Gateway (MyInternetGateway) . | Creates an Internet Gateway that enables internet connectivity for instances within the VPC. | . VPC Gateway Attachment (MyVPCGatewayAttachment) . | Attaches the Internet Gateway created above to the VPC. | . Public Route Table (MyPublicRouteTable) . | Creates a public route table associated with the VPC. | . Public Route (MyPublicRoute) . | Adds a default route to the public route table, directing all traffic (0.0.0.0/0) to the Internet Gateway. | . Public Subnet Route Table Association (MyPublicSubnetRouteTableAssociation) . | Associates the public subnet with the public route table, enabling internet access for instances in the public subnet. | . Security Group (MySecurityGroup) . | Creates a security group allowing SSH (TCP port 22) traffic from any IP (0.0.0.0/0). It is associated with the VPC and used for the bastion (public) instance. | . Bastion (Public) EC2 Instance (MyBastionInstance) . | Creates a bastion (public) EC2 instance of type t2.micro, associated with the specified KeyPair, placed in the public subnet, and allocated a public IP address. The instance uses the specified Amazon Machine Image (AMI) ID and is tagged with the name “bastion-instance”. | . Private Route Table 1 (MyPrivateRouteTable1) . | Creates the first private route table associated with the VPC. | . Private Route 1 (MyPrivateRoute1) . | Adds a default route to the first private route table, directing all traffic (0.0.0.0/0) to the NAT Gateway for the first private subnet. | . Private Subnet 1 Route Table Association (MyPrivateSubnetRouteTableAssociation1) . | Associates the first private subnet with the first private route table. | . Private Route Table 2 (MyPrivateRouteTable2) . | Creates the second private route table associated with the VPC. | . Private Route 2 (MyPrivateRoute2) . | Adds a default route to the second private route table, directing all traffic (0.0.0.0/0) to the NAT Gateway for the second private subnet. | . Private Subnet 2 Route Table Association (MyPrivateSubnetRouteTableAssociation2) . | Associates the second private subnet with the second private route table. | . NAT Gateway 1 (MyNatGateway1) . | Creates the first NAT Gateway associated with the public subnet. It allows instances in the first private subnet to access the internet. | . NAT Gateway 2 (MyNatGateway2) . | Creates the second NAT Gateway associated with the public subnet. It allows instances in the second private subnet to access the internet. | . Elastic IP 1 (MyEIP1) . | Allocates an Elastic IP address for the first NAT Gateway. | . Elastic IP 2 (MyEIP2) . | Allocates an Elastic IP address for the second NAT Gateway. | . Private Security Group (MyPrivateSecurityGroup) . | Creates a security group allowing SSH (TCP port 22) traffic from any IP (0.0.0.0/0). It is associated with the VPC and used for the private EC2 instance. | . Private EC2 Instance (MyPrivateEC2Instance) . | Creates a private EC2 instance of type t2.micro, associated with the specified KeyPair, placed in the first private subnet, and allocated a private IP address. The instance uses the specified Amazon Machine Image (AMI) ID and is tagged with the name “private-instance”. | . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#resources-1",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#resources-1"
  },"171": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Outputs",
    "content": ". | Instance1PublicIP: This output displays the public IP address of the bastion (public) instance (MyBastionInstance). You can use this IP address to SSH into the bastion instance. | PrivateInstance1PrivateIP: This output displays the private IP address of the private EC2 instance (MyPrivateEC2Instance). You can use this IP address to access the private EC2 instance within the private subnets. | . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#outputs-1",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#outputs-1"
  },"172": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Test",
    "content": "To test it out, ensure to ssh into the public IP address . ## SSH into the public EC2 instance ssh -i \"Dev.pem\" ec2-user@public-IP-address ## Get the copy of Dev.pem cat &lt;&lt; EOF&gt; Dev.pem -----BEGIN RSA PRIVATE KEY----- dscjdnsjkcdsncjks -------END RSA PRIVATE KEY------- EOF chmod 400 Dev.pem ssh -i \"Dev.pem\" ec2-user@private-IP-address . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#test-1",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#test-1"
  },"173": {
    "doc": "CloudFormation Stack Deployment Examples",
    "title": "Create 2 More Private EC2 Insntaces",
    "content": "This CloudFormation Stack provisions basic infrastructure, with public and private subnets, along with public and private EC2 instances, Plus two more private EC2 instancesqqus AWSTemplateFormatVersion: 2010-09-09 Description: Infrastructure CloudFormation Template with Bastion (Public), Private Subnets, NAT Gateway, and EC2 Instance Parameters: KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access to the instance Type: AWS::EC2::KeyPair::KeyName Default: Dev Resources: MyVPC: Type: AWS::EC2::VPC Properties: CidrBlock: \"10.0.0.0/16\" EnableDnsSupport: true EnableDnsHostnames: true MyPublicSubnet: Type: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC CidrBlock: \"10.0.0.0/24\" AvailabilityZone: \"us-east-1a\" MyPrivateSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC CidrBlock: \"10.0.1.0/24\" AvailabilityZone: \"us-east-1b\" MyPrivateSubnet2: Type: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC CidrBlock: \"10.0.2.0/24\" AvailabilityZone: \"us-east-1c\" MyInternetGateway: Type: AWS::EC2::InternetGateway MyVPCGatewayAttachment: Type: AWS::EC2::VPCGatewayAttachment Properties: VpcId: !Ref MyVPC InternetGatewayId: !Ref MyInternetGateway MyPublicRouteTable: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref MyVPC MyPublicRoute: Type: AWS::EC2::Route DependsOn: MyVPCGatewayAttachment Properties: RouteTableId: !Ref MyPublicRouteTable DestinationCidrBlock: \"0.0.0.0/0\" GatewayId: !Ref MyInternetGateway MyPublicSubnetRouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref MyPublicSubnet RouteTableId: !Ref MyPublicRouteTable MySecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: \"My Security Group\" VpcId: !Ref MyVPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: \"0.0.0.0/0\" MyBastionInstance: Type: AWS::EC2::Instance Properties: InstanceType: t2.micro KeyName: !Ref KeyName NetworkInterfaces: - GroupSet: - !Ref MySecurityGroup AssociatePublicIpAddress: true DeviceIndex: 0 DeleteOnTermination: true SubnetId: !Ref MyPublicSubnet ImageId: \"ami-97785bed\" Tags: - Key: Name Value: \"bastion-instance\" MyPrivateRouteTable1: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref MyVPC MyPrivateRoute1: Type: AWS::EC2::Route Properties: RouteTableId: !Ref MyPrivateRouteTable1 DestinationCidrBlock: \"0.0.0.0/0\" NatGatewayId: !Ref MyNatGateway1 MyPrivateSubnetRouteTableAssociation1: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref MyPrivateSubnet1 RouteTableId: !Ref MyPrivateRouteTable1 MyPrivateRouteTable2: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref MyVPC MyPrivateRoute2: Type: AWS::EC2::Route Properties: RouteTableId: !Ref MyPrivateRouteTable2 DestinationCidrBlock: \"0.0.0.0/0\" NatGatewayId: !Ref MyNatGateway2 MyPrivateSubnetRouteTableAssociation2: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref MyPrivateSubnet2 RouteTableId: !Ref MyPrivateRouteTable2 MyNatGateway1: Type: AWS::EC2::NatGateway Properties: AllocationId: !GetAtt MyEIP1.AllocationId SubnetId: !Ref MyPublicSubnet MyNatGateway2: Type: AWS::EC2::NatGateway Properties: AllocationId: !GetAtt MyEIP2.AllocationId SubnetId: !Ref MyPublicSubnet MyEIP1: Type: AWS::EC2::EIP MyEIP2: Type: AWS::EC2::EIP MyPrivateSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: \"Private Security Group\" VpcId: !Ref MyVPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: \"0.0.0.0/0\" MyPrivateEC2Instance1: Type: AWS::EC2::Instance Properties: InstanceType: t2.micro KeyName: !Ref KeyName NetworkInterfaces: - GroupSet: - !Ref MyPrivateSecurityGroup DeviceIndex: 0 DeleteOnTermination: true SubnetId: !Ref MyPrivateSubnet1 ImageId: \"ami-97785bed\" Tags: - Key: Name Value: \"private-instance-1\" MyPrivateEC2Instance2: Type: AWS::EC2::Instance Properties: InstanceType: t2.micro KeyName: !Ref KeyName NetworkInterfaces: - GroupSet: - !Ref MyPrivateSecurityGroup DeviceIndex: 0 DeleteOnTermination: true SubnetId: !Ref MyPrivateSubnet2 ImageId: \"ami-97785bed\" Tags: - Key: Name Value: \"private-instance-2\" Outputs: Instance1PublicIP: Description: Public IP Address of bastion-instance (instance-1) Value: !GetAtt MyBastionInstance.PublicIp PrivateInstance1PrivateIP: Description: Private IP Address of private-instance-1 Value: !GetAtt MyPrivateEC2Instance1.PrivateIp PrivateInstance2PrivateIP: Description: Private IP Address of private-instance-2 Value: !GetAtt MyPrivateEC2Instance2.PrivateIp . ",
    "url": "/my-tech/doc/AWS/creation-of-basic-infra/#create-2-more-private-ec2-insntaces",
    
    "relUrl": "/doc/AWS/creation-of-basic-infra/#create-2-more-private-ec2-insntaces"
  },"174": {
    "doc": "Kubernetes 1.24 Installation",
    "title": "Kubernetes 1.24 Installation",
    "content": " ",
    "url": "/my-tech/doc/Kubernetes/doc-1/",
    
    "relUrl": "/doc/Kubernetes/doc-1/"
  },"175": {
    "doc": "Kubernetes 1.24 Installation",
    "title": "",
    "content": "Date Posted: 2023 02 05 . Basic Kubernetes 1.24 Installation # Install Packages # Containerd is needed # Before installing containerd load configuration files # Disable swap # Install dependency packages (apt-transport-https, curl,ca-certifcate, gnupg, lsb-release) # Install kubeadm, kubectl, kubelet # Initialize the Cluster Sudo kubeadm init --pod-network-cidr 1927.168.0.0/16 --kubernetes-version 1.23.0 # Kubeconfig # Network Add On # Calico, vpc, etc. # Join worker nodes # Sudo kubeadm join # Upgrading kubeadm refernce: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ # Upgrade the control plane # Check the version # Drain the control plane node kubectl drain node-name --ignore-daemonsets # Upgrade kubectl kubelet # Uncordon the node # Upgrade the worker node # Upgrade kubeadm # $ sudo kubeadm upgrade node # Upgrade kubelet and kubectl # Restart the service # https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster # Download etcd # Run etcd # etcd --listen-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379 --advertise-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379 # List etcd serveres # ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ member list # Backup etcd # ETCDCTL_API=3etcdctl --endpoints $ENDPOINT snapshot save snapshotdb ## Worker Nodes ## From https://kubernetes.io/docs/setup/production-environment/container-runtimes/ # containerd preinstall configuration cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # Setup required sysctl params, these persist across reboots. cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # Apply sysctl params without reboot sudo sysctl --system # Install containerd ## Set up the repository ### Install packages to allow apt to use a repository over HTTPS sudo apt-get update sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release OR Sudo apt-get update &amp;&amp; sudo apt-get install -y containerd ## Add Docker’s official GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg ## Add Docker apt repository. echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null ## Install packages sudo apt-get update sudo apt-get install -y \\ containerd.io # Configure containerd sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml # Restart containerd sudo systemctl restart containerd Resource: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ # Update the package sudo apt-get update # install needed packages sudo apt-get install -y apt-transport-https ca-certificates curl sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update # Install kubelet, kubelet, and kubeadm sudo apt-get install -y kubelet=1.20.7-00 kubeadm=1.20.7-00 kubectl=1.20.7-00 # stop automatic update for kubernetes packages sudo apt-mark hold kubelet kubeadm kubectl Resource: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ # Switch to Control Plane and start kubeadm $ sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket unix:///run/containerd/containerd.sock # copy and paste what kubeadm output says $ sudo kubeadm join 172.16.1.11:6443 --token h8vno9.7eroqaei7v1isdpn \\ --discovery-token-ca-cert-hash sha256:44f1def2a041f116bc024f7e57cdc0cdcc8d8f36f0b942bdd27c7f864f645407 --cri-socket unix:///run/containerd/containerd.sock # Configure kubectl access, add kubeconfig mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # Deploy Flannel as a network plugin kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml $ kubectl get nodes https://github.com/alijahnas/CKA-practice-exercises/blob/CKA-v1.20/cluster-architecture-installation-configuration.md#:~:text=https%3A//kubernetes.io/docs/tasks/administer%2Dcluster/kubeadm/kubeadm%2Dupgrade/ ########## Switch to Master Node ######### # Upgrade kubeadm sudo apt-mark unhold kubeadm sudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm=1.21.1-00 sudo apt-mark hold kubeadm # Upgrade controlplane node kubectl drain k8s-controlplane --ignore-daemonsets sudo kubeadm upgrade plan sudo kubeadm upgrade apply v1.21.1 # Update Flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # Upgrade kubelet and kubectl sudo apt-mark unhold kubelet kubectl sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet=1.21.1-00 kubectl=1.21.1-00 sudo apt-mark hold kubelet kubectl sudo systemctl daemon-reload sudo systemctl restart kubelet # Make master node reschedulable kubectl uncordon k8s-controlplane ####### Swtich to Worker Node # Upgrade kubeadm sudo apt-mark unhold kubeadm sudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm=1.21.1-00 sudo apt-mark hold kubeadm # Upgrade worker node kubectl drain k8s-node-1 --ignore-daemonsets sudo kubeadm upgrade node # Upgrade kubelet and kubectl sudo apt-mark unhold kubelet kubectl sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet=1.21.1-00 kubectl=1.21.1-00 sudo apt-mark hold kubelet kubectl sudo systemctl daemon-reload sudo systemctl restart kubelet # Make worker node reschedulable kubectl uncordon k8s-node-1 # Hold kubernetes from upgrading sudo apt-mark hold kubeadm kubelet kubectl # Upgrade node kubectl drain k8s-node-1 --ignore-daemonsets sudo apt update &amp;&amp; sudo apt upgrade -y # Be careful about container runtime (e.g., docker) upgrade. # Reboot node if necessary sudo reboot # Make worker node reschedulable kubectl uncordon k8s-node-1 https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster ## Check the version $ kubectl exec-it -n kube-system etcd-k8s-controlplane -- etcd --version etcd Version: 3.4.13 Git SHA: ae9734ed2 Go Version: go1.12.17 Go OS/Arch: linux/amd64 ## Download etcd client wget https://github.com/etcd-io/etcd/releases/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz tar xzvf etcd-v3.4.13-linux-amd64.tar.gz sudo mv etcd-v3.4.13-linux-amd64/etcdctl /usr/local/bin ## save etcd snapshot sudo ETCDCTL_API=3 etcdctl snapshot save --endpoints 172.16.1.11:2379 snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key # View the snapshot ETCDCTL_API=3 sudo etcdctl --write-out=table snapshot status snapshot.db +----------+----------+------------+------------+ | HASH | REVISION | TOTAL KEYS | TOTAL SIZE | +----------+----------+------------+------------+ | 4056f9fc | 18821 | 809 | 4.1 MB | +----------+----------+------------+------------+ . ",
    "url": "/my-tech/doc/Kubernetes/doc-1/",
    
    "relUrl": "/doc/Kubernetes/doc-1/"
  },"176": {
    "doc": "Terraform Deployment of basic infrastructure on AWS",
    "title": "Terraform Deployment of basic infrastructure on AWS",
    "content": " ",
    "url": "/my-tech/docs/Terraform/doc-1/",
    
    "relUrl": "/docs/Terraform/doc-1/"
  },"177": {
    "doc": "Terraform Deployment of basic infrastructure on AWS",
    "title": "",
    "content": "Date Posted: 2023 03 05 . Terraform main.tf file to deploy basic infrastructure terraform { required_version = \"&gt;= 0.14.5\" required_providers { local = { source = \"hashicorp/local\" version = \"2.0.0\" } tls = { source = \"hashicorp/tls\" version = \"3.0.0\" } } } provider \"local\" { } provider \"tls\" { } resource \"tls_private_key\" \"ssh_key\" { algorithm = \"RSA\" rsa_bits = 4096 } resource \"local_file\" \"cloud_config\" { filename = \"cloud-config.yaml\" content = &lt;&lt;-EOT write_files: - path: /etc/app/key permissions: '0600' content: | ${tls_private_key.ssh_key.private_key_pem} - path: /etc/app/key.pub permissions: '0644' content: | ${tls_private_key.ssh_key.public_key_openssh} EOT } . ",
    "url": "/my-tech/docs/Terraform/doc-1/",
    
    "relUrl": "/docs/Terraform/doc-1/"
  },"178": {
    "doc": "Docker Tutorial",
    "title": "Docker Tutorial",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-1/",
    
    "relUrl": "/doc/Docker/docker-1/"
  },"179": {
    "doc": "Docker Tutorial",
    "title": "",
    "content": "Date Posted: 2023 02 19 . ",
    "url": "/my-tech/doc/Docker/docker-1/",
    
    "relUrl": "/doc/Docker/docker-1/"
  },"180": {
    "doc": "Initializing the Docker Environment",
    "title": "Initializing the Docker Environment",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-2/",
    
    "relUrl": "/doc/Docker/docker-2/"
  },"181": {
    "doc": "Initializing the Docker Environment",
    "title": "This is the way to initialize the docker environment. This will need to",
    "content": "Date Posted: 2023 02 17 . ",
    "url": "/my-tech/doc/Docker/docker-2/",
    
    "relUrl": "/doc/Docker/docker-2/"
  },"182": {
    "doc": "Initializing the Docker Environment",
    "title": "Installing Docker",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-2/#installing-docker",
    
    "relUrl": "/doc/Docker/docker-2/#installing-docker"
  },"183": {
    "doc": "Initializing the Docker Environment",
    "title": "Install the Docker prerequisites:",
    "content": ". | sudo yum install -y yum-utils device-mapper-persistent-data lvm2 | . ",
    "url": "/my-tech/doc/Docker/docker-2/#install-the-docker-prerequisites",
    
    "relUrl": "/doc/Docker/docker-2/#install-the-docker-prerequisites"
  },"184": {
    "doc": "Initializing the Docker Environment",
    "title": "Using yum-config-manager, add the CentOS-specific Docker repo:",
    "content": ". | sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo | . ",
    "url": "/my-tech/doc/Docker/docker-2/#using-yum-config-manager-add-the-centos-specific-docker-repo",
    
    "relUrl": "/doc/Docker/docker-2/#using-yum-config-manager-add-the-centos-specific-docker-repo"
  },"185": {
    "doc": "Initializing the Docker Environment",
    "title": "Install Docker:",
    "content": ". | sudo yum -y install docker-ce | . ",
    "url": "/my-tech/doc/Docker/docker-2/#install-docker",
    
    "relUrl": "/doc/Docker/docker-2/#install-docker"
  },"186": {
    "doc": "Initializing the Docker Environment",
    "title": "Enable the Docker Daemon",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-2/#enable-the-docker-daemon",
    
    "relUrl": "/doc/Docker/docker-2/#enable-the-docker-daemon"
  },"187": {
    "doc": "Initializing the Docker Environment",
    "title": "Enable the Docker daemon:",
    "content": ". | sudo systemctl enable --now docker | . ",
    "url": "/my-tech/doc/Docker/docker-2/#enable-the-docker-daemon-1",
    
    "relUrl": "/doc/Docker/docker-2/#enable-the-docker-daemon-1"
  },"188": {
    "doc": "Initializing the Docker Environment",
    "title": "Configure User Permissions",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-2/#configure-user-permissions",
    
    "relUrl": "/doc/Docker/docker-2/#configure-user-permissions"
  },"189": {
    "doc": "Initializing the Docker Environment",
    "title": "Add the lab user to the docker group:",
    "content": ". | sudo usermod -aG docker cloud_user . | Caution: You will need to exit the server for the change to take effect. | . | . ",
    "url": "/my-tech/doc/Docker/docker-2/#add-the-lab-user-to-the-docker-group",
    
    "relUrl": "/doc/Docker/docker-2/#add-the-lab-user-to-the-docker-group"
  },"190": {
    "doc": "Initializing the Docker Environment",
    "title": "Run a Test Image",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-2/#run-a-test-image",
    
    "relUrl": "/doc/Docker/docker-2/#run-a-test-image"
  },"191": {
    "doc": "Initializing the Docker Environment",
    "title": "Using docker, run the hello-world image to verify that the environment is set up properly:",
    "content": ". | docker run hello-world | . ",
    "url": "/my-tech/doc/Docker/docker-2/#using-docker-run-the-hello-world-image-to-verify-that-the-environment-is-set-up-properly",
    
    "relUrl": "/doc/Docker/docker-2/#using-docker-run-the-hello-world-image-to-verify-that-the-environment-is-set-up-properly"
  },"192": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Working With Prebuilt Docker Images",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/",
    
    "relUrl": "/doc/Docker/docker-3/"
  },"193": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "This article is to migrate a website, from a traditional server to containers by using Dockers",
    "content": "Date Posted: 2023 02 01 . ",
    "url": "/my-tech/doc/Docker/docker-3/",
    
    "relUrl": "/doc/Docker/docker-3/"
  },"194": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Explore Docker Hub",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/#explore-docker-hub",
    
    "relUrl": "/doc/Docker/docker-3/#explore-docker-hub"
  },"195": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Sign in to Docker Hub.",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/#sign-in-to-docker-hub",
    
    "relUrl": "/doc/Docker/docker-3/#sign-in-to-docker-hub"
  },"196": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "At the top of the page, search for “httpd”.",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/#at-the-top-of-the-page-search-for-httpd",
    
    "relUrl": "/doc/Docker/docker-3/#at-the-top-of-the-page-search-for-httpd"
  },"197": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "In the left-hand menu, filter for Application Infrastructure, and Official Images.",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/#in-the-left-hand-menu-filter-for-application-infrastructure-and-official-images",
    
    "relUrl": "/doc/Docker/docker-3/#in-the-left-hand-menu-filter-for-application-infrastructure-and-official-images"
  },"198": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Select the httpd project.",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/#select-the-httpd-project",
    
    "relUrl": "/doc/Docker/docker-3/#select-the-httpd-project"
  },"199": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "At the top of the page, click the Tags tab.",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/#at-the-top-of-the-page-click-the-tags-tab",
    
    "relUrl": "/doc/Docker/docker-3/#at-the-top-of-the-page-click-the-tags-tab"
  },"200": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Under latest, select linux/amd64.",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/#under-latest-select-linuxamd64",
    
    "relUrl": "/doc/Docker/docker-3/#under-latest-select-linuxamd64"
  },"201": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Back in the list of available images, select nginx",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/#back-in-the-list-of-available-images-select-nginx",
    
    "relUrl": "/doc/Docker/docker-3/#back-in-the-list-of-available-images-select-nginx"
  },"202": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Review the How to use this image section.",
    "content": " ",
    "url": "/my-tech/doc/Docker/docker-3/#review-the-how-to-use-this-image-section",
    
    "relUrl": "/doc/Docker/docker-3/#review-the-how-to-use-this-image-section"
  },"203": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Get and View httpd",
    "content": "## In the Docker Instance, verify that docker is installed: docker ps ## Using docker, pull the httpd:2.4 image: docker pull httpd:2.4 ## Run the image: docker run --name httpd -p 8080:80 -d httpd:2.4 ## Check the status of the container: docker ps ## In a web browser, test connectivity to the container: &lt;PUBLIC_IP_ADDRESS&gt;:8080 . ",
    "url": "/my-tech/doc/Docker/docker-3/#get-and-view-httpd",
    
    "relUrl": "/doc/Docker/docker-3/#get-and-view-httpd"
  },"204": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Run a Copy of the Website in httpd",
    "content": "## Clone the Widget Factory Inc repository git clone https://github.com/linuxacademy/content-widget-factory-inc ## Change to the content-widget-factory-inc directory: cd content-widget-factory-inc ## Check the files: ll ## Move to the web directory: cd web ## Check the files: ll ## Stop the httpd container: docker stop httpd ## Remove the httpd container: docker rm httpd ## Verify that the container has been removed: docker ps -a ## Run the container with the website data: docker run --name httpd -p 8080:80 -v $(pwd):/usr/local/apache2/htdocs:ro -d httpd:2.4 ## Check the status of the container: docker ps ## In a web browser, check connectivity to the container: &lt;PUBLIC_IP_ADDRESS&gt;:8080 . ",
    "url": "/my-tech/doc/Docker/docker-3/#run-a-copy-of-the-website-in-httpd",
    
    "relUrl": "/doc/Docker/docker-3/#run-a-copy-of-the-website-in-httpd"
  },"205": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Get and View Nginx",
    "content": "## Using docker, pull the latest version of nginx: docker pull nginx ## Verify that the image was pulled successfully: docker images ## Run the container using the nginx image: docker run --name nginx -p 8081:80 -d nginx ## Check the status of the container: docker ps ## Verify connectivity to the nginx container: &lt;PUBLIC_IP_ADDRESS&gt;:8081 . ",
    "url": "/my-tech/doc/Docker/docker-3/#get-and-view-nginx",
    
    "relUrl": "/doc/Docker/docker-3/#get-and-view-nginx"
  },"206": {
    "doc": "Working With Prebuilt Docker Images",
    "title": "Run a Copy of the Website in Nginx",
    "content": "## Stop the nginx container: docker stop nginx ## Remove the nginx container: docker rm nginx ## Verify that the container has been removed: docker ps -a ## Run the nginx container, and mount the website data: docker run --name nginx -v $(pwd):/usr/share/nginx/html:ro -p 8081:80 -d nginx ## Check the status of the container: docker ps ## In a web browser, verify connectivity to the container: &lt;PUBLIC_IP_ADDRESS&gt;:8081 ## Stop the nginx container: docker stop nginx ## Remove the nginx container: docker rm nginx ## Verify that the container has been removed: docker ps -a . ",
    "url": "/my-tech/doc/Docker/docker-3/#run-a-copy-of-the-website-in-nginx",
    
    "relUrl": "/doc/Docker/docker-3/#run-a-copy-of-the-website-in-nginx"
  },"207": {
    "doc": "My Page",
    "title": " Welcome to my SRE Page ",
    "content": "I am a seasoned Infrastructure and Platform Engineer with over 8 years of experience in the IT industry, catering to diverse clients. My primary responsibility revolves around ensuring the constant availability and reliability of the infrastructure that hosts our clients' IT solutions. Through extensive hands-on experience in managing IT infrastructure and platforms, I have consistently succeeded in maintaining a reliable and secure environment. To achieve my goal of delivering top-notch services, I am committed to staying updated with the latest technologies. I regularly conduct intensive research to incorporate cutting-edge solutions into our infrastructure. Moreover, I actively maintain a robust set of tools and resources that contribute to the dependability and efficiency of the systems. As a leader in my field, I not only strive for excellence in my work but also inspire and guide my team members to perform at their best. My passion for this profession drives me to constantly seek new challenges and opportunities for growth, ensuring that our clients receive the highest level of service and satisfaction. From: Containerd: https://containerd.io/ . ",
    "url": "/my-tech/",
    
    "relUrl": "/"
  },"208": {
    "doc": "My Page",
    "title": " My Duties and Responsibilities ",
    "content": "AWS . | build Infrastructure on AWS | Deploy several AWS resources and services | Use CloudFormation to deploy and update resources | Terraform . | Build and update multi-plaform infrastructure using Terraform | Jenkins . | Run scheduled, triggered, or manual jobs on Jenkins | Create and manage automated scripts inside Jenkins Jobs | Kubernetes . | Create, operate, and update Openshift and EKS distributions of Kubernetes | Troubleshoot common issues of Kubernetes | Performance Monitoring . | Manage creation of logging, monitoring, and alerting tools such as Prometheus, Grafana, Splunk, etc. | Docker . | Create and operate images and containers that are quickly deployable. | ",
    "url": "/my-tech/",
    
    "relUrl": "/"
  },"209": {
    "doc": "My Page",
    "title": " Posts ",
    "content": "CloudFormation Stack Deployment Examples List of CloudFormation Stack Deployments for AWS resources . 03 May 2023 . a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql this page describes about a tutorial about creating an AWS basic infrastructure, with VPC, public, and private subnets, IGW, NAT, security groups, EC2 and RDS that runs postgresql . 16 Apr 2023 . tutorial about the running a sample docker image in numbered steps this page describes about tutorial about the running a sample docker image in numbered steps . 29 Mar 2023 . Terraform Deployment of basic infrastructure on AWS . 05 Mar 2023 . CICD . 01 Mar 2023 . Misc Front page of documents uncategorized . 01 Mar 2023 . Terraform . 25 Feb 2023 . Docker Tutorial . 19 Feb 2023 . Link . Menu | | | Expand . (external link) . | Document . Search | Copy Copied ",
    "url": "/my-tech/",
    
    "relUrl": "/"
  },"210": {
    "doc": "My Page",
    "title": "My Page",
    "content": " ",
    "url": "/my-tech/",
    
    "relUrl": "/"
  },"211": {
    "doc": "Terraform",
    "title": "Terraform",
    "content": " ",
    "url": "/my-tech/docs/Terraform",
    
    "relUrl": "/docs/Terraform"
  },"212": {
    "doc": "Terraform",
    "title": "",
    "content": "Date Posted: 2023 02 25 . ",
    "url": "/my-tech/docs/Terraform",
    
    "relUrl": "/docs/Terraform"
  },"213": {
    "doc": "Terraform",
    "title": "Table of Contents",
    "content": ". | Table of Contents | Terraform Architecture | Installation . | Windows | Linux (Ubuntu) Package Manager | macOS Package Manager | . | Terraform CLI . | terraform version | terraform -install-autocomplete | terraform fmt . | Options | . | terraform validate | terraform providers | terraform init | terraform plan | terraform apply . | Examples | . | terraform destroy | terraform taint | terraform untaint | terraform refresh | terraform workspace | terraform state . | Examples | . | terraform output | terraform graph | terraform import | terraform login [hostname] | terraform logout [hostname] | HCL Comment Styles | Terraform Providers (Plugins) | Provider Configuration | Terraform Resources | Terraform Variables . | Declaring Variables | Assigning values to variables | String Interpolation | . | Variable Types . | type number | type string | type bool | type list (of strings) | type map | type tuple | type object | . | Data Sources . | Use Data Sources | . | Output Values . | Declare an Output Value | . | Loops . | count | for_each | For Expressions | Splat Expressions | Dynamic Blocks | . | Conditional Expressions | Terraform Locals | Terraform provisioners | Built-in Functions | Backends and Remote State . | Backends | Configure Terraform to use the remote state from within the S3 bucket | . | Terraform Modules | Troubleshooting and Logging | . | . ",
    "url": "/my-tech/docs/Terraform#table-of-contents",
    
    "relUrl": "/docs/Terraform#table-of-contents"
  },"214": {
    "doc": "Terraform",
    "title": "Terraform Architecture",
    "content": ". ",
    "url": "/my-tech/docs/Terraform#terraform-architecture",
    
    "relUrl": "/docs/Terraform#terraform-architecture"
  },"215": {
    "doc": "Terraform",
    "title": "Installation",
    "content": "Windows . ",
    "url": "/my-tech/docs/Terraform#installation",
    
    "relUrl": "/docs/Terraform#installation"
  },"216": {
    "doc": "Terraform",
    "title": "Download the Windows binary for 32 or 64-bit CPUs from https://www.terraform.io/downloads.",
    "content": " ",
    "url": "/my-tech/docs/Terraform#download-the-windows-binary-for-32-or-64-bit-cpus-from-httpswwwterraformiodownloads",
    
    "relUrl": "/docs/Terraform#download-the-windows-binary-for-32-or-64-bit-cpus-from-httpswwwterraformiodownloads"
  },"217": {
    "doc": "Terraform",
    "title": "Unzip the package.",
    "content": " ",
    "url": "/my-tech/docs/Terraform#unzip-the-package",
    
    "relUrl": "/docs/Terraform#unzip-the-package"
  },"218": {
    "doc": "Terraform",
    "title": "Move the Terraform binary to the Windows PATH.",
    "content": "Linux (Ubuntu) Package Manager . ",
    "url": "/my-tech/docs/Terraform#move-the-terraform-binary-to-the-windows-path",
    
    "relUrl": "/docs/Terraform#move-the-terraform-binary-to-the-windows-path"
  },"219": {
    "doc": "Terraform",
    "title": "Run the following commands at the terminal:",
    "content": "``` curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" sudo apt-get update &amp;&amp; sudo apt-get install terraform ``` . ",
    "url": "/my-tech/docs/Terraform#run-the-following-commands-at-the-terminal",
    
    "relUrl": "/docs/Terraform#run-the-following-commands-at-the-terminal"
  },"220": {
    "doc": "Terraform",
    "title": "Install Terraform using the package manager:",
    "content": "``` sudo apt update &amp;&amp; sudo apt install terraform -y ``` . macOS Package Manager . ",
    "url": "/my-tech/docs/Terraform#install-terraform-using-the-package-manager",
    
    "relUrl": "/docs/Terraform#install-terraform-using-the-package-manager"
  },"221": {
    "doc": "Terraform",
    "title": "Run the following commands at the terminal:",
    "content": "``` brew tap hashicorp/tap brew install hashicorp/tap/terraform ``` . ",
    "url": "/my-tech/docs/Terraform#run-the-following-commands-at-the-terminal-1",
    
    "relUrl": "/docs/Terraform#run-the-following-commands-at-the-terminal-1"
  },"222": {
    "doc": "Terraform",
    "title": "Terraform CLI",
    "content": "The Terraform CLI provides a variety of commands to interact with and manage Terraform configurations. Here are some commonly used commands: . terraform version . Displays the version of Terraform and all installed plugins. terraform -install-autocomplete . Sets up tab auto-completion, requires logging back in. terraform fmt . Rewrites all Terraform configuration files to a canonical format. Both configuration files (.tf) and variable files (.tfvars) are updated. Options . | -check : Check if the input is formatted. It does not overwrite the file. | -recursive : Also process files in subdirectories. By default, only the given directory (or current directory) is processed. | . terraform validate . Validates the configuration files for errors. It refers only to the configuration and not accessing any remote services such as remote state, or provider APIs. terraform providers . Prints out a tree of modules in the referenced configuration annotated with their provider requirements. terraform init . Initializes a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc. This is the first command that should be run for any new or existing Terraform configuration per machine. This sets up all the local data necessary to run Terraform that is typically not committed to version control. This command is always safe to run multiple times. | Option | Description | . | -backend=false | Disable backend or Terraform Cloud initialization for this configuration and use what was previously initialized instead. | . | -reconfigure | Reconfigure a backend, ignoring any saved configuration. | . | -migrate-state | Reconfigure a backend and attempt to migrate any existing state. | . | -upgrade | Install the latest module and provider versions allowed within configured constraints, overriding the default behavior of selecting exactly the version recorded in the dependency lockfile. | . terraform plan . Generates an execution plan, showing what actions will Terraform take to apply the current configuration. This command will not actually perform the planned actions. | Option | Description | . | -out=path | Write a plan file to the given path. This can be used as input to the “apply” command. | . | -input=true | Ask for input for variables if not directly set. | . | -var 'foo=bar' | Set a value for one of the input variables in the root module of the configuration. Use this option more than once to set more than one variable. | . | -var-file=filename | Load variable values from the given file, in addition to the default files terraform.tfvars and *.auto.tfvars. Use this option more than once to include more than one variable file. | . | -destroy | Select the “destroy” planning mode, which creates a plan to destroy all objects currently managed by this Terraform configuration instead of the usual behavior. | . | -refresh-only | Select the “refresh only” planning mode, which checks whether remote objects still match the outcome of the most recent Terraform apply but does not propose any actions to undo any changes made outside of Terraform. | . | -target=resource | Limit the planning operation to only the given module, resource, or resource instance and all of its dependencies. You can use this option multiple times to include more than one object. This is for exceptional use only. | . terraform apply . Creates or updates infrastructure according to Terraform configuration files in the current directory. | Option | Description | . | -auto-approve | Skip interactive approval of plan before applying. | . | -replace | Force replacement of a particular resource instance using its resource address. | . | -var ‘foo=bar’ | Set a value for one of the input variables in the root module of the configuration. Use this option more than once to set more than one variable. | . | -var-file=filename | Load variable values from the given file, in addition to the default files terraform.tfvars and *.auto.tfvars. Use this option more than once to include more than one variable file. | . | -parallelism=n | Limit the number of concurrent operations. Defaults to 10. | . Examples . terraform apply -auto-approve -var-file=ec2.tfvars terraform apply -replace=\"aws_instance.web\" . terraform destroy . Destroys Terraform-managed infrastructure and is an alias for terraform apply -destroy. | Option | Description | . | -auto-approve | Skip interactive approval before destroying. | . | -target | Limit the destroying operation to only the given resource and all of its dependencies. You can use this option multiple times to include more than one object. | . Example: terraform destroy -target aws_vpc.my_vpc -auto-approve . terraform taint . Describes a resource instance that may not be fully functional, either because its creation partially failed or because you’ve manually marked it as such using this command. Subsequent Terraform plans will include actions to destroy the remote object and create a new object to replace it. terraform untaint . Removes that state from a resource instance, causing Terraform to see it as fully-functional and not in need of replacement. terraform refresh . Updates the state file of your infrastructure with metadata that matches the physical resources they are tracking. This will not modify your infrastructure, but it can modify your state file to update metadata. terraform workspace . | Option | Description | . | delete | Delete a workspace. | . | list | List workspaces. | . | new | Create a new workspace. | . | select | Select a workspace. | . | show | Show the name of the current workspace. | . terraform state . This does advanced state management. The state is stored by default in a local file named “terraform.tfstate”, but it can also be stored remotely, which works better in a team environment. | Option | Description | . | list | List resources in the state. | . | show | Show a resource in the state. | . | mv | Move an item in the state. | . | rm | Remove instances from the state. | . | pull | Pull current state and output to stdout. | . Examples . terraform state show aws_instance.web terraform state pull &gt; my_terraform.tfstate terraform state mv aws_iam_role.my_instance_role terraform state list terraform state rm aws_instance.web . terraform output . Reads an output variable from a Terraform state file and prints the value. With no additional arguments, output will display all the outputs for the root module. Examples: . | terraform output [-json]: Lists all outputs in the state file. | terraform output instance_public_ip: Lists a specific output value. | . terraform graph . Produces a representation of the dependency graph between different objects in the current configuration and state. The graph is presented in the DOT language. The typical program that can read this format is GraphViz, but many web services are also available to read this format. Linux Example: . sudo apt install graphviz terraform graph | dot -Tpng &gt; graph.png . terraform import . Import existing infrastructure into your Terraform state. This will find and import the specified resource into your Terraform state, allowing existing infrastructure to come under Terraform management without having to be initially created by Terraform. Example: terraform import aws_instance.new_server i-123abc . Imports EC2 instance with id i-abc123 into the Terraform resource named “new_server” of type “aws_instance”. terraform login [hostname] . Retrieves an authentication token for the given hostname, if it supports automatic login, and saves it in a credentials file in your home directory. If no hostname is provided, the default hostname is app.terraform.io, to log in to Terraform Cloud. terraform logout [hostname] . Removes locally-stored credentials for the specified hostname. If no hostname is provided, the default hostname is app.terraform.io. HCL Comment Styles . | #: single-line comment. | //: single-line comment (alternative to #). | /* ... */: multi-line comment (block comment). | . Terraform Providers (Plugins) . A provider is a Terraform plugin that allows users to manage an external API. A provider usually provides resources to manage a cloud or infrastructure platform, such as AWS or Azure, or technology (for example Kubernetes). There are providers for Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). Provider Configuration . terraform { required_providers { aws = { source = \"hashicorp/aws\" # global and unique source address version = \"~&gt; 3.0\" # version constraint } } } # Configure the AWS Provider provider \"aws\" { region = \"eu-west-1\" # provider configuration options } . Terraform Resources . Resources are the most important element in the Terraform language. It describes one or more infrastructure objects to manage. Together the resource type and local name serve as an identifier for a given resource and must be unique within a module. Example: aws_vpc.main . Creating resources: . resource \"&lt;provider&gt;_&lt;resource_type&gt;\" \"local_name\"{ argument1 = value argument2 = value … } # Example: resource \"aws_vpc\" \"main\" { cidr_block = \"10.0.0.0/16\" enable_dns_support = true tags = { \"Name\" = \"Main VPC\" } } . Terraform Variables . Input variables allow you customize aspects of Terraform without using hard-coded values in the source. Declaring Variables . Variable declarations can appear anywhere in your configuration files. However, it’s recommended to put them into a separate file called variables.tf. # variable declaration variable \"vpc_cidr_block\" { description = \"CIDR block for VPC\". default = \"192.168.0.0/16\" type = string } . Assigning values to variables . ",
    "url": "/my-tech/docs/Terraform#terraform-cli",
    
    "relUrl": "/docs/Terraform#terraform-cli"
  },"223": {
    "doc": "Terraform",
    "title": "Using the default argument in the variable declaration block.",
    "content": " ",
    "url": "/my-tech/docs/Terraform#using-the-default-argument-in-the-variable-declaration-block",
    
    "relUrl": "/docs/Terraform#using-the-default-argument-in-the-variable-declaration-block"
  },"224": {
    "doc": "Terraform",
    "title": "Assign a value to the variable in the variable definition file which by default is terraform.tfvars. Example: vpc_cidr_block = “172.16.0.0/16”",
    "content": " ",
    "url": "/my-tech/docs/Terraform#assign-a-value-to-the-variable-in-the-variable-definition-file-which-by-default-is-terraformtfvars-example-vpc_cidr_block--172160016",
    
    "relUrl": "/docs/Terraform#assign-a-value-to-the-variable-in-the-variable-definition-file-which-by-default-is-terraformtfvars-example-vpc_cidr_block--172160016"
  },"225": {
    "doc": "Terraform",
    "title": "Using -var command-line option. Example: terraform apply -var=”vpc_cidr_block=10.0.10.0/24”",
    "content": " ",
    "url": "/my-tech/docs/Terraform#using--var-command-line-option-example-terraform-apply--varvpc_cidr_block10010024",
    
    "relUrl": "/docs/Terraform#using--var-command-line-option-example-terraform-apply--varvpc_cidr_block10010024"
  },"226": {
    "doc": "Terraform",
    "title": "Using -var-file command-line option. Example: terraform apply -auto-approve -var-file=web-prod.tfvars",
    "content": " ",
    "url": "/my-tech/docs/Terraform#using--var-file-command-line-option-example-terraform-apply--auto-approve--var-fileweb-prodtfvars",
    
    "relUrl": "/docs/Terraform#using--var-file-command-line-option-example-terraform-apply--auto-approve--var-fileweb-prodtfvars"
  },"227": {
    "doc": "Terraform",
    "title": "Exporting the variable at the terminal. Example: export TF_VAR_vpc_cidr_block=”192.168.100.0/24”",
    "content": "Variable definition precedence (from highest to lowest): . ",
    "url": "/my-tech/docs/Terraform#exporting-the-variable-at-the-terminal-example-export-tf_var_vpc_cidr_block192168100024",
    
    "relUrl": "/docs/Terraform#exporting-the-variable-at-the-terminal-example-export-tf_var_vpc_cidr_block192168100024"
  },"228": {
    "doc": "Terraform",
    "title": "Variables specified at the terminal using -var and -var-file options.",
    "content": " ",
    "url": "/my-tech/docs/Terraform#variables-specified-at-the-terminal-using--var-and--var-file-options",
    
    "relUrl": "/docs/Terraform#variables-specified-at-the-terminal-using--var-and--var-file-options"
  },"229": {
    "doc": "Terraform",
    "title": "Variables defined in terraform.tfvars.",
    "content": " ",
    "url": "/my-tech/docs/Terraform#variables-defined-in-terraformtfvars",
    
    "relUrl": "/docs/Terraform#variables-defined-in-terraformtfvars"
  },"230": {
    "doc": "Terraform",
    "title": "Variables defined as environment variables using TF_VAR prefix.",
    "content": "String Interpolation . You can interpolate other values in strings by these values in ${}, such as ${var.foo}. The interpolation syntax is powerful and allows you to reference variables, attributes of resources, call functions, etc. You can escape interpolation with double dollar signs: $${foo} will be rendered as a literal ${foo}. Variable Types . | Simple types a. number b. string c. bool d. null | Complex types a. Collection types i. list ii. map iii. set b. Structural types i. tuple object | . type number . variable \"web_port\" { description = \"Web Port\" default = 80 type = number } . type string . variable \"aws_region\" { description = \"AWS Region\" type = string default = \"eu-west-1\" } . type bool . variable \"enable_dns\" { description = \"DNS Support for the VPC\" type = bool default = true } . type list (of strings) . type list (of strings) variable \"azs\" { description = \"AZs in the Region\" type = list(string) default = [ \"eu-west-1a\", \"eu-west-1b\", \"eu-west-1c\" ] } . type map . variable \"amis\" { type = map(string) default = { \"eu-central-1\" = \"ami-0dcc0ebde7b2e00db\", \"us-west-1\" = \"ami-04a50faf2a2ec1901\" } } . type tuple . variable \"my_instance\" { type = tuple([string, number, bool]) default = [\"t2.micro\", 1, true ] } . type object . variable \"egress_dsg\" { type = object({ from_port = number to_port = number protocol = string cidr_blocks = list(string) }) default = { from_port = 0, to_port = 65365, protocol = \"tcp\", cidr_blocks = [\"100.0.0.0/16\", \"200.0.0.0/16\", \"0.0.0.0/0\"] } } . Data Sources . Data sources in Terraform are used to get information about resources external to Terraform. For example, the public IP address of an EC2 instance. Data sources are provided by providers. Use Data Sources . A data block requests that Terraform read from a given data source (“aws_ami”) and export the result under the given local name (“ubuntu”). The data source and name together serve as an identifier for a given resource and therefore must be unique within a module. Within the block body (between { and }) are query constraints defined by the data source. data \"aws_ami\" \"ubuntu\" { most_recent = true owners = [\"self\"] tags = { Name = \"app-server\" Tested = \"true\" } } . Output Values . Output values print out information about your infrastructure at the terminal, and can expose information for other Terraform configurations (e.g. modules) to use. Declare an Output Value . Each output value exported by a module must be declared using an output block. The label immediately after the output keyword is the name. output \"instance_ip_addr\" { value = aws_instance.server.private_ip } . Loops . Terraform offers the following looping constructs, each intended to be used in a slightly different scenario: . | count meta-argument: loop over resources. | for_each meta-argument: loop over resources and inline blocks within a resource. | for expressions: loop over lists and maps. | . count . The count meta-argument is defined by the Terraform language and can be used to manage similar resources. count is a looping technique and can be used with modules and with every resource type. # creating 3 EC2 instances using count resource \"aws_instance\" \"web\" { ami = \"ami-06ec8443c2a35b0ba\" instance_type = \"t2.micro\" count = 3 } . In blocks where count is set, an additional count object is available. count.index represents the distinct index number (starting with 0) corresponding to the current object. for_each . for_each is another meta-argument used to duplicate resources that are similar but need to be configured differently. for_each was introduced more recently to overcome the downsides of count. If your resources are almost identical, count is appropriate. If some of their arguments need distinct values that can’t be directly derived from an integer, it’s safer to use for_each. # declaring a variable variable \"users\" { type = list(string) default = [\"demo\", \"test\", \"john\"] } # creating IAM users resource \"aws_iam_user\" \"user\" { for_each = toset(var.users) # converts a list to a set name = each.key } . Second example . variable \"example_map\" { type = map(string) default = { \"key1\" = \"value1\" \"key2\" = \"value2\" \"key3\" = \"value3\" } } #use the var resource \"aws_s3_bucket\" \"example\" { for_each = var.example_map bucket = each.key } . For Expressions . A for expression creates a complex type value by transforming another complex type value. variable \"names\" { type = list default = [\"daniel\", \"ada'\", \"john wick\"] } output \"short_upper_names\" { # filter the resulting list by specifying a condition: value = [for name in var.names : upper(name) if length(name) &gt; 7] } . If you run terraform apply -auto-approve you’ll get: . Outputs: short_upper_names = [ \"JOHN WICK\", ] . Splat Expressions . A splat expression provides a more concise way to express a common operation that could otherwise be performed with a for expression. Dynamic Blocks . Dynamic blocks act much like a for expression, but produce nested blocks instead of a complex typed value. They iterate over a given complex value, and generate a nested block for each element of that complex value. They are supported inside resource, data, provider, and provisioner blocks. A dynamic block produces nested blocks instead of a complex typed value. # Declaring a variable of type list variable \"ingress_ports\" { description = \"List Of Ingress Ports\" type = list(number) default = [22, 80, 110, 143] } resource \"aws_default_security_group\" \"default_sec_group\" { vpc_id = aws_vpc.main.id # Creating the ingress rules using dynamic blocks dynamic \"ingress\"{ # it produces ingress nested blocks for_each = var.ingress_ports # iterating over the list variable iterator = iport content { from_port = iport.value to_port = iport.value protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } } } . A second example . variable \"security_group_rules\" { type = map(object({ type = string from_port = number to_port = number protocol = string cidr_blocks = list(string) })) default = { \"ssh\" = { type = \"ssh\" from_port = 22 to_port = 22 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] }, \"http\" = { type = \"http\" from_port = 80 to_port = 80 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } } } #use it now resource \"aws_security_group\" \"example\" { dynamic \"ingress\" { for_each = var.security_group_rules content { type = ingress.value.type from_port = ingress.value.from_port to_port = ingress.value.to_port protocol = ingress.value.protocol cidr_blocks = ingress.value.cidr_blocks } } } . Conditional Expressions . A conditional expression uses the value of a boolean expression to select one of two values. Syntax: condition ? true_val : false_val . If condition is true then the result is true_val. If condition is false then the result is false_val. The condition can be any expression that resolves to a boolean value. This will usually be an expression that uses the equality, comparison, or logical operators. variable \"iscreate\" { type = bool default = true } # Creating the test-server instance if `iscreate` equals true resource \"aws_instance\" \"test-server\" { ami = \"ami-05cafdf7c9f772ad2\" instance_type = \"t2.micro\" count = var.iscreate == true ? 1:0 # conditional expression } # Creating the prod-server instance if `iscreate` equals false resource \"aws_instance\" \"prod-server\" { ami = \"ami-05cafdf7c9f772ad2\" instance_type = \"t2.large\" # it's not free tier eligible count = var.iscreate == false ? 1:0 # conditional expression } . Terraform Locals . Terraform local values are named values that you can refer to in your configuration. Compared to variables, Terraform locals do not change values during or between Terraform runs and unlike input variables, locals are not submitted by users but calculated inside the configuration. Locals are available only in the current module. They are locally scoped. # the local values are declared in a single `locals` block locals { owner = \"Saif\" cidr_blocks = [\"172.16.10.0/24\", \"172.16.20.0/24\", \"172.16.30.0/24\"] common-tags = { Name = \"dev\" Environment = \"development\" Version = 1.10 } } # Create a VPC. resource \"aws_vpc\" \"dev_vpc\" { cidr_block = \"172.16.0.0/16\" tags = local.common-tags } # Create a subnet in the VPC resource \"aws_subnet\" \"dev_subnets\" { vpc_id = aws_vpc.dev_vpc.id cidr_block = local.cidr_blocks[0] availability_zone = \"eu-west-1a\" tags = local.common-tags } # Create an Internet Gateway Resource resource \"aws_internet_gateway\" \"dev_igw\" { vpc_id = aws_vpc.dev_vpc.id tags = { \"Name\" = \"${local.common-tags[\"Name\"]}-igw\" \"Version\" = \"${local.common-tags[\"Version\"]}\" } } . Note: Local values are created by a locals block (plural), but you reference them as attributes on an object named local (singular). Terraform provisioners . Terraform Provisioners are used to execute scripts or commands on a resource after it has been created or destroyed. Provisioners are typically used to configure a newly created resource, such as installing software or executing a post-creation script. In Terraform, there are two types of provisioners: . Local-exec Provisioner: This provisioner is used to execute commands on the machine running Terraform. Example . resource \"aws_instance\" \"example\" { ami = \"ami-0c55b159cbfafe1f0\" instance_type = \"t2.micro\" provisioner \"local-exec\" { command = \"echo ${aws_instance.example.private_ip} &gt; private_ip.txt\" } } . Remote-exec Provisioner: This provisioner is used to execute commands on the newly created resource. resource \"aws_instance\" \"example\" { ami = \"ami-0c55b159cbfafe1f0\" instance_type = \"t2.micro\" key_name = \"my_key_pair\" provisioner \"remote-exec\" { inline = [ \"sudo apt-get update\", \"sudo apt-get install -y nginx\", \"sudo service nginx start\" ] connection { type = \"ssh\" user = \"ubuntu\" private_key = file(\"~/.ssh/my_key_pair.pem\") host = aws_instance.example.public_ip } } } . Built-in Functions . Terraform includes a number of built-in functions that can be called from within expressions to transform and combine values. Examples of functions: min, max, file, concat, element, index, lookup. Terraform does not support user-defined functions. There are functions for numbers, strings, collections, file system, date and time, IP Network, Type Conversions and more. You can experiment with the behavior of Terraform’s built-in functions from the Terraform console, by running the terraform console command. Examples: . &gt; max(1, 12, 10) 12 &gt; min(12, 54, 9) 9 &gt; format(\"There are %d lights\", 4) There are 4 lights &gt; join(\", \", [\"foo\", \"bar\"]) foo, bar &gt; split(\",\", \"foo,bar\") [ \"foo\", \"bar\" ] &gt; replace(\"hello world\", \"/w.*d/\", \"everybody\") hello everybody &gt; substr(\"hello world\", 1, 4) ello &gt; element([\"a\", \"b\", \"c\"], 1) b &gt; lookup({a=\"ay\", b=\"bee\"}, \"a\", \"what?\") ay &gt; lookup({a=\"ay\", b=\"bee\"}, \"c\", \"what?\") what? &gt; slice([\"a\", \"b\", \"c\", \"d\"], 1, 3) [ \"b\", \"c\", ] &gt; timestamp() \"2023-25-02T13:52:18Z\" &gt; formatdate(\"DD MMM YYYY hh:mm ZZZ\", \"2023-25-02T13:52:18Z\") 25 Feb 2023 13:52 UTC &gt; cidrhost(\"10.1.2.240/28\", 1) 10.1.2.241 &gt; cidrhost(\"10.1.2.240/28\", 14) 10.1.2.254 . Backends and Remote State . Backends . Each Terraform configuration has an associated backend that defines how operations are executed and where the Terraform state is stored. The default backend is local, and it stores the state as a plain file in the current working directory. The backend needs to be initialized by running terraform init. If you switch the backend, Terraform provides a migration option which is terraform init -migrate-state. Terraform supports both local and remote backends: . local (default) backend stores state in a local JSON file on disk. remote backends stores state remotely. Examples of remote backends are AzureRM, Consul, GCS, Amazon S3, and Terraform Cloud. They can support features like remote operation, state locking, encryption, and versioning. Configure Remote State on Amazon S3 On the AWS console go to Amazon S3 and create a bucket. Configure Terraform to use the remote state from within the S3 bucket . | On the AWS console go to Amazon S3 and create a bucket. | ",
    "url": "/my-tech/docs/Terraform#variables-defined-as-environment-variables-using-tf_var-prefix",
    
    "relUrl": "/docs/Terraform#variables-defined-as-environment-variables-using-tf_var-prefix"
  },"231": {
    "doc": "Terraform",
    "title": "Configure Terraform to use the remote state from within the S3 bucket.",
    "content": "| . terraform { backend \"s3\" { bucket = \"terraform-state\" key = \"s3-backend.tfstate\" region = \"eu-west-1\" } } . | Run terraform init to initialize the backend. | . Terraform Modules . Terraform modules are a powerful way to reuse code and stick to the DRY principle, which stands for Do Not Repeat Yourself. Think of modules as functions in a programming language. Modules will help you organize configuration, encapsulate configuration, re-use configuration and provide consistency and ensure best-practices. Terraform supports Local and Remote modules: . | Local modules are stored locally, in a separate directory, outside of the root environment and have the source path prefixed with ./ or ../ . | Remote modules are stored externally in a separate repository, and support versioning. External Terraform modules are found on the Terraform Registry. | . A Terraform module is a set of Terraform configuration files in a single directory. When you run Terraform commands like terraform plan or terraform apply directly from such a directory, then that directory will be considered the root module. The modules that are imported from other directories into the root module are called child modules. Calling a child module from within the root module: . module \"myec2\" { # path to the module's directory # the source argument is mandatory for all modules. source = \"../modules/ec2\" # module inputs ami_id = var.ami_id instance_type = var.instance_type servers = var.servers } . It’s good practice to start building everything as a module, create a library of modules to share with your team and from the very beginning to start thinking of your entire infrastructure as a collection of reusable modules. After adding or removing a module, you must re-run terraform init to install the module. Troubleshooting and Logging . The TF_LOG enables logging and can be set to one of the following log levels: TRACE, DEBUG, INFO, WARN or ERROR. Once you have configured your logging you can save the output to a file. This is useful for further inspection. The TF_LOG_PATH variable will create the specified file and append the logs generated by Terraform. Example: . export TF_LOG_PATH=terraform.log terraform apply . You can generate logs from the core application and the Terraform provider separately. To enable core logging, set the TF_LOG_CORE environment variable, and to generate provider logs set the TF_LOG_PROVIDER to the appropriate log level. ",
    "url": "/my-tech/docs/Terraform#configure-terraform-to-use-the-remote-state-from-within-the-s3-bucket-1",
    
    "relUrl": "/docs/Terraform#configure-terraform-to-use-the-remote-state-from-within-the-s3-bucket-1"
  },"232": {
    "doc": "AWS",
    "title": "AWS",
    "content": " ",
    "url": "/my-tech/docs/AWS",
    
    "relUrl": "/docs/AWS"
  },"233": {
    "doc": "AWS",
    "title": "",
    "content": "Date Posted: 2023 02 01 . ",
    "url": "/my-tech/docs/AWS",
    
    "relUrl": "/docs/AWS"
  },"234": {
    "doc": "AWS",
    "title": "This is the first page of deploying AWS resources",
    "content": " ",
    "url": "/my-tech/docs/AWS#this-is-the-first-page-of-deploying-aws-resources",
    
    "relUrl": "/docs/AWS#this-is-the-first-page-of-deploying-aws-resources"
  },"235": {
    "doc": "Kubernetes",
    "title": "Kubernetes",
    "content": " ",
    "url": "/my-tech/docs/Kubernetes",
    
    "relUrl": "/docs/Kubernetes"
  },"236": {
    "doc": "Kubernetes",
    "title": "",
    "content": "Date Posted: 2023 02 04 . ",
    "url": "/my-tech/docs/Kubernetes",
    
    "relUrl": "/docs/Kubernetes"
  },"237": {
    "doc": "Kubernetes",
    "title": "Objective 1: Cluster Architecture, Installation &amp; Configuration",
    "content": ". | Objective 1: Cluster Architecture, Installation \\&amp; Configuration . | 1.1 Manage Role Based Access Control (RBAC) . | Lab Environment | Lab Practice | . | 1.2 Use Kubeadm to Install a Basic Cluster . | Kubeadm Tasks for All Nodes | Kubeadm Tasks for Single Control Node | Kubeadm Tasks for Worker Node(s) | Kubeadm Troubleshooting | Kubeadm Optional Tasks | . | 1.3 Manage A Highly-Available Kubernetes Cluster . | HA Deployment Types | Upgrading from Single Control-Plane to High Availability | . | 1.4 Provision Underlying Infrastructure to Deploy a Kubernetes Cluster | 1.5 Perform a Version Upgrade on a Kubernetes Cluster using Kubeadm . | First Control Plane Node | Additional Control Plane Nodes | Upgrade Control Plane Node Kubectl And Kubelet Tools | Upgrade Worker Nodes | . | 1.6 Implement Etcd Backup And Restore . | Snapshot The Keyspace | Restore From Snapshot | . | . | Objective 2: Workloads \\&amp; Scheduling . | 2.1 Understand Deployments And How To Perform Rolling Update And Rollbacks . | Create Deployment | Perform Rolling Update | Perform Rollbacks | . | 2.2 Use Configmaps And Secrets To Configure Applications . | Configmaps | Secrets | Other Concepts | . | 2.3 Know How To Scale Applications | 2.4 Understand The Primitives Used To Create Robust, Self-Healing, Application Deployments | 2.5 Understand How Resource Limits Can Affect Pod Scheduling | 2.6 Awareness Of Manifest Management And Common Templating Tools | . | Objective 3: Services \\&amp; Networking . | 3.1 Understand Host Networking Configuration On The Cluster Nodes | 3.2 Understand Connectivity Between Pods | 3.3 Understand ClusterIP, NodePort, LoadBalancer Service Types And Endpoints . | ClusterIP | NodePort | LoadBalancer | ExternalIP | ExternalName | Networking Cleanup for Objective 3.3 | . | 3.4 Know How To Use Ingress Controllers And Ingress Resources | 3.5 Know How To Configure And Use CoreDNS | 3.6 Choose An Appropriate Container Network Interface Plugin | . | Objective 4: Storage . | 4.1 Understand Storage Classes, Persistent Volumes . | Storage Classes | Persistent Volumes | . | 4.2 Understand Volume Mode, Access Modes And Reclaim Policies For Volumes . | Volume Mode | Access Modes | Reclaim Policies | . | 4.3 Understand Persistent Volume Claims Primitive | 4.4 Know How To Configure Applications With Persistent Storage | . | Objective 5: Troubleshooting . | 5.1 Evaluate Cluster And Node Logging . | Cluster Logging | Node Logging | . | 5.2 Understand How To Monitor Applications | 5.3 Manage Container Stdout \\&amp; Stderr Logs | 5.4 Troubleshoot Application Failure | 5.5 Troubleshoot Cluster Component Failure | 5.6 Troubleshoot Networking | . | . ",
    "url": "/my-tech/docs/Kubernetes#objective-1-cluster-architecture-installation--configuration",
    
    "relUrl": "/docs/Kubernetes#objective-1-cluster-architecture-installation--configuration"
  },"238": {
    "doc": "Kubernetes",
    "title": "1.1 Manage Role Based Access Control (RBAC)",
    "content": "Documentation and Resources: . | Kubectl Cheat Sheet | Using RBAC Authorization | A Practical Approach to Understanding Kubernetes Authorization | . RBAC is handled by roles (permissions) and bindings (assignment of permissions to subjects): . | Object | Description | . | Role | Permissions within a particular namespace | . | ClusterRole | Permissions to non-namespaced resources; can be used to grant the same permissions as a Role | . | RoleBinding | Grants the permissions defined in a role to a user or set of users | . | ClusterRoleBinding | Grant permissions across a whole cluster | . Lab Environment . If desired, use a managed Kubernetes cluster, such as Amazon EKS, to immediately begin working with RBAC. The command aws --region REGION eks update-kubeconfig --name CLUSTERNAME will generate a .kube configuration file on your workstation to permit kubectl commands. Lab Practice . Create the wahlnetwork1 namespace. kubectl create namespace wahlnetwork1 . Create a deployment in the wahlnetwork1 namespace using the image of your choice: . ",
    "url": "/my-tech/docs/Kubernetes#11-manage-role-based-access-control-rbac",
    
    "relUrl": "/docs/Kubernetes#11-manage-role-based-access-control-rbac"
  },"239": {
    "doc": "Kubernetes",
    "title": "kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4 -n wahlnetwork1",
    "content": " ",
    "url": "/my-tech/docs/Kubernetes#kubectl-create-deployment-hello-node---imagek8sgcrioechoserver14--n-wahlnetwork1",
    
    "relUrl": "/docs/Kubernetes#kubectl-create-deployment-hello-node---imagek8sgcrioechoserver14--n-wahlnetwork1"
  },"240": {
    "doc": "Kubernetes",
    "title": "kubectl create deployment busybox --image=busybox -n wahlnetwork1 -- sleep 2000",
    "content": "You can view the yaml file by adding --dry-run=client -o yaml to the end of either deployment. apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: hello-node name: hello-node namespace: wahlnetwork1 spec: replicas: 1 selector: matchLabels: app: hello-node strategy: {} template: metadata: creationTimestamp: null labels: app: hello-node spec: containers: - image: k8s.gcr.io/echoserver:1.4 name: echoserver resources: {} . Create the pod-reader role in the wahlnetwork1 namespace. kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods -n wahlnetwork1 . Alternatively, use kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods -n wahlnetwork1 --dry-run=client -o yaml to output a proper yaml configuration. apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: null name: pod-reader namespace: wahlnetwork1 rules: - apiGroups: - \"\" resources: - pods verbs: - get - list - watch . Create the read-pods rolebinding between the role named pod-reader and the user spongebob in the wahlnetwork1 namespace. kubectl create rolebinding --role=pod-reader --user=spongebob read-pods -n wahlnetwork1 . Alternatively, use kubectl create rolebinding --role=pod-reader --user=spongebob read-pods -n wahlnetwork1 --dry-run=client -o yaml to output a proper yaml configuration. apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: creationTimestamp: null name: read-pods roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: pod-reader subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: spongebob . Create the cluster-secrets-reader clusterrole. kubectl create clusterrole cluster-secrets-reader --verb=get --verb=list --verb=watch --resource=secrets . Alternatively, use kubectl create clusterrole cluster-secrets-reader --verb=get --verb=list --verb=watch --resource=secrets --dry-run=client -o yaml to output a proper yaml configuration. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: creationTimestamp: null name: cluster-secrets-reader rules: - apiGroups: - \"\" resources: - secrets verbs: - get - list - watch . Create the cluster-read-secrets clusterrolebinding between the clusterrole named cluster-secrets-reader and the user gizmo. kubectl create clusterrolebinding --clusterrole=cluster-secrets-reader --user=gizmo cluster-read-secrets . Alternatively, use kubectl create clusterrolebinding --clusterrole=cluster-secrets-reader --user=gizmo cluster-read-secrets --dry-run=client -o yaml to output a proper yaml configuration. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: creationTimestamp: null name: cluster-read-secrets roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-secrets-reader subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: gizmo . Test to see if this works by running the auth command. kubectl auth can-i get secrets --as=gizmo . Attempt to get secrets as the gizmo user. kubectl get secrets --as=gizmo . NAME TYPE DATA AGE default-token-lz87v kubernetes.io/service-account-token 3 7d1h . ",
    "url": "/my-tech/docs/Kubernetes#kubectl-create-deployment-busybox---imagebusybox--n-wahlnetwork1----sleep-2000",
    
    "relUrl": "/docs/Kubernetes#kubectl-create-deployment-busybox---imagebusybox--n-wahlnetwork1----sleep-2000"
  },"241": {
    "doc": "Kubernetes",
    "title": "1.2 Use Kubeadm to Install a Basic Cluster",
    "content": "Official documentation: Creating a cluster with kubeadm . Terraform code is available here to create the resources necessary to experiment with kubeadm . Kubeadm Tasks for All Nodes . | Create Amazon EC2 Instances . | Create an AWS Launch Template using an Ubuntu 18.04 LTS image (or newer) of size t3a.small (2 CPU, 2 GiB Memory). | Disable the swap file. | Note: This can be validated by using the console command free when SSH’d to the instance. The swap space total should be 0. | . | Consume this template as part of an Auto Scaling Group of 1 or more instances. This makes deployment of new instances and removal of old instances trivial. | . | Configure iptables . | This allows iptables to see bridged traffic. | . | Install the Docker container runtime . | The docker-install script is handy for this. | . | Install kubeadm, kubelet, and kubectl | . Alternatively, use a user-data bash script attached to the Launch Template: . #!/bin/bash # Disable Swap sudo swapoff -a # Bridge Network sudo modprobe br_netfilter sudo cat &lt;&lt;'EOF' | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system # Install Docker sudo curl -fsSL https://get.docker.com -o /home/ubuntu/get-docker.sh sudo sh /home/ubuntu/get-docker.sh # Install Kube tools sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat &lt;&lt;'EOF' | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl . Optionally, add sudo kubeadm config images pull to the end of the script to pre-pull images required for setting up a Kubernetes cluster. $ sudo kubeadm config images pull [config/images] Pulled k8s.gcr.io/kube-apiserver:v1.19.2 [config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.19.2 [config/images] Pulled k8s.gcr.io/kube-scheduler:v1.19.2 [config/images] Pulled k8s.gcr.io/kube-proxy:v1.19.2 [config/images] Pulled k8s.gcr.io/pause:3.2 [config/images] Pulled k8s.gcr.io/etcd:3.4.13-0 [config/images] Pulled k8s.gcr.io/coredns:1.7.0 . Kubeadm Tasks for Single Control Node . | Initialize the cluster . | Choose your Container Network Interface (CNI) plugin. This guide uses Calico’s CNI. | Run sudo kubeadm init --pod-network-cidr=192.168.0.0/16 to initialize the cluster and provide a pod network aligned to Calico’s default configuration. | Write down the kubeadm join output to join worker nodes later in this guide. | Example kubeadm join 10.0.0.100:6443 --token 12345678901234567890 --discovery-token-ca-cert-hash sha256:123456789012345678901234567890123456789012345678901234567890 | . | . | Install Calico | Configure local kubectl access . | This step simply copies the admin.conf file into a location accessible for a regular user. | . | . Alternatively, use the Flannel CNI. | Run sudo kubeadm init --pod-network-cidr=10.244.0.0/16 to initialize the cluster and provide a pod network aligned to Flannel’s default configuration. | Note: The kube-flannel.yml file is hosted in the same location. | . | . Kubeadm Tasks for Worker Node(s) . | Join the cluster . | Note: You can view the cluster config with kubectl config view. This includes the cluster server address (e.g. server: https://10.0.0.100:6443) | . | . Kubeadm Troubleshooting . | If using kubeadm init without a pod network CIDR the CoreDNS pods will remain stuck in pending state | Broke cluster and want to start over? Use kubeadm reset and rm -rf .kube in the user home directory to remove the old config and avoid TLS certificate errors | If seeing error: error loading config file \"/etc/kubernetes/admin.conf\": open /etc/kubernetes/admin.conf: permission denied it likely means the KUBECONFIG variable is set to that path, try unset KUBECONFIG to use the $HOME/.kube/config file. | . Kubeadm Optional Tasks . | Install kubectl client locally on Windows for those using this OS. | Single node cluster? Taint the control node to accept pods without dedicated worker nodes. | Deploy the “hello-node” app from the minikube tutorial to test basic functionality. | . ",
    "url": "/my-tech/docs/Kubernetes#12-use-kubeadm-to-install-a-basic-cluster",
    
    "relUrl": "/docs/Kubernetes#12-use-kubeadm-to-install-a-basic-cluster"
  },"242": {
    "doc": "Kubernetes",
    "title": "1.3 Manage A Highly-Available Kubernetes Cluster",
    "content": "High Availability Production Environment . Kubernetes Components for HA: . | Load Balancer / VIP | DNS records | etcd Endpoint | Certificates | Any HA specific queries / configuration / settings | . HA Deployment Types . | With stacked control plane nodes. This approach requires less infrastructure. The etcd members and control plane nodes are co-located. | With an external etcd cluster. This approach requires more infrastructure. The control plane nodes and etcd members are separated. (source) | . Upgrading from Single Control-Plane to High Availability . If you have plans to upgrade this single control-plane kubeadm cluster to high availability you should specify the –control-plane-endpoint to set the shared endpoint for all control-plane nodes. Such an endpoint can be either a DNS name or an IP address of a load-balancer. (source) . ",
    "url": "/my-tech/docs/Kubernetes#13-manage-a-highly-available-kubernetes-cluster",
    
    "relUrl": "/docs/Kubernetes#13-manage-a-highly-available-kubernetes-cluster"
  },"243": {
    "doc": "Kubernetes",
    "title": "1.4 Provision Underlying Infrastructure to Deploy a Kubernetes Cluster",
    "content": "See Objective 1.2 Use Kubeadm to Install a Basic Cluster. Note: Make sure that swap is disabled on all nodes. ",
    "url": "/my-tech/docs/Kubernetes#14-provision-underlying-infrastructure-to-deploy-a-kubernetes-cluster",
    
    "relUrl": "/docs/Kubernetes#14-provision-underlying-infrastructure-to-deploy-a-kubernetes-cluster"
  },"244": {
    "doc": "Kubernetes",
    "title": "1.5 Perform a Version Upgrade on a Kubernetes Cluster using Kubeadm",
    "content": ". | Upgrading kubeadm clusters | Safely Drain a Node while Respecting the PodDisruptionBudget | Cluster Management: Maintenance on a Node | . Note: All containers are restarted after upgrade, because the container spec hash value is changed. Upgrades are constrained from one minor version to the next minor version. First Control Plane Node . Update the kubeadm tool and verify the new version . Note: The --allow-change-held-packages flag is used because kubeadm updates should be held to prevent automated updates. apt-get update &amp;&amp; \\ apt-get install -y --allow-change-held-packages kubeadm=1.19.x-00 kubeadm version . Drain the node to mark as unschedulable . kubectl drain $NODENAME --ignore-daemonsets . Drain Diagram ![drain](https://kubernetes.io/images/docs/kubectl_drain.svg) . Perform an upgrade plan to validate that your cluster can be upgraded . Note: This also fetches the versions you can upgrade to and shows a table with the component config version states. sudo kubeadm upgrade plan . Upgrade the cluster . sudo kubeadm upgrade apply v1.19.x . Uncordon the node to mark as schedulable . kubectl uncordon $NODENAME . Additional Control Plane Nodes . Repeat the first control plane node steps while replacing the “upgrade the cluster” step using the command below: . sudo kubeadm upgrade node . Upgrade Control Plane Node Kubectl And Kubelet Tools . Upgrade the kubelet and kubectl on all control plane nodes . apt-get update &amp;&amp; \\ apt-get install -y --allow-change-held-packages kubelet=1.19.x-00 kubectl=1.19.x-00 . Restart the kubelet . sudo systemctl daemon-reload sudo systemctl restart kubelet . Upgrade Worker Nodes . Upgrade kubeadm . apt-get update &amp;&amp; \\ apt-get install -y --allow-change-held-packages kubeadm=1.19.x-00 . Drain the node . kubectl drain $NODENAME --ignore-daemonsets . Upgrade the kubelet configuration . sudo kubeadm upgrade node . Upgrade kubelet and kubectl . apt-get update &amp;&amp; \\ apt-get install -y --allow-change-held-packages kubelet=1.19.x-00 kubectl=1.19.x-00 sudo systemctl daemon-reload sudo systemctl restart kubelet . Uncordon the node . kubectl uncordon $NODENAME . ",
    "url": "/my-tech/docs/Kubernetes#15-perform-a-version-upgrade-on-a-kubernetes-cluster-using-kubeadm",
    
    "relUrl": "/docs/Kubernetes#15-perform-a-version-upgrade-on-a-kubernetes-cluster-using-kubeadm"
  },"245": {
    "doc": "Kubernetes",
    "title": "1.6 Implement Etcd Backup And Restore",
    "content": ". | Operating etcd clusters for Kubernetes: Backing up an etcd cluster | Etcd Documentation: Disaster Recovery | Kubernetes Tips: Backup and Restore Etcd | . Snapshot The Keyspace . Use etcdctl snapshot save. Snapshot the keyspace served by $ENDPOINT to the file snapshot.db: . ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshot.db . Restore From Snapshot . Use etcdctl snapshot restore. Note: Restoring overwrites some snapshot metadata (specifically, the member ID and cluster ID); the member loses its former identity. Note: Snapshot integrity is verified when restoring from a snapshot using an integrity hash created by etcdctl snapshot save, but not when restoring from a file copy. Create new etcd data directories (m1.etcd, m2.etcd, m3.etcd) for a three member cluster: . $ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name m1 \\ --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host1:2380 $ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name m2 \\ --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host2:2380 $ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name m3 \\ --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host3:2380 . ",
    "url": "/my-tech/docs/Kubernetes#16-implement-etcd-backup-and-restore",
    
    "relUrl": "/docs/Kubernetes#16-implement-etcd-backup-and-restore"
  },"246": {
    "doc": "Kubernetes",
    "title": "Objective 2: Workloads &amp; Scheduling",
    "content": ". | Objective 1: Cluster Architecture, Installation \\&amp; Configuration . | 1.1 Manage Role Based Access Control (RBAC) . | Lab Environment | Lab Practice | . | 1.2 Use Kubeadm to Install a Basic Cluster . | Kubeadm Tasks for All Nodes | Kubeadm Tasks for Single Control Node | Kubeadm Tasks for Worker Node(s) | Kubeadm Troubleshooting | Kubeadm Optional Tasks | . | 1.3 Manage A Highly-Available Kubernetes Cluster . | HA Deployment Types | Upgrading from Single Control-Plane to High Availability | . | 1.4 Provision Underlying Infrastructure to Deploy a Kubernetes Cluster | 1.5 Perform a Version Upgrade on a Kubernetes Cluster using Kubeadm . | First Control Plane Node | Additional Control Plane Nodes | Upgrade Control Plane Node Kubectl And Kubelet Tools | Upgrade Worker Nodes | . | 1.6 Implement Etcd Backup And Restore . | Snapshot The Keyspace | Restore From Snapshot | . | . | Objective 2: Workloads \\&amp; Scheduling . | 2.1 Understand Deployments And How To Perform Rolling Update And Rollbacks . | Create Deployment | Perform Rolling Update | Perform Rollbacks | . | 2.2 Use Configmaps And Secrets To Configure Applications . | Configmaps | Secrets | Other Concepts | . | 2.3 Know How To Scale Applications | 2.4 Understand The Primitives Used To Create Robust, Self-Healing, Application Deployments | 2.5 Understand How Resource Limits Can Affect Pod Scheduling | 2.6 Awareness Of Manifest Management And Common Templating Tools | . | Objective 3: Services \\&amp; Networking . | 3.1 Understand Host Networking Configuration On The Cluster Nodes | 3.2 Understand Connectivity Between Pods | 3.3 Understand ClusterIP, NodePort, LoadBalancer Service Types And Endpoints . | ClusterIP | NodePort | LoadBalancer | ExternalIP | ExternalName | Networking Cleanup for Objective 3.3 | . | 3.4 Know How To Use Ingress Controllers And Ingress Resources | 3.5 Know How To Configure And Use CoreDNS | 3.6 Choose An Appropriate Container Network Interface Plugin | . | Objective 4: Storage . | 4.1 Understand Storage Classes, Persistent Volumes . | Storage Classes | Persistent Volumes | . | 4.2 Understand Volume Mode, Access Modes And Reclaim Policies For Volumes . | Volume Mode | Access Modes | Reclaim Policies | . | 4.3 Understand Persistent Volume Claims Primitive | 4.4 Know How To Configure Applications With Persistent Storage | . | Objective 5: Troubleshooting . | 5.1 Evaluate Cluster And Node Logging . | Cluster Logging | Node Logging | . | 5.2 Understand How To Monitor Applications | 5.3 Manage Container Stdout \\&amp; Stderr Logs | 5.4 Troubleshoot Application Failure | 5.5 Troubleshoot Cluster Component Failure | 5.6 Troubleshoot Networking | . | . ",
    "url": "/my-tech/docs/Kubernetes#objective-2-workloads--scheduling",
    
    "relUrl": "/docs/Kubernetes#objective-2-workloads--scheduling"
  },"247": {
    "doc": "Kubernetes",
    "title": "2.1 Understand Deployments And How To Perform Rolling Update And Rollbacks",
    "content": "Official Documentation . Deployments are used to manage Pods and ReplicaSets in a declarative manner. Create Deployment . Using the nginx image on Docker Hub, we can use a Deployment to push any number of replicas of that image to the cluster. Create the nginx deployment in the wahlnetwork1 namespace. kubectl create deployment nginx --image=nginx --replicas=3 -n wahlnetwork1 . Alternatively, use kubectl create deployment nginx --image=nginx --replicas=3 -n wahlnetwork1 --dry-run=client -o yaml to output a proper yaml configuration. apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: nginx name: nginx namespace: wahlnetwork1 spec: replicas: 3 selector: matchLabels: app: nginx strategy: {} template: metadata: creationTimestamp: null labels: app: nginx spec: containers: - image: nginx name: nginx resources: {} . Perform Rolling Update . Official Documentation . Used to make changes to the pod’s template and roll them out to the cluster. Triggered when data within .spec.template is changed. Update the nginx deployment in the wahlnetwork1 namespace to use version 1.16.1 . kubectl set image deployment/nginx nginx=nginx:1.16.1 -n wahlnetwork1 --record . Track the rollout status. kubectl rollout status deployment.v1.apps/nginx -n wahlnetwork1 . Waiting for deployment \"nginx\" rollout to finish: 1 out of 2 new replicas have been updated... Waiting for deployment \"nginx\" rollout to finish: 1 out of 2 new replicas have been updated... Waiting for deployment \"nginx\" rollout to finish: 1 out of 2 new replicas have been updated... Waiting for deployment \"nginx\" rollout to finish: 1 old replicas are pending termination... Waiting for deployment \"nginx\" rollout to finish: 1 old replicas are pending termination... deployment \"nginx\" successfully rolled out . Perform Rollbacks . Official Documentation . Rollbacks offer a method for reverting the changes to a pod’s .spec.template data to a previous version. By default, executing the rollout undo command will revert to the previous version. The desired version can also be declared. Review the version history for the nginx deployment in the wahlnetwork1 namespace. In this scenario, other revisions 1-4 have been made to simulate a deployment lifecycle. The 4th revision specifies a fake image version of 1.222222222222 to force a rolling update failure. kubectl rollout history deployment.v1.apps/nginx -n wahlnetwork1 . deployment.apps/nginx REVISION CHANGE-CAUSE 1 &lt;none&gt; 2 kubectl.exe set image deployment/nginx nginx=nginx:1.16.1 --record=true --namespace=wahlnetwork1 3 kubectl.exe set image deployment/nginx nginx=nginx:1.14.1 --record=true --namespace=wahlnetwork1 4 kubectl.exe set image deployment/nginx nginx=nginx:1.222222222222 --record=true --namespace=wahlnetwork1 . Revert to the previous version of the nginx deployment to use image version 1.14.1. This forces revision 3 to become revision # Note that revision 3 no longer exists. kubectl rollout undo deployment.v1.apps/nginx -n wahlnetwork1 . deployment.apps/nginx rolled back ~ kubectl rollout history deployment.v1.apps/nginx -n wahlnetwork1 deployment.apps/nginx REVISION CHANGE-CAUSE 1 &lt;none&gt; 2 kubectl.exe set image deployment/nginx nginx=nginx:1.16.1 --record=true --namespace=wahlnetwork1 4 kubectl.exe set image deployment/nginx nginx=nginx:1.222222222222 --record=true --namespace=wahlnetwork1 5 kubectl.exe set image deployment/nginx nginx=nginx:1.14.1 --record=true --namespace=wahlnetwork1 . Revert to revision 2 of the nginx deployment, which becomes revision 6 (the next available revision number). Note that revision 2 no longer exists. kubectl rollout undo deployment.v1.apps/nginx -n wahlnetwork1 --to-revision=2 . ~ kubectl rollout history deployment.v1.apps/nginx -n wahlnetwork1 deployment.apps/nginx REVISION CHANGE-CAUSE 1 &lt;none&gt; 4 kubectl.exe set image deployment/nginx nginx=nginx:1.222222222222 --record=true --namespace=wahlnetwork1 5 kubectl.exe set image deployment/nginx nginx=nginx:1.14.1 --record=true --namespace=wahlnetwork1 6 kubectl.exe set image deployment/nginx nginx=nginx:1.16.1 --record=true --namespace=wahlnetwork1 . ",
    "url": "/my-tech/docs/Kubernetes#21-understand-deployments-and-how-to-perform-rolling-update-and-rollbacks",
    
    "relUrl": "/docs/Kubernetes#21-understand-deployments-and-how-to-perform-rolling-update-and-rollbacks"
  },"248": {
    "doc": "Kubernetes",
    "title": "2.2 Use Configmaps And Secrets To Configure Applications",
    "content": "Configmaps . API object used to store non-confidential data in key-value pairs . | Official Documentation Configure a Pod to Use a ConfigMap | . Create a configmap named game-config using a directory. kubectl create configmap game-config --from-file=/code/configmap/ . ~ k describe configmap game-config Name: game-config Namespace: default Labels: &lt;none&gt; Annotations: &lt;none&gt; Data ==== game.properties: ---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: ---- color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice Events: &lt;none&gt; . Create a configmap named game-config using a file. kubectl create configmap game-config-2 --from-file=/code/configmap/game.properties . Create a configmap named game-config using an env-file. kubectl create configmap game-config-env-file --from-env-file=/code/configmap/game-env-file.properties . Create a configmap named special-config using a literal key/value pair. kubectl create configmap special-config --from-literal=special.how=very . Edit a configmap named game-config. kubectl edit configmap game-config . Get a configmap named game-config and output the response into yaml. kubectl get configmaps game-config -o yaml . Use a configmap with a pod by declaring a value for .spec.containers.env.name.valueFrom.configMapKeyRef. apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [\"/bin/sh\", \"-c\", \"env\"] env: # Define the environment variable - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY name: special-config # Specify the key associated with the value key: special.how restartPolicy: Never . Investigate the configmap value very from the key SPECIAL_LEVEL_KEY by reviewing the logs for the pod or by connecting to the pod directly. kubectl exec -n wahlnetwork1 --stdin nginx-6889dfccd5-msmn8 --tty -- /bin/bash . ~ kubectl logs dapi-test-pod KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=dapi-test-pod SHLVL=1 HOME=/root KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP_PROTO=tcp SPECIAL_LEVEL_KEY=very KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 PWD=/ KUBERNETES_SERVICE_HOST=10.96.0.1 . Secrets . | Managing Secret using kubectl | Using Secrets | . Create a secret named db-user-pass using files. kubectl create secret generic db-user-pass ` --from-file=./username.txt ` --from-file=./password.txt . The key name can be modified by inserting a key name into the file path. For example, setting the key names to funusername and funpassword can be done as shown below: . kubectl create secret generic fundb-user-pass ` --from-file=funusername=./username.txt ` --from-file=funpassword=./password.txt . Check to make sure the key names matches the defined names. kubectl describe secret fundb-user-pass . Name: fundb-user-pass Namespace: default Labels: &lt;none&gt; Annotations: &lt;none&gt; Type: Opaque Data ==== funpassword: 14 bytes funusername: 7 bytes . Get secret values from db-user-pass. kubectl get secret db-user-pass -o jsonpath='{.data}' . Edit secret values using the edit command. kubectl edit secrets db-user-pass . apiVersion: v1 data: password.txt: PASSWORD username.txt: USERNAME kind: Secret metadata: creationTimestamp: \"2020-10-13T22:48:27Z\" name: db-user-pass namespace: default resourceVersion: \"1022459\" selfLink: /api/v1/namespaces/default/secrets/db-user-pass uid: 6bb24810-dd33-4b92-9a37-424f3c7553b6 type: Opaque . Use a secret with a pod by declaring a value for .spec.containers.env.name.valueFrom.secretKeyRef. apiVersion: v1 kind: Pod metadata: name: secret-env-pod spec: containers: - name: mycontainer image: redis env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Never . Other Concepts . | Using imagePullSecrets | . ",
    "url": "/my-tech/docs/Kubernetes#22-use-configmaps-and-secrets-to-configure-applications",
    
    "relUrl": "/docs/Kubernetes#22-use-configmaps-and-secrets-to-configure-applications"
  },"249": {
    "doc": "Kubernetes",
    "title": "2.3 Know How To Scale Applications",
    "content": "Scaling is accomplished by changing the number of replicas in a Deployment. | Running Multiple Instances of Your App | . Scale a deployment named nginx from 3 to 4 replicas. kubectl scale deployments/nginx --replicas=4 . ",
    "url": "/my-tech/docs/Kubernetes#23-know-how-to-scale-applications",
    
    "relUrl": "/docs/Kubernetes#23-know-how-to-scale-applications"
  },"250": {
    "doc": "Kubernetes",
    "title": "2.4 Understand The Primitives Used To Create Robust, Self-Healing, Application Deployments",
    "content": ". | Don’t use naked Pods (that is, Pods not bound to a ReplicaSet or Deployment) if you can avoid it. Naked Pods will not be rescheduled in the event of a node failure. (source) | A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a strategy to replace Pods (such as RollingUpdate), is almost always preferable to creating Pods directly, except for some explicit restartPolicy: Never scenarios. A Job may also be appropriate. (source) | Define and use labels that identify semantic attributes of your application or Deployment, such as { app: myapp, tier: frontend, phase: test, deployment: v3 }. (source) | . ",
    "url": "/my-tech/docs/Kubernetes#24-understand-the-primitives-used-to-create-robust-self-healing-application-deployments",
    
    "relUrl": "/docs/Kubernetes#24-understand-the-primitives-used-to-create-robust-self-healing-application-deployments"
  },"251": {
    "doc": "Kubernetes",
    "title": "2.5 Understand How Resource Limits Can Affect Pod Scheduling",
    "content": "Resource limits are a mechanism to control the amount of resources needed by a container. This commonly translates into CPU and memory limits. | Limits set an upper boundary on the amount of resources a container is allowed to consume from the host. | Requests set an upper boundary on the amount of resources a container is allowed to consume from the host. | If a limit is set without a request, the request value is set to equal the limit value. | Managing Resources for Containers | Resource Quotas | . Here is an example of pod configured with resource requests and limits. apiVersion: v1 kind: Pod metadata: name: frontend spec: containers: - name: app image: images.my-company.example/app:v4 resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" - name: log-aggregator image: images.my-company.example/log-aggregator:v6 resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" . ",
    "url": "/my-tech/docs/Kubernetes#25-understand-how-resource-limits-can-affect-pod-scheduling",
    
    "relUrl": "/docs/Kubernetes#25-understand-how-resource-limits-can-affect-pod-scheduling"
  },"252": {
    "doc": "Kubernetes",
    "title": "2.6 Awareness Of Manifest Management And Common Templating Tools",
    "content": ". | Templating YAML in Kubernetes with real code | yq: Command-line YAML/XML processor | kustomize: lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. | Helm: A tool for managing Charts. Charts are packages of pre-configured Kubernetes resources. | . ",
    "url": "/my-tech/docs/Kubernetes#26-awareness-of-manifest-management-and-common-templating-tools",
    
    "relUrl": "/docs/Kubernetes#26-awareness-of-manifest-management-and-common-templating-tools"
  },"253": {
    "doc": "Kubernetes",
    "title": "Objective 3: Services &amp; Networking",
    "content": ". | Objective 3: Services &amp; Networking . | 3.1 Understand Host Networking Configuration On The Cluster Nodes | 3.2 Understand Connectivity Between Pods | 3.3 Understand ClusterIP, NodePort, LoadBalancer Service Types And Endpoints . | ClusterIP | NodePort | LoadBalancer | ExternalIP | ExternalName | Networking Cleanup for Objective 3.3 | . | 3.4 Know How To Use Ingress Controllers And Ingress Resources | 3.5 Know How To Configure And Use CoreDNS | 3.6 Choose An Appropriate Container Network Interface Plugin | . | . Note: If you need access to the pod network while working through the networking examples, use the Get a Shell to a Running Container guide to deploy a shell container. I often like to have a tab open to the shell container to run arbitrary network commands without the need to exec in and out of it repeatedly. ",
    "url": "/my-tech/docs/Kubernetes#objective-3-services--networking",
    
    "relUrl": "/docs/Kubernetes#objective-3-services--networking"
  },"254": {
    "doc": "Kubernetes",
    "title": "3.1 Understand Host Networking Configuration On The Cluster Nodes",
    "content": ". | Design . | All nodes can talk | All pods can talk (without NAT) | Every pod gets a unique IP address | . | Network Types . | Pod Network | Node Network | Services Network . | Rewrites egress traffic destined to a service network endpoint with a pod network IP address | . | . | Proxy Modes . | IPTables Mode . | The standard mode | kube-proxy watches the Kubernetes control plane for the addition and removal of Service and Endpoint objects | For each Service, it installs iptables rules, which capture traffic to the Service’s clusterIP and port, and redirect that traffic to one of the Service’s backend sets. | For each Endpoint object, it installs iptables rules which select a backend Pod. | Official Documentation | Kubernetes Networking Demystified: A Brief Guide | . | IPVS Mode . | Since 1.11 | Linux IP Virtual Server (IPVS) | L4 load balancer | . | . | . ",
    "url": "/my-tech/docs/Kubernetes#31-understand-host-networking-configuration-on-the-cluster-nodes",
    
    "relUrl": "/docs/Kubernetes#31-understand-host-networking-configuration-on-the-cluster-nodes"
  },"255": {
    "doc": "Kubernetes",
    "title": "3.2 Understand Connectivity Between Pods",
    "content": "Official Documentation . Read The Kubernetes network model: . | Every pod gets its own address | Fundamental requirements on any networking implementation . | Pods on a node can communicate with all pods on all nodes without NAT | Agents on a node (e.g. system daemons, kubelet) can communicate with all pods on that node | Pods in the host network of a node can communicate with all pods on all nodes without NAT | . | Kubernetes IP addresses exist at the Pod scope . | Containers within a pod can communicate with one another over localhost | “IP-per-pod” model | . | . ",
    "url": "/my-tech/docs/Kubernetes#32-understand-connectivity-between-pods",
    
    "relUrl": "/docs/Kubernetes#32-understand-connectivity-between-pods"
  },"256": {
    "doc": "Kubernetes",
    "title": "3.3 Understand ClusterIP, NodePort, LoadBalancer Service Types And Endpoints",
    "content": "Services are all about abstracting away the details of which pods are running behind a particular network endpoint. For many applications, work must be processed by some other service. Using a service allows the application to “toss over” the work to Kubernetes, which then uses a selector to determine which pods are healthy and available to receive the work. The service abstracts numerous replica pods that are available to do work. | Official Documentation | Katakoda Networking Introduction | . Note: This section was completed using a GKE cluster and may differ from what your cluster looks like. ClusterIP . | Exposes the Service on a cluster-internal IP. | Choosing this value makes the Service only reachable from within the cluster. | This is the default ServiceType. | Using Source IP | Kubectl Expose Command Reference | . The imperative option is to create a deployment and then expose the deployment. In this example, the deployment is exposed using a ClusterIP service that accepts traffic on port 80 and translates it to the pod using port 8080. kubectl create deployment funkyapp1 --image=k8s.gcr.io/echoserver:1.4 . kubectl expose deployment funkyapp1 --name=funkyip --port=80 --target-port=8080 --type=ClusterIP . Note: The --type=ClusterIP parameter is optional when deploying a ClusterIP service since this is the default type. apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: funkyapp1 #Selector name: funkyip spec: ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: funkyapp1 type: ClusterIP #Note this! . Using kubectl describe svc funkyip shows more details: . Name: funkyip Namespace: default Labels: app=funkyapp1 Annotations: cloud.google.com/neg: {\"ingress\":true} Selector: app=funkyapp1 Type: ClusterIP IP: 10.108.3.156 Port: &lt;unset&gt; 80/TCP TargetPort: 8080/TCP Endpoints: 10.104.2.7:8080 Session Affinity: None Events: &lt;none&gt; . Check to make sure the funkyip service exists. This also shows the assigned service (cluster IP) address. kubectl get svc funkyip . NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE funkyip ClusterIP 10.108.3.156 &lt;none&gt; 80/TCP 21m . From there, you can see the endpoint created to match any pod discovered using the app: funkyapp1 label. kubectl get endpoints funkyip . NAME ENDPOINTS AGE funkyip 10.104.2.7:8080 21m . The endpoint matches the IP address of the matching pod. kubectl get pods -o wide . NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES funkyapp1-7b478ccf9b-2vlc2 1/1 Running 0 21m 10.104.2.7 gke-my-first-cluster-1-default-pool-504c1e77-zg6v &lt;none&gt; &lt;none&gt; shell-demo 1/1 Running 0 3m12s 10.128.0.14 gke-my-first-cluster-1-default-pool-504c1e77-m9lk &lt;none&gt; &lt;none&gt; . The .spec.ports.port value defines the port used to access the service. The .spec.ports.targetPort value defines the port used to access the container’s application. User -&gt; Port -&gt; Kubernetes Service -&gt; Target Port -&gt; Application . This can be tested using curl: . export CLUSTER_IP=$(kubectl get services/funkyip -o go-template='(index .spec.clusterIP)') echo CLUSTER_IP=$CLUSTER_IP . From there, use curl $CLUSTER_IP:80 to hit the service port, which redirects to the targetPort of 8080. curl 10.108.3.156:80 . CLIENT VALUES: client_address=10.128.0.14 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.108.3.156:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.108.3.156 user-agent=curl/7.64.0 BODY: -no body in request-root . NodePort . | Exposes the Service on each Node’s IP at a static port (the NodePort). | Official Documentation | . kubectl expose deployment funkyapp1 --name=funkynode --port=80 --target-port=8080 --type=NodePort . apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: funkyapp1 #Selector name: funkynode spec: ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: funkyapp1 type: NodePort #Note this! . This service is available on each node at a specific port. kubectl describe svc funkynode . Name: funkynode Namespace: default Labels: app=funkyapp1 Annotations: cloud.google.com/neg: {\"ingress\":true} Selector: app=funkyapp1 Type: NodePort IP: 10.108.5.37 Port: &lt;unset&gt; 80/TCP TargetPort: 8080/TCP NodePort: &lt;unset&gt; 30182/TCP Endpoints: 10.104.2.7:8080 Session Affinity: None External Traffic Policy: Cluster Events: &lt;none&gt; . By using the node IP address with the nodePort value, we can see the desired payload. Make sure to scale the deployment so that each node is running one replica of the pod. For a cluster with 2 worker nodes, this can be done with kubectl scale deploy funkyapp1 --replicas=3. From there, it is possible to curl directly to a node IP address using the nodePort when using the shell pod demo. If working from outside the pod network, use the service IP address. curl 10.128.0.14:30182 . CLIENT VALUES: client_address=10.128.0.14 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.128.0.14:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.128.0.14:30182 user-agent=curl/7.64.0 BODY: -no body in request-root . LoadBalancer . | Exposes the Service externally using a cloud provider’s load balancer. | NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. | Source IP for Services with Type LoadBalancer | . kubectl expose deployment funkyapp1 --name=funkylb --port=80 --target-port=8080 --type=LoadBalancer . apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: funkyapp1 name: funkylb spec: ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: funkyapp1 type: LoadBalancer #Note this! . Get information on the funkylb service to determine the External IP address. kubectl get svc funkylb . NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE funkylb LoadBalancer 10.108.11.148 35.232.149.96 80:31679/TCP 64s . It is then possible to retrieve the payload using the External IP address and port value from anywhere on the Internet; no need to use the pod shell demo! . curl 35.232.149.96:80 . CLIENT VALUES: client_address=10.104.2.1 command=GET real path=/ query=nil request_version=1.1 request_uri=http://35.232.149.96:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=35.232.149.96 user-agent=curl/7.55.1 BODY: -no body in request- . ExternalIP . Official Documentation . | Exposes a Kubernetes service on an external IP address. | Kubernetes has no control over this external IP address. | . Here is an example spec: . apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 externalIPs: - 80.11.12.10 #Take note! . ExternalName . | Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. | No proxy of any kind is set up. | . Networking Cleanup for Objective 3.3 . Run these commands to cleanup the resources, if desired. kubectl delete svc funkyip kubectl delete svc funkynode kubectl delete svc funkylb kubectl delete deploy funkyapp1 . ",
    "url": "/my-tech/docs/Kubernetes#33-understand-clusterip-nodeport-loadbalancer-service-types-and-endpoints",
    
    "relUrl": "/docs/Kubernetes#33-understand-clusterip-nodeport-loadbalancer-service-types-and-endpoints"
  },"257": {
    "doc": "Kubernetes",
    "title": "3.4 Know How To Use Ingress Controllers And Ingress Resources",
    "content": "Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. | Traffic routing is controlled by rules defined on the Ingress resource. | An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. | For example, the NGINX Ingress Controller for Kubernetes | . | The name of an Ingress object must be a valid DNS subdomain name. | Ingress Documentation | A list of Ingress Controllers | Katacoda - Create Ingress Routing lab | Katacoda - Nginx on Kubernetes lab | . Example of an ingress resource: . apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: minimal-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /testpath pathType: Prefix backend: service: name: test port: number: 80 . Information on some of the objects within this resource: . | Ingress Rules | Path Types | . And, in the case of Nginx, a custom resource definition (CRD) is often used to extend the usefulness of an ingress. An example is shown below: . apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: cafe spec: host: cafe.example.com tls: secret: cafe-secret upstreams: - name: tea service: tea-svc port: 80 - name: coffee service: coffee-svc port: 80 routes: - path: /tea action: pass: tea - path: /coffee action: pass: coffee . ",
    "url": "/my-tech/docs/Kubernetes#34-know-how-to-use-ingress-controllers-and-ingress-resources",
    
    "relUrl": "/docs/Kubernetes#34-know-how-to-use-ingress-controllers-and-ingress-resources"
  },"258": {
    "doc": "Kubernetes",
    "title": "3.5 Know How To Configure And Use CoreDNS",
    "content": "CoreDNS is a general-purpose authoritative DNS server that can serve as cluster DNS. | A bit of history: . | As of Kubernetes v1.12, CoreDNS is the recommended DNS Server, replacing kube-dns. | In Kubernetes version 1.13 and later the CoreDNS feature gate is removed and CoreDNS is used by default. | In Kubernetes 1.18, kube-dns usage with kubeadm has been deprecated and will be removed in a future version. | . | Using CoreDNS for Service Discovery | Customizing DNS Service | . CoreDNS is installed with the following default Corefile configuration: . apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } . If you need to customize CoreDNS behavior, you create and apply your own ConfigMap to override settings in the Corefile. The Configuring DNS Servers for Kubernetes Clusters document describes this in detail. Review your configmaps for the kube-system namespace to determine if there is a coredns-custom configmap. kubectl get configmaps --namespace=kube-system . NAME DATA AGE cluster-kubestore 0 23h clustermetrics 0 23h extension-apiserver-authentication 6 24h gke-common-webhook-lock 0 23h ingress-gce-lock 0 23h ingress-uid 2 23h kube-dns 0 23h kube-dns-autoscaler 1 23h metrics-server-config 1 23h . Create a file named coredns.yml containing a configmap with the desired DNS entries in the data field such as the example below: . apiVersion: v1 kind: ConfigMap metadata: name: coredns-custom namespace: kube-system data: example.server: | # All custom server files must have a “.server” file extension. # Change example.com to the domain you wish to forward. example.com { # Change 1.1.1.1 to your customer DNS resolver. forward . 1.1.1.1 } . Apply the configmap. kubectl apply -f coredns.yml . Validate the existence of the coredns-custom configmap. kubectl get configmaps --namespace=kube-system . NAME DATA AGE cluster-kubestore 0 24h clustermetrics 0 24h coredns-custom 1 6s extension-apiserver-authentication 6 24h gke-common-webhook-lock 0 24h ingress-gce-lock 0 24h ingress-uid 2 24h kube-dns 0 24h kube-dns-autoscaler 1 24h metrics-server-config 1 24h . Get the configmap and output the value in yaml format. kubectl get configmaps --namespace=kube-system coredns-custom -o yaml . apiVersion: v1 data: example.server: | # Change example.com to the domain you wish to forward. example.com { # Change 1.1.1.1 to your customer DNS resolver. forward . 1.1.1.1 } kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"data\":{\"example.server\":\"# Change example.com to the domain you wish to forward.\\nexample.com {\\n # Change 1.1.1.1 to your customer DNS resolver.\\n forward . 1.1.1.1\\n}\\n\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"coredns-custom\",\"namespace\":\"kube-system\"}} creationTimestamp: \"2020-10-27T19:49:24Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:example.server: {} f:metadata: f:annotations: .: {} f:kubectl.kubernetes.io/last-applied-configuration: {} manager: kubectl-client-side-apply operation: Update time: \"2020-10-27T19:49:24Z\" name: coredns-custom namespace: kube-system resourceVersion: \"519480\" selfLink: /api/v1/namespaces/kube-system/configmaps/coredns-custom uid: 8d3250a5-cbb4-4f01-aae3-4e83bd158ebe . ",
    "url": "/my-tech/docs/Kubernetes#35-know-how-to-configure-and-use-coredns",
    
    "relUrl": "/docs/Kubernetes#35-know-how-to-configure-and-use-coredns"
  },"259": {
    "doc": "Kubernetes",
    "title": "3.6 Choose An Appropriate Container Network Interface Plugin",
    "content": "Generally, it seems that Flannel is good for starting out in a very simplified environment, while Calico (and others) extend upon the basic functionality to meet design-specific requirements. | Network Plugins | Choosing a CNI Network Provider for Kubernetes | Comparing Kubernetes CNI Providers: Flannel, Calico, Canal, and Weave | . Common decision points include: . | Network Model: Layer 2, Layer 3, VXLAN, etc. | Routing: Routing and route distribution for pod traffic between nodes | Network Policy: Essentially the firewall between network / pod segments | IP Address Management (IPAM) | Datastore: . | etcd - for direct connection to an etcd cluster | Kubernetes - for connection to a Kubernetes API server | . | . ",
    "url": "/my-tech/docs/Kubernetes#36-choose-an-appropriate-container-network-interface-plugin",
    
    "relUrl": "/docs/Kubernetes#36-choose-an-appropriate-container-network-interface-plugin"
  },"260": {
    "doc": "Kubernetes",
    "title": "Objective 4: Storage",
    "content": ". | Objective 4: Storage . | 4.1 Understand Storage Classes, Persistent Volumes . | Storage Classes | Persistent Volumes | . | 4.2 Understand Volume Mode, Access Modes And Reclaim Policies For Volumes . | Volume Mode | Access Modes | Reclaim Policies | . | 4.3 Understand Persistent Volume Claims Primitive | 4.4 Know How To Configure Applications With Persistent Storage | . | . ",
    "url": "/my-tech/docs/Kubernetes#objective-4-storage",
    
    "relUrl": "/docs/Kubernetes#objective-4-storage"
  },"261": {
    "doc": "Kubernetes",
    "title": "4.1 Understand Storage Classes, Persistent Volumes",
    "content": ". | Storage Classes | Persistent Volumes | . Storage Classes . | Reclaim Policy: PersistentVolumes that are dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of the class . | Delete: When PersistentVolumeClaim is deleted, also deletes PersistentVolume and underlying storage object | Retain: When PersistentVolumeClaim is deleted, PersistentVolume remains and volume is “released” | . | Volume Binding Mode: . | Immediate: By default, the Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created | WaitForFirstConsumer: Delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created . | Supported by AWSElasticBlockStore, GCEPersistentDisk, and AzureDisk | . | . | Allow Volume Expansion: Allow volumes to be expanded . | Note: It is not possible to reduce the size of a PersistentVolume | . | Default Storage Class: A default storage class is used when a PersistentVolumeClaim does not specify the storage class . | Can be handy when a single default services all pod volumes | . | Provisioner . | Determines the volume plugin to use for provisioning PVs. | Example: gke-pd, azure-disk | . | . View all storage classes . kubectl get storageclass or kubectl get sc . NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) kubernetes.io/gce-pd Delete Immediate true 25h . View the storage class in yaml format . kubectl get sc standard -o yaml . allowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard parameters: type: pd-standard provisioner: kubernetes.io/gce-pd reclaimPolicy: Delete volumeBindingMode: Immediate . Make a custom storage class using the yaml configuration below and save it as speedyssdclass.yaml . allowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: speedyssdclass parameters: type: pd-ssd # Note: This will use SSD backed disks fstype: ext4 replication-type: none provisioner: kubernetes.io/gce-pd reclaimPolicy: Delete volumeBindingMode: Immediate . Apply the storage class configuration to the cluster . kubectl apply -f speedyssdclass.yaml . storageclass.storage.k8s.io/speedyssdclass created . Get the storage classes . kubectl get sc . NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE speedyssdclass kubernetes.io/gce-pd Retain WaitForFirstConsumer true 5m19s standard (default) kubernetes.io/gce-pd Delete Immediate true 8d . Persistent Volumes . View a persistent volume in yaml format . kubectl get pv pvc-d2f6e37e-277f-4b7b-8725-542609f1dea4 -o yaml . apiVersion: v1 kind: PersistentVolume metadata: name: pvc-d2f6e37e-277f-4b7b-8725-542609f1dea4 spec: accessModes: - ReadWriteOnce capacity: storage: 1Gi persistentVolumeReclaimPolicy: Delete storageClassName: standard volumeMode: Filesystem . Create a new disk named pv100 in Google Cloud to be used as a persistent volume . Note: Use the zone of your GKE cluster . gcloud compute disks create pv100 --size 10GiB --zone=us-central1-c . Make a custom persistent volume using the yaml configuration below and save it as pv100.yaml . apiVersion: v1 kind: PersistentVolume metadata: name: pv100 spec: accessModes: - ReadWriteOnce capacity: storage: 1Gi persistentVolumeReclaimPolicy: Delete storageClassName: standard volumeMode: Filesystem gcePersistentDisk: # This section is required since we are not using a Storage Class fsType: ext4 pdName: pv100 . Apply the persistent volume to the cluster . kubectl apply -f pv100.yaml . persistentvolume/pv100 created . Get the persistent volume and notice that it has a status of Available since there is no PersistentVolumeClaim to bind against . kubectl get pv pv100 . NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv100 1Gi RWO Delete Available standard 2m51s . ",
    "url": "/my-tech/docs/Kubernetes#41-understand-storage-classes-persistent-volumes",
    
    "relUrl": "/docs/Kubernetes#41-understand-storage-classes-persistent-volumes"
  },"262": {
    "doc": "Kubernetes",
    "title": "4.2 Understand Volume Mode, Access Modes And Reclaim Policies For Volumes",
    "content": ". | Volume Mode | Access Modes | Reclaim Policy | . Volume Mode . | Filesystem: Kubernetes formats the volume and presents it to a specified mount point. | If the volume is backed by a block device and the device is empty, Kuberneretes creates a filesystem on the device before mounting it for the first time. | . | Block: Kubernetes exposes a raw block device to the container. | Improved time to usage and perhaps performance. | The container must know what to do with the device; there is no filesystem. | . | Defined in .spec.volumeMode for a PersistentVolumeClaim. | . View the volume mode for persistent volume claims using the -o wide to see the VOLUMEMODE column . kubectl get pvc -o wide . NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE www-web-0 Bound pvc-f3e92637-7e0d-46a3-ad87-ef1275bb5a72 1Gi RWO standard 19m Filesystem www-web-1 Bound pvc-d2f6e37e-277f-4b7b-8725-542609f1dea4 1Gi RWO standard 19m Filesystem . Access Modes . | ReadWriteOnce (RWO): can be mounted as read-write by a single node | ReadOnlyMany (ROX): can be mounted as read-only by many nodes | ReadWriteMany (RWX): can be mounted as read-write by many nodes | Defined in .spec.accessModes for a PersistentVolumeClaim and PersistentVolume | . View the access mode for persistent volume claims . kubectl get pvc . NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-f3e92637-7e0d-46a3-ad87-ef1275bb5a72 1Gi RWO standard 28m www-web-1 Bound pvc-d2f6e37e-277f-4b7b-8725-542609f1dea4 1Gi RWO standard 27m . Reclaim Policies . | Reclaim Policy: PersistentVolumes that are dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of the class . | Delete: When PersistentVolumeClaim is deleted, also deletes PersistentVolume and underlying storage object | Retain: When PersistentVolumeClaim is deleted, PersistentVolume remains and volume is “released” | . | Change the Reclaim Policy of a PersistentVolume | Defined in .spec.persistentVolumeReclaimPolicy for PersistentVolume. | . View the reclaim policy set on persistent volumes . kubectl get pv . NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-d2f6e37e-277f-4b7b-8725-542609f1dea4 1Gi RWO Delete Bound default/www-web-1 standard 45m pvc-f3e92637-7e0d-46a3-ad87-ef1275bb5a72 1Gi RWO Delete Bound default/www-web-0 standard 45m . ",
    "url": "/my-tech/docs/Kubernetes#42-understand-volume-mode-access-modes-and-reclaim-policies-for-volumes",
    
    "relUrl": "/docs/Kubernetes#42-understand-volume-mode-access-modes-and-reclaim-policies-for-volumes"
  },"263": {
    "doc": "Kubernetes",
    "title": "4.3 Understand Persistent Volume Claims Primitive",
    "content": "Make a custom persistent volume claim using the yaml configuration below and save it as pvc01.yaml . apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc01 spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 3Gi . Apply the persistent volume claim . kubectl apply -f pvc01.yaml . persistentvolumeclaim/pvc01 created . Get the persistent volume claim . kubectl get pvc pvc01 . NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc01 Bound pvc-9f2e7c5d-b64c-467e-bba6-86ccb333d981 3Gi RWO standard 5m19s . ",
    "url": "/my-tech/docs/Kubernetes#43-understand-persistent-volume-claims-primitive",
    
    "relUrl": "/docs/Kubernetes#43-understand-persistent-volume-claims-primitive"
  },"264": {
    "doc": "Kubernetes",
    "title": "4.4 Know How To Configure Applications With Persistent Storage",
    "content": ". | Configure a Pod to Use a PersistentVolume for Storage | . Create a new yaml file using the configuration below and save it as pv-pod.yaml . Note: Make sure to create pvc01 in this earlier step . apiVersion: v1 kind: Pod metadata: name: pv-pod spec: volumes: - name: pv-pod-storage # The name of the volume, used by .spec.containers.volumeMounts.name persistentVolumeClaim: claimName: pvc01 # This pvc was created in an earlier step containers: - name: pv-pod-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: pv-pod-storage # This refers back to .spec.volumes.name . Apply the pod . kubectl apply -f pv-pod.yaml . pod/pv-pod created . Watch the pod provisioning process . kubectl get pod -w pv-pod . NAME READY STATUS RESTARTS AGE pv-pod 1/1 Running 0 30s . View the binding on pvc01 . kubectl describe pvc pvc01 . Name: pvc01 Namespace: default StorageClass: standard Status: Bound Volume: pvc-9f2e7c5d-b64c-467e-bba6-86ccb333d981 Labels: &lt;none&gt; Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd Finalizers: [kubernetes.io/pvc-protection] Capacity: 3Gi Access Modes: RWO VolumeMode: Filesystem Mounted By: pv-pod # Here it is! Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ProvisioningSucceeded 36m persistentvolume-controller Successfully provisioned volume pvc-9f2e7c5d-b64c-467e-bba6-86ccb333d981 using kubernetes.io/gce-pd . ",
    "url": "/my-tech/docs/Kubernetes#44-know-how-to-configure-applications-with-persistent-storage",
    
    "relUrl": "/docs/Kubernetes#44-know-how-to-configure-applications-with-persistent-storage"
  },"265": {
    "doc": "Kubernetes",
    "title": "Objective 5: Troubleshooting",
    "content": ". | Troubleshooting Kubernetes deployments . | Objective 5: Troubleshooting . | 5.1 Evaluate Cluster And Node Logging . | Cluster Logging | Node Logging | . | 5.2 Understand How To Monitor Applications | 5.3 Manage Container Stdout &amp; Stderr Logs | 5.4 Troubleshoot Application Failure | 5.5 Troubleshoot Cluster Component Failure | 5.6 Troubleshoot Networking | . | . ",
    "url": "/my-tech/docs/Kubernetes#objective-5-troubleshooting",
    
    "relUrl": "/docs/Kubernetes#objective-5-troubleshooting"
  },"266": {
    "doc": "Kubernetes",
    "title": "5.1 Evaluate Cluster And Node Logging",
    "content": "Cluster Logging . Having a separate storage location for cluster component logging, such as nodes, pods, and applications. | Cluster-level logging architectures | Kubernetes Logging Best Practices | . Commonly deployed in one of three ways: . ",
    "url": "/my-tech/docs/Kubernetes#51-evaluate-cluster-and-node-logging",
    
    "relUrl": "/docs/Kubernetes#51-evaluate-cluster-and-node-logging"
  },"267": {
    "doc": "Kubernetes",
    "title": "Logging agent on each node that sends log data to a backend storage repository",
    "content": "# These agents can be deployed using a DaemonSet replica to ensure nodes have the agent running # Note: This approach only works for applications’ standard output (stdout) and standard error (stderr) . ",
    "url": "/my-tech/docs/Kubernetes#logging-agent-on-each-node-that-sends-log-data-to-a-backend-storage-repository",
    
    "relUrl": "/docs/Kubernetes#logging-agent-on-each-node-that-sends-log-data-to-a-backend-storage-repository"
  },"268": {
    "doc": "Kubernetes",
    "title": "Logging agent as a sidecar to specific deployments that sends log data to a backend storage repository",
    "content": "# Note: Writing logs to a file and then streaming them to stdout can double disk usage . ",
    "url": "/my-tech/docs/Kubernetes#logging-agent-as-a-sidecar-to-specific-deployments-that-sends-log-data-to-a-backend-storage-repository",
    
    "relUrl": "/docs/Kubernetes#logging-agent-as-a-sidecar-to-specific-deployments-that-sends-log-data-to-a-backend-storage-repository"
  },"269": {
    "doc": "Kubernetes",
    "title": "Configure the containerized application to send log data to a backend storage repository",
    "content": "Node Logging . Having a log file on the node that is populated with standard output (stdout) and standard error (stderr) log entries from containers running on the node. | Logging at the node level | . ",
    "url": "/my-tech/docs/Kubernetes#configure-the-containerized-application-to-send-log-data-to-a-backend-storage-repository",
    
    "relUrl": "/docs/Kubernetes#configure-the-containerized-application-to-send-log-data-to-a-backend-storage-repository"
  },"270": {
    "doc": "Kubernetes",
    "title": "5.2 Understand How To Monitor Applications",
    "content": ". | Using kubectl describe pod to fetch details about pods | Interacting with running Pods | . ",
    "url": "/my-tech/docs/Kubernetes#52-understand-how-to-monitor-applications",
    
    "relUrl": "/docs/Kubernetes#52-understand-how-to-monitor-applications"
  },"271": {
    "doc": "Kubernetes",
    "title": "5.3 Manage Container Stdout &amp; Stderr Logs",
    "content": ". | Kubectl Commands - Logs | How to find—and use—your GKE logs with Cloud Logging | Enable Log Rotation in Kubernetes Cluster | . kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER] . | -f will follow the logs | -p will pull up the previous instance of the container | -c will select a specific container for pods that have more than one | . ",
    "url": "/my-tech/docs/Kubernetes#53-manage-container-stdout--stderr-logs",
    
    "relUrl": "/docs/Kubernetes#53-manage-container-stdout--stderr-logs"
  },"272": {
    "doc": "Kubernetes",
    "title": "5.4 Troubleshoot Application Failure",
    "content": ". | Troubleshoot Applications | Status: Pending . | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. | If no resources available on cluster, Cluster Autoscaling will increased node count if enabled | Once node count satisfied, pods in Pending status will be deployed | . | Status: Waiting . | A container in the Waiting state is still running the operations it requires in order to complete start up | . | . Describe the pod to get details on the configuration, containers, events, conditions, volumes, etc. | Is the status equal to RUNNING? | Are there enough resources to schedule the pod? | Are there enough hostPorts remaining to schedule the pod? | . kubectl describe pod counter . Name: counter Namespace: default Priority: 0 Node: gke-my-first-cluster-1-default-pool-504c1e77-xcvj/10.128.0.15 Start Time: Tue, 10 Nov 2020 16:33:10 -0600 Labels: &lt;none&gt; Annotations: &lt;none&gt; Status: Running IP: 10.104.1.7 IPs: IP: 10.104.1.7 Containers: count: Container ID: docker://430313804a529153c1dc5badd1394164906a7dead8708a4b850a0466997e1c34 Image: busybox Image ID: docker-pullable://busybox@sha256:a9286defaba7b3a519d585ba0e37d0b2cbee74ebfe590960b0b1d6a5e97d1e1d Port: &lt;none&gt; Host Port: &lt;none&gt; Args: /bin/sh -c i=0; while true; do echo \"$i: $(date)\" &gt;&gt; /var/log/1.log; echo \"$(date) INFO $i\" &gt;&gt; /var/log/2.log; i=$((i+1)); sleep 1; done State: Running Started: Tue, 10 Nov 2020 16:33:12 -0600 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/log from varlog (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-2qnnp (ro) count-log-1: Container ID: docker://d5e95aa4aec3a55435d610298f94e7b8b2cfdf2fb88968f00ca4719a567a6e37 Image: busybox Image ID: docker-pullable://busybox@sha256:a9286defaba7b3a519d585ba0e37d0b2cbee74ebfe590960b0b1d6a5e97d1e1d Port: &lt;none&gt; Host Port: &lt;none&gt; Args: /bin/sh -c tail -n+1 -f /var/log/1.log State: Running Started: Tue, 10 Nov 2020 16:33:13 -0600 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/log from varlog (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-2qnnp (ro) count-log-2: Container ID: docker://eaa9983cbd55288a139b63c30cfe3811031dedfae0842b9233ac48db65387d4d Image: busybox Image ID: docker-pullable://busybox@sha256:a9286defaba7b3a519d585ba0e37d0b2cbee74ebfe590960b0b1d6a5e97d1e1d Port: &lt;none&gt; Host Port: &lt;none&gt; Args: /bin/sh -c tail -n+1 -f /var/log/2.log State: Running Started: Tue, 10 Nov 2020 16:33:13 -0600 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/log from varlog (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-2qnnp (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: varlog: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: SizeLimit: &lt;unset&gt; default-token-2qnnp: Type: Secret (a volume populated by a Secret) SecretName: default-token-2qnnp Optional: false QoS Class: BestEffort Node-Selectors: &lt;none&gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 30m default-scheduler Successfully assigned default/counter to gke-my-first-cluster-1-default-pool-504c1e77-xcvj Normal Pulling 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Pulling image \"busybox\" Normal Pulled 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Successfully pulled image \"busybox\" Normal Created 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Created container count Normal Started 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Started container count Normal Pulling 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Pulling image \"busybox\" Normal Created 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Created container count-log-1 Normal Pulled 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Successfully pulled image \"busybox\" Normal Started 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Started container count-log-1 Normal Pulling 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Pulling image \"busybox\" Normal Pulled 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Successfully pulled image \"busybox\" Normal Created 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Created container count-log-2 Normal Started 30m kubelet, gke-my-first-cluster-1-default-pool-504c1e77-xcvj Started container count-log-2 . Validate the commands being presented to the pod to ensure nothing was configured incorrectly. kubectl apply --validate -f mypod.yaml . ",
    "url": "/my-tech/docs/Kubernetes#54-troubleshoot-application-failure",
    
    "relUrl": "/docs/Kubernetes#54-troubleshoot-application-failure"
  },"273": {
    "doc": "Kubernetes",
    "title": "5.5 Troubleshoot Cluster Component Failure",
    "content": ". | Troubleshoot Clusters | A general overview of cluster failure modes | Control Plane Components | Node Components | . Components to investigate: . | Control Plane Components . | kube-apiserver | etcd | kube-scheduler | kube-controller-manager | cloud-controller-manager | . | Node Components . | kubelet | kube-proxy | Container runtime (e.g. Docker) | . | . View the components with: . kubectl get all -n kube-system . NAME READY STATUS RESTARTS AGE pod/konnectivity-agent-56nck 1/1 Running 0 15d pod/konnectivity-agent-gmklx 1/1 Running 0 15d pod/konnectivity-agent-wg92c 1/1 Running 0 15d pod/kube-dns-576766df6b-cz4ln 3/3 Running 0 15d pod/kube-dns-576766df6b-rcsk7 3/3 Running 0 15d pod/kube-dns-autoscaler-7f89fb6b79-pq66d 1/1 Running 0 15d pod/kube-proxy-gke-my-first-cluster-1-default-pool-504c1e77-m9lk 1/1 Running 0 15d pod/kube-proxy-gke-my-first-cluster-1-default-pool-504c1e77-xcvj 1/1 Running 0 15d pod/kube-proxy-gke-my-first-cluster-1-default-pool-504c1e77-zg6v 1/1 Running 0 15d pod/l7-default-backend-7fd66b8b88-ng57f 1/1 Running 0 15d pod/metrics-server-v0.3.6-7c5cb99b6f-2d8bx 2/2 Running 0 15d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/default-http-backend NodePort 10.108.1.184 &lt;none&gt; 80:32084/TCP 15d service/kube-dns ClusterIP 10.108.0.10 &lt;none&gt; 53/UDP,53/TCP 15d service/metrics-server ClusterIP 10.108.1.154 &lt;none&gt; 443/TCP 15d NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/konnectivity-agent 3 3 3 3 3 &lt;none&gt; 15d daemonset.apps/kube-proxy 0 0 0 0 0 kubernetes.io/os=linux,node.kubernetes.io/kube-proxy-ds-ready=true 15d daemonset.apps/metadata-proxy-v0.1 0 0 0 0 0 cloud.google.com/metadata-proxy-ready=true,kubernetes.io/os=linux 15d daemonset.apps/nvidia-gpu-device-plugin 0 0 0 0 0 &lt;none&gt; 15d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/kube-dns 2/2 2 2 15d deployment.apps/kube-dns-autoscaler 1/1 1 1 15d deployment.apps/l7-default-backend 1/1 1 1 15d deployment.apps/metrics-server-v0.3.6 1/1 1 1 15d NAME DESIRED CURRENT READY AGE replicaset.apps/kube-dns-576766df6b 2 2 2 15d replicaset.apps/kube-dns-autoscaler-7f89fb6b79 1 1 1 15d replicaset.apps/l7-default-backend-7fd66b8b88 1 1 1 15d replicaset.apps/metrics-server-v0.3.6-7c5cb99b6f 1 1 1 15d replicaset.apps/metrics-server-v0.3.6-7ff8cdbc49 0 0 0 15d . Retrieve detailed information about the cluster . kubectl cluster-info or kubectl cluster-info dump . Retrieve a list of known API resources to aid with describing or troubleshooting . kubectl api-resources . NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node persistentvolumeclaims pvc true PersistentVolumeClaim persistentvolumes pv false PersistentVolume &lt;snip&gt; . Check the logs in /var/log on the master and worker nodes: . | Master . | /var/log/kube-apiserver.log - API Server, responsible for serving the API | /var/log/kube-scheduler.log - Scheduler, responsible for making scheduling decisions | /var/log/kube-controller-manager.log - Controller that manages replication controllers | . | Worker Nodes . | /var/log/kubelet.log - Kubelet, responsible for running containers on the node | /var/log/kube-proxy.log - Kube Proxy, responsible for service load balancing | . | . ",
    "url": "/my-tech/docs/Kubernetes#55-troubleshoot-cluster-component-failure",
    
    "relUrl": "/docs/Kubernetes#55-troubleshoot-cluster-component-failure"
  },"274": {
    "doc": "Kubernetes",
    "title": "5.6 Troubleshoot Networking",
    "content": ". | Flannel Troubleshooting . | The flannel kube subnet manager relies on the fact that each node already has a podCIDR defined. | . | Calico Troubleshooting . | Containers do not have network connectivity | . | . ",
    "url": "/my-tech/docs/Kubernetes#56-troubleshoot-networking",
    
    "relUrl": "/docs/Kubernetes#56-troubleshoot-networking"
  },"275": {
    "doc": "Misc",
    "title": "Misc",
    "content": " ",
    "url": "/my-tech/docs/Misc",
    
    "relUrl": "/docs/Misc"
  },"276": {
    "doc": "Misc",
    "title": "Front page of documents uncategorized",
    "content": "Date Posted: 2023 03 01 . Summary Code . ",
    "url": "/my-tech/docs/Misc",
    
    "relUrl": "/docs/Misc"
  },"277": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "tutorial about the running a sample docker image in numbered steps",
    "content": " ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/"
  },"278": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "this page describes about tutorial about the running a sample docker image in numbered steps",
    "content": "Date Posted: 2023 03 29 . ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/"
  },"279": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Tutorial: Running a Sample Docker Image",
    "content": "In this tutorial, we will guide you through the process of running a sample Docker image. Docker allows you to package an application and its dependencies into a standardized unit called a container, which can be easily deployed and run on any platform. Follow the numbered steps below to get started: . ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#tutorial-running-a-sample-docker-image",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#tutorial-running-a-sample-docker-image"
  },"280": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Install Docker: Before you can run a Docker image, ensure Docker is installed on your system. Visit the Docker website and follow the installation instructions for your operating system.",
    "content": " ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#install-docker-before-you-can-run-a-docker-image-ensure-docker-is-installed-on-your-system-visit-the-docker-website-and-follow-the-installation-instructions-for-your-operating-system",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#install-docker-before-you-can-run-a-docker-image-ensure-docker-is-installed-on-your-system-visit-the-docker-website-and-follow-the-installation-instructions-for-your-operating-system"
  },"281": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Pull the Docker Image: Docker images are hosted in repositories, and you need to pull the desired image to your local system. Open a terminal or command prompt and enter the following command:",
    "content": "docker pull IMAGE_NAME . Replace IMAGE_NAME with the name of the Docker image you want to use. This command will download the image from the repository. ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#pull-the-docker-image-docker-images-are-hosted-in-repositories-and-you-need-to-pull-the-desired-image-to-your-local-system-open-a-terminal-or-command-prompt-and-enter-the-following-command",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#pull-the-docker-image-docker-images-are-hosted-in-repositories-and-you-need-to-pull-the-desired-image-to-your-local-system-open-a-terminal-or-command-prompt-and-enter-the-following-command"
  },"282": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Verify Image Pull: To verify that the image has been successfully pulled, use the following command:",
    "content": "docker images . You should see the pulled image listed among the available images. ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#verify-image-pull-to-verify-that-the-image-has-been-successfully-pulled-use-the-following-command",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#verify-image-pull-to-verify-that-the-image-has-been-successfully-pulled-use-the-following-command"
  },"283": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Run the Docker Image: Now it’s time to run the Docker image as a container. Execute the following command:",
    "content": "docker run IMAGE_NAME . Replace IMAGE_NAME with the name of the image you pulled in the previous step. Docker will create a new container based on this image. ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#run-the-docker-image-now-its-time-to-run-the-docker-image-as-a-container-execute-the-following-command",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#run-the-docker-image-now-its-time-to-run-the-docker-image-as-a-container-execute-the-following-command"
  },"284": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Access Container Output: If the containerized application produces any output, you can access it using the Docker logs. Run the following command to view the logs:",
    "content": "docker logs CONTAINER_ID . Replace CONTAINER_ID with the ID of the running container, which you can find using the command docker ps. ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#access-container-output-if-the-containerized-application-produces-any-output-you-can-access-it-using-the-docker-logs-run-the-following-command-to-view-the-logs",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#access-container-output-if-the-containerized-application-produces-any-output-you-can-access-it-using-the-docker-logs-run-the-following-command-to-view-the-logs"
  },"285": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Interact with the Container: Sometimes, you may need to interact with the running container, such as executing commands or modifying files. Use the following command to access the container’s shell:",
    "content": "docker exec -it CONTAINER_ID bash . This will open a shell inside the running container, allowing you to execute commands as if you were on the host system. ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#interact-with-the-container-sometimes-you-may-need-to-interact-with-the-running-container-such-as-executing-commands-or-modifying-files-use-the-following-command-to-access-the-containers-shell",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#interact-with-the-container-sometimes-you-may-need-to-interact-with-the-running-container-such-as-executing-commands-or-modifying-files-use-the-following-command-to-access-the-containers-shell"
  },"286": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Stop the Container: Once you are done with the container, you can stop it using the command:",
    "content": "docker stop CONTAINER_ID . Replace CONTAINER_ID with the ID of the running container. ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#stop-the-container-once-you-are-done-with-the-container-you-can-stop-it-using-the-command",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#stop-the-container-once-you-are-done-with-the-container-you-can-stop-it-using-the-command"
  },"287": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Remove the Container: If you no longer need the container, you can remove it using the following command:",
    "content": "docker rm CONTAINER_ID . Remember to replace CONTAINER_ID with the ID of the container you want to remove. ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#remove-the-container-if-you-no-longer-need-the-container-you-can-remove-it-using-the-following-command",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#remove-the-container-if-you-no-longer-need-the-container-you-can-remove-it-using-the-following-command"
  },"288": {
    "doc": "tutorial about the running a sample docker image in numbered steps",
    "title": "Remove the Docker Image: If you no longer need the Docker image, you can remove it from your system using the command:",
    "content": "docker rmi IMAGE_NAME . Replace IMAGE_NAME with the name of the image you want to remove. That’s it! You have successfully run a sample Docker image as a container. Docker provides a powerful and flexible platform for deploying and managing applications. Explore further by experimenting with other Docker images and building your own containers. Remember to check the official Docker documentation for more detailed information and advanced topics. ",
    "url": "/my-tech/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#remove-the-docker-image-if-you-no-longer-need-the-docker-image-you-can-remove-it-from-your-system-using-the-command",
    
    "relUrl": "/doc/Misc/tutorial-about-the-running-a-sample-docker-image-in-numbered-steps/#remove-the-docker-image-if-you-no-longer-need-the-docker-image-you-can-remove-it-from-your-system-using-the-command"
  }
}
